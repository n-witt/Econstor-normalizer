{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luke the Downloader\n",
    "## Purpose\n",
    "\n",
    "This notebook downloads files that occur in JSON files generated by the [Econbiz API](https://api.econbiz.de/doc) ([Example](https://api.econbiz.de/v1/search?q=serendipity)), tries to determine the corresponding RePec handle, obtains the citations count information (via citEc API) and stores those information.\n",
    "\n",
    "## Output\n",
    "A directory called `data` including the subdirectories `pdf`, `json`, `failed` will be created in the working directory.\n",
    "1. `pdf` contains the PDF files\n",
    "2. `json` includes the corresponding meta data.\n",
    "3. `failed` keeps track of files that couldn't be downloaded\n",
    "\n",
    "The meta data of a file with name `pdf/foobar.pdf` can be found in `json/foobar.pdf.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word of caution\n",
    "The code in this notebook utilizes multiple APIs. It does this in a way that applies a lot of workload onto the services. Therefore, you (more precisely: your IP address) could be blacklisted which precludes you from using that service (temporarily).\n",
    "To mitigate this issue, the programm creates cache files whenever possible.\n",
    "\n",
    "The first run of this program will take a lot of time (depending on your machine and your internet connection, but we are talking about hours), so be patient. The subsequent runs are much faster (less than a minute), because the local caches will be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "Let's start simple and define some helper functions that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "\n",
    "maxNumDocs = 200000\n",
    "\n",
    "def readData(path='repec.json'):\n",
    "    '''\n",
    "    helper function that reads json data and \n",
    "    converts it to python objects\n",
    "    '''\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def apiToJson(url, toFile=True, cacheFile='repec.json'):\n",
    "    '''\n",
    "    Queries `url` and stores the result to `repec.json`. By overriding\n",
    "    the `cacheFile` parameter the result will be written into another\n",
    "    file. If `toFile` is set to false, the function will return the\n",
    "    object instead of persisting it\n",
    "    '''\n",
    "    eBData = urllib2.urlopen(url)\n",
    "    eBData = json.loads(eBData.read())\n",
    "    if toFile and (type(cacheFile) == str or type(cacheFile) == unicode) and len(cacheFile) > 0:\n",
    "        with open(cacheFile, 'w') as f:\n",
    "            json.dump(eBData, f)\n",
    "    elif toFile == False:\n",
    "        return json.dumps(eBData)\n",
    "    else:\n",
    "        raise ArgumentValidationError('If `toFile` is set to True you need to pass a valid path in the `cacheFile` parameter')\n",
    "        \n",
    "def citationCount(repecHdl):\n",
    "    '''\n",
    "    Return citation counts from RePec's citec API\n",
    "    '''\n",
    "    # do we have a valid repec-handle?\n",
    "    if type(repecHdl) == str or type(repecHdl) == unicode:\n",
    "        citecUrl = 'http://citec.repec.org/api/plain/' + repecHdl + '/us435'\n",
    "        try:\n",
    "            citationData = xmltodict.parse(urllib2.urlopen(citecUrl, timeout=10).read())\n",
    "        except:\n",
    "            raise URLError('Couldn\\'t fetch data. Check you Configuration and' + \\\n",
    "                          'the availability of http://citec.repec.org')\n",
    "        else:\n",
    "            if citationData.has_key('errorString'):\n",
    "                raise IOError(citationData['errorString'])\n",
    "                \n",
    "            if citationData.has_key('citationData'):\n",
    "                citedBy = citationData['citationData']['citedBy']\n",
    "                cites = citationData['citationData']['cites']\n",
    "                return {'citedBy': citedBy, 'cites': cites}\n",
    "            else:\n",
    "                return {'citedBy': None, 'cites': None}\n",
    "    else:\n",
    "        raise TypeError('You need to pass a string')\n",
    "        \n",
    "def mkDir(dir):\n",
    "    '''\n",
    "    creates a dir with name `dir` if it doesn't exist\n",
    "    '''\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def validateURL(url):\n",
    "    regex = re.compile(\n",
    "    r'^(?:http|ftp)s?://' # http:// or https://\n",
    "    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' # domain...\n",
    "    r'localhost|' # localhost...\n",
    "    r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|' # ...or ipv4\n",
    "    r'\\[?[A-F0-9]*:[A-F0-9:]+\\]?)' # ...or ipv6\n",
    "    r'(?::\\d+)?' # optional port\n",
    "    r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    return regex.match(url) != None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper that allows the execution of cells from the command prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib2\n",
    "import IPython\n",
    "import ipykernel as kernel\n",
    "connection_file_path = kernel.get_connection_file()\n",
    "connection_file = os.path.basename(connection_file_path)\n",
    "kernel_id = connection_file.split('-', 1)[1].split('.')[0]\n",
    "\n",
    "def executeCell(x=0):\n",
    "    ''' executes the code in cell no 'x' (zero-based indexing)\n",
    "    '''\n",
    "    sessions = json.load(urllib2.urlopen('http://127.0.0.1:8888/api/sessions'))\n",
    "    ipynbFileName = \"\"\n",
    "    for sess in sessions:\n",
    "        if sess['kernel']['id'] == kernel_id:\n",
    "            ipynbFileName = sess['notebook'][u'path']\n",
    "            ipynbFileName = ipynbFileName.split(os.sep)[-1]\n",
    "            break\n",
    "\n",
    "    # read this notebook's file\n",
    "    if ipynbFileName != \"\":\n",
    "        with open(ipynbFileName) as f:\n",
    "            nb = json.load(f)\n",
    "    \n",
    "    # locate cell's code\n",
    "    if type(nb) == dict:\n",
    "        try:\n",
    "            code = \"\"\n",
    "            if nb[u'cells'][x][u'cell_type'] == u'code':\n",
    "                for s in nb[u'cells'][x]['source']:\n",
    "                    code += s\n",
    "            else:\n",
    "                raise TypeError(\"The cell you request is not of type 'code'\")\n",
    "        except IndexError:\n",
    "            raise IndexError('No cell #' + str(x))\n",
    "    # execute\n",
    "    get_ipython().run_cell(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the RePec handle\n",
    "In order to receive citation count data from RePec for a given Econbiz document the corresponding RePec handle (a unique identifier) is required.\n",
    "Unfortunately, there is no straight-forward way to do so. This notebook implements three ways to obtain the RePec handle. They are presented during the course of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wolfgang's method\n",
    "This method obtains the RePec handle through two stages of indirection from a given Econbiz ID (e.g. 10011374989).\n",
    "1. Receive more data for the Econbiz item at hand through the `/record` method of the [Econbiz API](https://api.econbiz.de/doc)\n",
    "2. Find the [Handle.net](http://handle.net)-handler in the `identifier_number` field\n",
    "3. Use Wolfgang's handle.net-handler to repec-handler (a lot of handles here, i know ;)) [service](http://www.econstor.eu/repec/handleToRepec/<Handle.net-handle>) to obtain the RePec handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib2\n",
    "import re\n",
    "from urllib2 import URLError\n",
    "import xmltodict\n",
    "import os\n",
    "\n",
    "def determineRepecHandle_WolfgangsMethod():\n",
    "    cacheFile = 'wolfgangsCache.json'\n",
    "    \n",
    "    # Build LookUpTable\n",
    "    if os.path.exists(cacheFile):\n",
    "        with open(cacheFile) as f:\n",
    "            lut = json.load(f)\n",
    "    else:\n",
    "        lut = {}\n",
    "        \n",
    "    def fetchRepecHandler(id):\n",
    "        # Pass the Econbiz ID an receive the RePec handler (if exists) \n",
    "        try:\n",
    "            econbizRecordURL = 'http://api.econbiz.de/v1/record/' + id\n",
    "        except TypeError:\n",
    "            raise TypeError('You need to pass the id as a str or unicode.')\n",
    "        try:\n",
    "            # fetch more details corresponding to current item\n",
    "            # looking for a handle.net handle\n",
    "            itemMetadata = urllib2.urlopen(econbizRecordURL).read().decode('utf8')\n",
    "            itemMetadata = json.loads(itemMetadata)\n",
    "        except Exception:\n",
    "            raise IOError(\"Couldn't read ressource. Not a JSON file?\")\n",
    "        else:\n",
    "            hdlStrings = \"\"\n",
    "            for identifier_url in itemMetadata['record']['identifier_number']:\n",
    "                # is it a handle.net handle?\n",
    "                if re.match(r'(hdl:)?[0-9]{4,6}/[0-9]{3,6} \\[[H|h]andle\\]', identifier_url) != None:\n",
    "                    match = re.search(r'[0-9]{4,6}/[0-9]{3,6}', identifier_url)\n",
    "                    if match != None:\n",
    "                        hdlStrings = match.group().split('/')            \n",
    "\n",
    "            # do we have a valid handle.net-handle?\n",
    "            if type(hdlStrings) == list:\n",
    "                handleToRepecUrl = 'http://www.econstor.eu/repec/handleToRepec/' + hdlStrings[0] + '/' + hdlStrings[1] + '.txt'\n",
    "                try:\n",
    "                    return urllib2.urlopen(handleToRepecUrl).read()\n",
    "                except URLError:\n",
    "                    return None\n",
    "    \n",
    "    def lookup(id):    \n",
    "        # read cache file an return repec handler if existing\n",
    "        if lut.has_key(id):\n",
    "            return lut[id]\n",
    "\n",
    "        # handler not in local cache. fetch and persist it\n",
    "        repecHandler = fetchRepecHandler(id)\n",
    "        lut.update({id: repecHandler})\n",
    "        with open(cacheFile, 'w') as f:\n",
    "            json.dump(lut, f)\n",
    "            \n",
    "        return repecHandler\n",
    "    \n",
    "    return lookup\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many RePec handles are uncovered by Wolfgang's method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wolfgangsMetadataFile = 'wolfgangsMetadata.json'\n",
    "\n",
    "if not os.path.exists(wolfgangsMetadataFile):\n",
    "    apiToJson(url='http://api.econbiz.de/v1/search?q=source:econstor+identifier_url:pdf&secret=Z-8_uu&size=' + str(maxNumDocs) + '&fields=title,identifier_url,person,date,id', cacheFile=wolfgangsMetadataFile)\n",
    "\n",
    "data = readData(wolfgangsMetadataFile)\n",
    "\n",
    "hasRepec = 0\n",
    "numDocs = len(data['hits']['hits'])\n",
    "lookup = determineRepecHandle_WolfgangsMethod()\n",
    "for i, item in enumerate(data['hits']['hits']):\n",
    "    try:\n",
    "        repecHdl = lookup(item['id'])\n",
    "    except:\n",
    "        # we don't care about any errors ;)\n",
    "        continue\n",
    "    if repecHdl != None:\n",
    "        hasRepec += 1\n",
    "    if i % 1000 == 0:\n",
    "        print \"{:.1f}% finished\".format((i/float(numDocs))*100)\n",
    "print \"\\nRESULT:\\n{:.1f}% items have a repec handle\".format((hasRepec/float(numDocs))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Henning's method\n",
    "In contrast to Wolfgang's method, Henning's method is more direct and concise as it receives RePec handles from the Econbiz API directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "import os\n",
    "\n",
    "henningsMetadataFile = 'henningsMetadata.json'\n",
    "\n",
    "def determineRepecHandle_HenningsMethod():\n",
    "    '''\n",
    "    For efficiency reasons (using closures), this methods returns \n",
    "    a methods that allows querying the dataset using an Econbiz ID,\n",
    "    instead of doing the job itself.\n",
    "    '''\n",
    "    cacheFile = 'henningsCache.json'\n",
    "    if os.path.exists(cacheFile):\n",
    "        with open(cacheFile) as f:\n",
    "            lut = json.load(f)\n",
    "    else:\n",
    "        if not os.path.exists(henningsMetadataFile):\n",
    "            apiToJson(url='http://api.econbiz.de/v1/search?q=source:econstor+identifier_url:pdf&secret=Z-8_uu&size=' + str(maxNumDocs) + '&fields=title,identifier_url,person,date,id,identifier_repec', cacheFile=henningsMetadataFile)\n",
    "            \n",
    "        eBData = readData(henningsMetadataFile)\n",
    "        \n",
    "        lut = {i['id']: i['identifier_repec'] for i in eBData['hits']['hits'] if i.has_key('identifier_repec')}\n",
    "        with open(cacheFile, 'w') as f:\n",
    "            json.dump(lut, f)\n",
    "\n",
    "    \n",
    "    def lookup(id):\n",
    "        if type(id) != str and type(id) != unicode:\n",
    "            raise TypeError('You need to pass the id as a str or unicode.')    \n",
    "        try:\n",
    "            return lut[id]\n",
    "        except KeyError:\n",
    "            return None\n",
    "    \n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many RePec handles are uncovered by Henning's method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(henningsMetadataFile):\n",
    "    apiToJson(url='http://api.econbiz.de/v1/search?q=source:econstor+identifier_url:pdf&secret=Z-8_uu&size=' + str(maxNumDocs) + '&fields=title,identifier_url,person,date,id,identifier_repec', cacheFile=henningsMetadataFile)\n",
    "\n",
    "data = readData(henningsMetadataFile)\n",
    "\n",
    "hasRepec = 0\n",
    "numDocs = len(data['hits']['hits'])\n",
    "lookup = determineRepecHandle_HenningsMethod()\n",
    "for i, item in enumerate(data['hits']['hits']):\n",
    "    try:\n",
    "        id = lookup(item['id'])\n",
    "    except:\n",
    "        # we don't care about any errors ;)\n",
    "        continue\n",
    "    if id != None:\n",
    "        hasRepec += 1\n",
    "    if i % 1000 == 0:\n",
    "        print \"{:.1f}% finished\".format((i/float(numDocs))*100)\n",
    "print \"\\nRESULT:\\n{:.1f}% items have a repec handle\".format((hasRepec/float(numDocs))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what is the intersection between both results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT:\n",
      "Wolfgang without Henning: 137\n",
      "Henning without Wolfgang: 27209\n",
      "Henning: 56624\n",
      "Wolfgang: 29552\n",
      "Wolfgang and Henning: 56761\n"
     ]
    }
   ],
   "source": [
    "data = readData('henningsMetadata.json') # picked hennings file randonly\n",
    "henningsMethod = determineRepecHandle_HenningsMethod()\n",
    "wolfgangsMethod = determineRepecHandle_WolfgangsMethod()\n",
    "\n",
    "henningsSet = set()\n",
    "wolfgangsSet = set()\n",
    "numDocs = len(data['hits']['hits'])\n",
    "\n",
    "for i, item in enumerate(data['hits']['hits']):\n",
    "    try:\n",
    "        eBId = item['id']\n",
    "    except TypeError:\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            henningsId = henningsMethod(eBId)\n",
    "            wolfgangsId = wolfgangsMethod(eBId)\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            if henningsId != None:\n",
    "                henningsSet.add(henningsId)\n",
    "            if wolfgangsId != None:\n",
    "                wolfgangsSet.add(wolfgangsId)\n",
    "        \n",
    "print '\\nRESULT:\\nWolfgang without Henning: {}\\nHenning without Wolfgang: {}' \\\n",
    ".format(str(len(wolfgangsSet.difference(henningsSet))), str(len(henningsSet.difference(wolfgangsSet))))\n",
    "print'Henning: {}\\nWolfgang: {}\\nWolfgang and Henning: {}'.format(str(len(henningsSet)), str(len(wolfgangsSet)), str(len(wolfgangsSet.union(henningsSet))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RePec crawler\n",
    "The third way to obtain RePec handles is based on RePec's search engine called [IDEAS](https://ideas.repec.org/). It's obviously build to interface humans, but it can also be used by robots, as we do it.\n",
    "This methods produces a RePec handle given the title of a document.\n",
    "It mimics a human user that queries the search engine with the title of a document. It \"clicks\" the first match (if there is a match) and extract the desired information from the detail page.\n",
    "It should be noted that this method is fragile and error-prone. In case the layout of the website changes, the corresponding xPath's need to be adapted appropriately. Moreover, this method can produce false-positive values. There is no guarantee that the RePec handle is the one you where looking for. But random sampling showed, that the results are quite reasonable (supposedly, because the titles from EconBiz match those on RePec very well, although not perfectly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib2\n",
    "import json\n",
    "from lxml import etree\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def determineRepecHandle_ideasCrawler(query):\n",
    "    numResults = '1'\n",
    "    xpathFirstResult = '//*[@id=\"content-block\"]/dl/dt/a'\n",
    "    \n",
    "    def genXpathRepecDetailPage(row=1, col=1, bold=False):\n",
    "        xpath = u'//*[@id=\"biblio-body\"]/table/tr[' + str(row) + ']/td[' + str(col) + ']'\n",
    "        if bold == True:\n",
    "            xpath += '/b'\n",
    "        return xpath\n",
    "    \n",
    "    # normalize text \n",
    "    #    replaces e.g. ä with a\n",
    "    unicodedata.normalize(\"NFKD\", query).encode(\"ascii\", \"ignore\").decode(\"utf8\")\n",
    "    #    remove everthing that's not alphanumeric\n",
    "    query = re.sub(r'[^A-Za-z0-9 ]*', '', query)\n",
    "    \n",
    "    # percentage encoding\n",
    "    queryPercentageEncoded = urllib.quote_plus(query)\n",
    "    htmlParser = etree.HTMLParser()\n",
    "    # Request result list\n",
    "    ideasHdl = urllib2.urlopen('http://ideas.repec.org/cgi-bin/htsearch?ul=&q=' + queryPercentageEncoded + '&cmd=Search%21&wf=4BFF&s=R&dt=range&db=&de=&m=all&fmt=long&sy=1&ps=' + numResults)\n",
    "    # parse received page\n",
    "    tree = etree.parse(ideasHdl, htmlParser)\n",
    "    # find first match in result list\n",
    "    match = tree.xpath(xpathFirstResult)\n",
    "    \n",
    "    # is there a match?\n",
    "    if len(match) > 0:\n",
    "        urlsDetailPages = match[0].values()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    for url in urlsDetailPages:\n",
    "        if validateURL(url):\n",
    "            detailPageHdl = urllib2.urlopen(url)\n",
    "            detailsPageTree = etree.parse(detailPageHdl, htmlParser)\n",
    "            \n",
    "            for i in xrange(1, 10):\n",
    "                '''\n",
    "                Go through the table util you find 'Handle:'. In this\n",
    "                case return value from the same row and the next \n",
    "                column, which is hopefully the RePEc handle\n",
    "                '''\n",
    "                xpath = genXpathRepecDetailPage(row=i, col=1, bold=True)\n",
    "                matchRow = detailsPageTree.xpath(xpath)\n",
    "                if len(matchRow) > 0 and matchRow[0].text.strip() == 'Handle:':\n",
    "                    xpath = genXpathRepecDetailPage(row=i, col=2)\n",
    "                    matchCol = detailsPageTree.xpath(xpath)\n",
    "                    if len(matchCol) > 0 and matchCol[0].text != None:\n",
    "                        return matchCol[0].text\n",
    "                    else:\n",
    "                        return None\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build a function that combines Wolfgang's and Henning's methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetchRepecHandlerByEBId():\n",
    "    henningsMethod = determineRepecHandle_HenningsMethod()\n",
    "    wolfgangsMethod = determineRepecHandle_WolfgangsMethod()\n",
    "    \n",
    "    def lookup(id):\n",
    "        # hennings method\n",
    "        repecHdl = henningsMethod(id)\n",
    "        if repecHdl != None:\n",
    "            return repecHdl\n",
    "        # wolfgangs method\n",
    "        repecHdl = wolfgangsMethod(id)\n",
    "        if repecHdl != None:\n",
    "            return repecHdl\n",
    "\n",
    "        return None\n",
    "    \n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "Now that we have all components in place, let's glue them together. \n",
    "What the code roughly does, for every item in the meta data file, is:\n",
    "1. Download the corresponding PDF-file\n",
    "2. Save the corresponding meta data into a separate file\n",
    "3. Determine the RePec handle\n",
    "  1. Try Henning's method\n",
    "  2. If the previous method failed, try Wolfgang's method\n",
    "  3. If the previous method failed, try scraping RePec\n",
    "4. Fetch citation count informaton form citEc and save it into the file create in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib2\n",
    "import logging\n",
    "import re\n",
    "import xmltodict\n",
    "import sys\n",
    "import time\n",
    "\n",
    "repecDelay = 0.5\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "wd = os.getcwd() + os.sep + '..' + os.sep + 'data'\n",
    "metadataFile = henningsMetadataFile\n",
    "failedPath = 'failedToDownload.json'\n",
    "lookupRepecHdl = fetchRepecHandlerByEBId()\n",
    "\n",
    "with open(metadataFile, \"r\") as data_file:\n",
    "    data = json.load(data_file)\n",
    "    if data.has_key(\"hits\") and data[\"hits\"].has_key(\"hits\"):\n",
    "        data = data[\"hits\"][\"hits\"]\n",
    "    else:\n",
    "        raise Exception(\"unknown Datastructure\")\n",
    "\n",
    "# create directories if not existing\n",
    "pdfDir = wd + os.sep +  u'pdf'\n",
    "jsonDir = wd  + os.sep + u'json'\n",
    "failDir = wd + os.sep + os.sep + u'failed'\n",
    "for f in (pdfDir, jsonDir, failDir):\n",
    "    mkDir(f)\n",
    "    \n",
    "u = \"\"\n",
    "failedDownloads = []\n",
    "for itemNumber, item in enumerate(data):\n",
    "    url = item[\"identifier_url\"][0]\n",
    "    filename = url.split(\"/\")[-1]\n",
    "\n",
    "    # download the pdf file\n",
    "    try:\n",
    "        if not os.path.exists(pdfDir + os.sep + filename):\n",
    "            u = urllib2.urlopen(url)\n",
    "            with open(pdfDir + os.sep + filename, 'w') as f:\n",
    "                f.write(u.read())\n",
    "            logging.log(logging.INFO, filename + \" successfully downloaded.\")\n",
    "        else:\n",
    "            logging.log(logging.INFO, filename + \" skipped download. Already downloaded.\")        \n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.log(logging.INFO, url + \" couldn't be opened.\") \n",
    "        failedDownloads.append(item)\n",
    "        logging.error(logging.ERROR, e)\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        # write meta data to json file\n",
    "        jsonFile = os.path.join(jsonDir, filename + '.json')\n",
    "        if os.path.exists(jsonFile):\n",
    "            with open(jsonFile, 'r') as f:\n",
    "                try:\n",
    "                    itemFromFile = json.load(f)\n",
    "                except ValueError:\n",
    "                    itemFromFile = {}\n",
    "        else:\n",
    "            itemFromFile = {}\n",
    "            \n",
    "        if itemFromFile.has_key('citedBy') and \\\n",
    "           itemFromFile.has_key('cites'):\n",
    "            logging.log(logging.INFO, filename + u'.json skipped. Has citations counts already')\n",
    "            continue\n",
    "        else:\n",
    "            citeCount = None\n",
    "            # try to obtain repec handle\n",
    "            repecHdl = lookupRepecHdl(item['id'])\n",
    "            \n",
    "            if repecHdl == None:\n",
    "                # no handle so far. maybe we can find one on repec\n",
    "                title = \"\"\n",
    "                for s in item['title']:\n",
    "                    title += s + ' '\n",
    "                title = title.strip()\n",
    "                try:\n",
    "                    repecHdl = determineRepecHandle_ideasCrawler(title)\n",
    "                except Exception as e:\n",
    "                    if e.args:\n",
    "                        errString = \"Error while crawling RePEc. Error was:\\n\" + unicode(e.args[0])\n",
    "                    else:\n",
    "                        errString = \"Error while crawling RePEc.\"\n",
    "                    logging.log(logging.INFO, errString)\n",
    "                else:\n",
    "                    if repecHdl == None:\n",
    "                        logging.log(logging.INFO, \"Couldn't obtained RePec Handle for \" + unicode(filename))\n",
    "                    else:\n",
    "                        logging.log(logging.INFO, \"Obtained RePec Handle from ideas:\" + unicode(repecHdl))\n",
    "            \n",
    "            if repecHdl != None:\n",
    "                # Fetch citation count figures\n",
    "                try:\n",
    "                    citeCount = citationCount(repecHdl.strip())\n",
    "                    # let's be kind and lower the workload\n",
    "                    time.sleep(repecDelay)\n",
    "                except IOError as e:\n",
    "                    if e.args[0].find('exceeded') != -1:\n",
    "                        logging.log(logging.INFO, \"Unfortunately you may have been blacklisted by the citec-API.\")\n",
    "                        #sys.exit('citec service unavailable')\n",
    "                    else:\n",
    "                        logging.log(logging.INFO, \"No citation count data available for this document\")\n",
    "                \n",
    "            if citeCount == None:\n",
    "                citeCount = {'citedBy': None, 'cites': None}\n",
    "                \n",
    "            itemFromFile.update(citeCount)\n",
    "            itemFromFile.update(item)\n",
    "            \n",
    "            with open(jsonDir + os.sep + filename + u'.json', 'w') as jf:\n",
    "                json.dump(itemFromFile, jf)\n",
    "                logging.log(logging.INFO, filename + u'.json updated')\n",
    "    logging.log(logging.INFO, '{:.2f}% finished'.format(float(itemNumber)*100/len(data)))\n",
    "\n",
    "if len(failedDownloads) > 0:\n",
    "    handler = open(failDir + os.sep + failedPath, \"w\")\n",
    "    handler.write(json.dumps(failedDownloads))\n",
    "logging.log(logging.INFO, \"Downloads complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
