{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luke the Downloader\n",
    "\n",
    "This script downloads files that are occur in XML files generated by the [Econbiz API](https://api.econbiz.de/doc) ([Example](https://api.econbiz.de/v1/search?q=serendipity)).\n",
    "\n",
    "## Workflow\n",
    "1. Generate a XML file using the Econbiz API\n",
    "2. Rename the file to 'econbiz.xml' or change the `metadataFile` variable according to the file name and copy the file into the working directory of this notebook\n",
    "3. Run the notebook\n",
    "\n",
    "## Output\n",
    "A directory called `data` including the subdirectories `pdf`, `json`, `failed` will be created in the working directory.\n",
    "1. `pdf` stores the PDF files\n",
    "2. `json` includes the corresponding meta data.\n",
    "3. `failed` keeps track of files that couldn't be downloaded\n",
    "\n",
    "The meta data of a file with name `pdf/foobar.pdf` can be found in `json/foobar.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word of caution\n",
    "The code in this notebook utilizes multiple APIs. It does this in a way that applies a lot of workload onto the services. Therefore, you (more precisely: your IP address) could be blacklisted which precludes you from using that service (temporarily).\n",
    "To mitigate this issue, the programm creates cache files whenever possible.\n",
    "\n",
    "The first run of this program will take a lot of time (depending on your machine and your connection, but we are talking about hours) so be patient. The subsequent run are much faster, because the local caches will be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the RePec handle\n",
    "In order to receive citation count data from RePec for a given Econbiz document the corresponding RePec handle (a unique identifier) is required.\n",
    "Unfortunately, there is no straight-forward way to do so. This notebook implements several ways to obtain the RePec handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wolfgang's method\n",
    "This method obtains the RePec handle through two stages of indirection from a given Econbiz ID (e.g. 10011374989).\n",
    "1. Receive more data for the Econbiz item at hand through the `/record` method of the [Econbiz API](https://api.econbiz.de/doc)\n",
    "2. Find the [Handle.net](http://handle.net)-handler in the `identifier_number` field\n",
    "3. Use Wolfgang's handle.net-handler to repec-handler (a lot of handles here, i know ;)) [service](http://www.econstor.eu/repec/handleToRepec/<Handle.net-handle>) to obtain the RePec handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helper functions and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "import pdb\n",
    "\n",
    "maxNumDocs = 200000\n",
    "\n",
    "def readData(path='repec.json'):\n",
    "    '''\n",
    "    helper function that reads json data and \n",
    "    converts it to python objects\n",
    "    '''\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def apiToJson(url, toFile=True, cacheFile='repec.json'):\n",
    "    '''\n",
    "    Queries `url` and stores the result to `repec.json`. By overriding\n",
    "    the `cacheFile` parameter the result will be written into another\n",
    "    file. If `toFile` is set to false, the function will return the\n",
    "    object instead of persisting it\n",
    "    '''\n",
    "    eBData = urllib2.urlopen(url)\n",
    "    eBData = json.loads(eBData.read())\n",
    "    if toFile and (type(cacheFile) == str or type(cacheFile) == unicode) and len(cacheFile) > 0:\n",
    "        with open(cacheFile, 'w+') as f:\n",
    "            json.dump(eBData, f)\n",
    "    elif toFile == False:\n",
    "        return json.dumps(eBData)\n",
    "    else:\n",
    "        raise ArgumentValidationError('If `toFile` is set to True you need to pass a valid path in the `cacheFile` parameter')\n",
    "        \n",
    "def citationCount(repecHdl):\n",
    "    '''\n",
    "    Return citation counts from RePec's citec API\n",
    "    '''\n",
    "    # do we have a valid repec-handle?\n",
    "    if type(repecHdl) == str or type(repecHdl) == unicode:\n",
    "        citecUrl = 'http://citec.repec.org/api/plain/' + repecHdl\n",
    "        try:\n",
    "            citationData = xmltodict.parse(urllib2.urlopen(citecUrl).read())\n",
    "        except URLError:\n",
    "            raise URLError('Couldn\\'t fetch data. Check you Configuration and' + \\\n",
    "                          'the availability of http://citec.repec.org')\n",
    "        else:\n",
    "            #pdb.set_trace()\n",
    "            if citationData.has_key('errorString'):\n",
    "                raise IOError(citationData['errorString'])\n",
    "                \n",
    "            if citationData.has_key('citationData'):\n",
    "                citedBy = citationData['citationData']['citedBy']\n",
    "                cites = citationData['citationData']['cites']\n",
    "                return {'citedBy': citedBy, 'cites': cites}\n",
    "            else:\n",
    "                return {'citedBy': None, 'cites': None}\n",
    "    else:\n",
    "        raise TypeError('You need to pass a string')\n",
    "        \n",
    "def mkDir(dir):\n",
    "    '''\n",
    "    creates a dir with name `dir` if it doesn't exist\n",
    "    '''\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def validateURL(url):\n",
    "    regex = re.compile(\n",
    "    r'^(?:http|ftp)s?://' # http:// or https://\n",
    "    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' # domain...\n",
    "    r'localhost|' # localhost...\n",
    "    r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|' # ...or ipv4\n",
    "    r'\\[?[A-F0-9]*:[A-F0-9:]+\\]?)' # ...or ipv6\n",
    "    r'(?::\\d+)?' # optional port\n",
    "    r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    return regex.match(url) != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib2\n",
    "import re\n",
    "from urllib2 import URLError\n",
    "import xmltodict\n",
    "import os\n",
    "\n",
    "def determineRepecHandle_WolfgangsMethod():\n",
    "    cacheFile = 'wolfgangsCache.json'\n",
    "    \n",
    "    # Build LookUpTable\n",
    "    if os.path.exists(cacheFile):\n",
    "        with open(cacheFile) as f:\n",
    "            lut = json.load(f)\n",
    "    else:\n",
    "        lut = {}\n",
    "        \n",
    "    def fetchRepecHandler(id):\n",
    "        # Pass the Econbiz ID an receive the RePec handler (if exists) \n",
    "        try:\n",
    "            econbizRecordURL = 'http://api.econbiz.de/v1/record/' + id\n",
    "        except TypeError:\n",
    "            raise TypeError('You need to pass the id as a str or unicode.')\n",
    "        try:\n",
    "            # fetch more details corresponding to current item\n",
    "            # looking for a handle.net handle\n",
    "            itemMetadata = urllib2.urlopen(econbizRecordURL).read().decode('utf8')\n",
    "            itemMetadata = json.loads(itemMetadata)\n",
    "        except Exception:\n",
    "            raise IOError(\"Couldn't read ressource. Not a JSON file?\")\n",
    "        else:\n",
    "            for identifier_url in itemMetadata['record']['identifier_number']:\n",
    "                # is it a handle.net handle?\n",
    "                if re.match(r'(hdl:)?[0-9]{4,6}/[0-9]{3,6} \\[[H|h]andle\\]', identifier_url) != None:\n",
    "                    match = re.search(r'[0-9]{4,6}/[0-9]{3,6}', identifier_url)\n",
    "                    if match != None:\n",
    "                        hdlStrings = match.group().split('/')            \n",
    "\n",
    "            # do we have a valid handle.net-handle?\n",
    "            if type(hdlStrings) == list:\n",
    "                handleToRepecUrl = 'http://www.econstor.eu/repec/handleToRepec/' + hdlStrings[0] + '/' + hdlStrings[1] + '.txt'\n",
    "                try:\n",
    "                    return urllib2.urlopen(handleToRepecUrl).read()\n",
    "                except URLError:\n",
    "                    return None\n",
    "    \n",
    "    def lookup(id):    \n",
    "        # read cache file an return repec handler if existing\n",
    "        if lut.has_key(id):\n",
    "            return lut[id]\n",
    "\n",
    "        # handler not in local cache. fetch and persist it\n",
    "        repecHandler = fetchRepecHandler(id)\n",
    "        lut.update({id: repecHandler})\n",
    "        with open(cacheFile, 'w+') as f:\n",
    "            json.dump(lut, f)\n",
    "            \n",
    "        return repecHandler\n",
    "    \n",
    "    return lookup\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many RePec handles are uncovered by Wolfgang's method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% finished\n",
      "1.0% finished\n",
      "1.9% finished\n",
      "2.9% finished\n",
      "3.9% finished\n",
      "4.8% finished\n",
      "5.8% finished\n",
      "6.8% finished\n",
      "7.7% finished\n",
      "8.7% finished\n",
      "9.7% finished\n",
      "10.6% finished\n",
      "11.6% finished\n",
      "12.6% finished\n",
      "13.5% finished\n",
      "14.5% finished\n",
      "15.5% finished\n",
      "16.4% finished\n",
      "17.4% finished\n",
      "18.4% finished\n",
      "19.3% finished\n",
      "20.3% finished\n",
      "21.3% finished\n",
      "22.2% finished\n",
      "23.2% finished\n",
      "24.2% finished\n",
      "25.1% finished\n",
      "26.1% finished\n",
      "27.1% finished\n",
      "28.0% finished\n",
      "29.0% finished\n",
      "30.0% finished\n",
      "30.9% finished\n",
      "31.9% finished\n",
      "32.8% finished\n",
      "33.8% finished\n",
      "34.8% finished\n",
      "35.7% finished\n",
      "36.7% finished\n",
      "37.7% finished\n",
      "38.6% finished\n",
      "39.6% finished\n",
      "40.6% finished\n",
      "41.5% finished\n",
      "42.5% finished\n",
      "43.5% finished\n",
      "44.4% finished\n",
      "45.4% finished\n",
      "46.4% finished\n",
      "47.3% finished\n",
      "48.3% finished\n",
      "49.3% finished\n",
      "50.2% finished\n",
      "51.2% finished\n",
      "52.2% finished\n",
      "53.1% finished\n",
      "54.1% finished\n",
      "55.1% finished\n",
      "56.0% finished\n",
      "57.0% finished\n",
      "58.0% finished\n",
      "58.9% finished\n",
      "59.9% finished\n",
      "60.9% finished\n",
      "61.8% finished\n",
      "62.8% finished\n",
      "63.8% finished\n",
      "64.7% finished\n",
      "65.7% finished\n",
      "66.7% finished\n",
      "67.6% finished\n",
      "68.6% finished\n",
      "69.6% finished\n",
      "70.5% finished\n",
      "71.5% finished\n",
      "72.5% finished\n",
      "73.4% finished\n",
      "74.4% finished\n",
      "75.4% finished\n",
      "76.3% finished\n",
      "77.3% finished\n",
      "78.3% finished\n",
      "79.2% finished\n",
      "80.2% finished\n",
      "81.2% finished\n",
      "82.1% finished\n",
      "83.1% finished\n",
      "84.1% finished\n",
      "85.0% finished\n",
      "86.0% finished\n",
      "87.0% finished\n",
      "87.9% finished\n",
      "88.9% finished\n",
      "89.9% finished\n",
      "90.8% finished\n",
      "91.8% finished\n",
      "92.7% finished\n",
      "93.7% finished\n",
      "94.7% finished\n",
      "95.6% finished\n",
      "96.6% finished\n",
      "97.6% finished\n",
      "98.5% finished\n",
      "99.5% finished\n",
      "\n",
      "RESULT:\n",
      "28.5% item have a repec handle\n"
     ]
    }
   ],
   "source": [
    "wolfgangsMetadataFile = 'wolfgangsMetadata.json'\n",
    "\n",
    "if not os.path.exists(wolfgangsMetadataFile):\n",
    "    apiToJson(url='http://api.econbiz.de/v1/search?q=source:econstor+identifier_url:pdf&secret=Z-8_uu&size=' + str(maxNumDocs) + '&fields=title,identifier_url,person,date,id', cacheFile=wolfgangsMetadataFile)\n",
    "\n",
    "data = readData(wolfgangsMetadataFile)\n",
    "\n",
    "hasRepec = 0\n",
    "numDocs = len(data['hits']['hits'])\n",
    "lookup = determineRepecHandle_WolfgangsMethod()\n",
    "for i, item in enumerate(data['hits']['hits']):\n",
    "    try:\n",
    "        repecHdl = lookup(item['id'])\n",
    "    except:\n",
    "        # we don't care about any errors ;)\n",
    "        continue\n",
    "    if repecHdl != None:\n",
    "        hasRepec += 1\n",
    "    if i % 1000 == 0:\n",
    "        print \"{:.1f}% finished\".format((i/float(numDocs))*100)\n",
    "print \"\\nRESULT:\\n{:.1f}% item have a repec handle\".format((hasRepec/float(numDocs))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Henning's method\n",
    "In contrast to Wolfgang's method, Henning's method is more direct and concise as it receives RePec handles from the Econbiz API directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "import os\n",
    "\n",
    "henningsMetadataFile = 'henningsMetadata.json'\n",
    "\n",
    "def determineRepecHandle_HenningsMethod():\n",
    "    '''\n",
    "    For efficiency reasons (using closures), this methods returns \n",
    "    a methods that allows querying the dataset using an Econbiz ID,\n",
    "    instead of doing the job itself.\n",
    "    '''\n",
    "    cacheFile = 'henningsCache.json'\n",
    "    if os.path.exists(cacheFile):\n",
    "        with open(cacheFile) as f:\n",
    "            lut = json.load(f)\n",
    "    else:\n",
    "        if not os.path.exists(henningsMetadataFile):\n",
    "            apiToJson(url='http://api.econbiz.de/v1/search?q=source:econstor+identifier_url:pdf&secret=Z-8_uu&size=' + str(maxNumDocs) + '&fields=title,identifier_url,person,date,id,identifier_repec', cacheFile=henningsMetadataFile)\n",
    "            \n",
    "        eBData = readData(henningsMetadataFile)\n",
    "        \n",
    "        lut = {i['id']: i['identifier_repec'] for i in eBData['hits']['hits'] if i.has_key('identifier_repec')}\n",
    "        with open(cacheFile, 'w+') as f:\n",
    "            json.dump(lut, f)\n",
    "\n",
    "    \n",
    "    def lookup(id):\n",
    "        if type(id) != str and type(id) != unicode:\n",
    "            raise TypeError('You need to pass the id as a str or unicode.')    \n",
    "        try:\n",
    "            return lut[id]\n",
    "        except KeyError:\n",
    "            return None\n",
    "    \n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many RePec handles are uncovered by Henning's method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% finished\n",
      "1.0% finished\n",
      "1.9% finished\n",
      "2.9% finished\n",
      "3.9% finished\n",
      "4.8% finished\n",
      "5.8% finished\n",
      "6.8% finished\n",
      "7.7% finished\n",
      "8.7% finished\n",
      "9.7% finished\n",
      "10.6% finished\n",
      "11.6% finished\n",
      "12.6% finished\n",
      "13.5% finished\n",
      "14.5% finished\n",
      "15.5% finished\n",
      "16.4% finished\n",
      "17.4% finished\n",
      "18.3% finished\n",
      "19.3% finished\n",
      "20.3% finished\n",
      "21.2% finished\n",
      "22.2% finished\n",
      "23.2% finished\n",
      "24.1% finished\n",
      "25.1% finished\n",
      "26.1% finished\n",
      "27.0% finished\n",
      "28.0% finished\n",
      "29.0% finished\n",
      "29.9% finished\n",
      "30.9% finished\n",
      "31.9% finished\n",
      "32.8% finished\n",
      "33.8% finished\n",
      "34.8% finished\n",
      "35.7% finished\n",
      "36.7% finished\n",
      "37.7% finished\n",
      "38.6% finished\n",
      "39.6% finished\n",
      "40.6% finished\n",
      "41.5% finished\n",
      "42.5% finished\n",
      "43.5% finished\n",
      "44.4% finished\n",
      "45.4% finished\n",
      "46.4% finished\n",
      "47.3% finished\n",
      "48.3% finished\n",
      "49.3% finished\n",
      "50.2% finished\n",
      "51.2% finished\n",
      "52.1% finished\n",
      "53.1% finished\n",
      "54.1% finished\n",
      "55.0% finished\n",
      "56.0% finished\n",
      "57.0% finished\n",
      "57.9% finished\n",
      "58.9% finished\n",
      "59.9% finished\n",
      "60.8% finished\n",
      "61.8% finished\n",
      "62.8% finished\n",
      "63.7% finished\n",
      "64.7% finished\n",
      "65.7% finished\n",
      "66.6% finished\n",
      "67.6% finished\n",
      "68.6% finished\n",
      "69.5% finished\n",
      "70.5% finished\n",
      "71.5% finished\n",
      "72.4% finished\n",
      "73.4% finished\n",
      "74.4% finished\n",
      "75.3% finished\n",
      "76.3% finished\n",
      "77.3% finished\n",
      "78.2% finished\n",
      "79.2% finished\n",
      "80.2% finished\n",
      "81.1% finished\n",
      "82.1% finished\n",
      "83.0% finished\n",
      "84.0% finished\n",
      "85.0% finished\n",
      "85.9% finished\n",
      "86.9% finished\n",
      "87.9% finished\n",
      "88.8% finished\n",
      "89.8% finished\n",
      "90.8% finished\n",
      "91.7% finished\n",
      "92.7% finished\n",
      "93.7% finished\n",
      "94.6% finished\n",
      "95.6% finished\n",
      "96.6% finished\n",
      "97.5% finished\n",
      "98.5% finished\n",
      "99.5% finished\n",
      "\n",
      "RESULT:\n",
      "54.7% item have a repec handle\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(henningsMetadataFile):\n",
    "    apiToJson(url='http://api.econbiz.de/v1/search?q=source:econstor+identifier_url:pdf&secret=Z-8_uu&size=' + str(maxNumDocs) + '&fields=title,identifier_url,person,date,id,identifier_repec', cacheFile=henningsMetadataFile)\n",
    "\n",
    "data = readData(henningsMetadataFile)\n",
    "\n",
    "hasRepec = 0\n",
    "numDocs = len(data['hits']['hits'])\n",
    "lookup = determineRepecHandle_HenningsMethod()\n",
    "for i, item in enumerate(data['hits']['hits']):\n",
    "    try:\n",
    "        id = lookup(item['id'])\n",
    "    except:\n",
    "        # we don't care about any errors ;)\n",
    "        continue\n",
    "    if id != None:\n",
    "        hasRepec += 1\n",
    "    if i % 1000 == 0:\n",
    "        print \"{:.1f}% finished\".format((i/float(numDocs))*100)\n",
    "print \"\\nRESULT:\\n{:.1f}% item have a repec handle\".format((hasRepec/float(numDocs))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what is the intersection between both results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT:\n",
      "Wolfgang without Henning: 137\n",
      "Henning without Wolfgang: 27209\n",
      "Henning: 56624\n",
      "Wolfgang: 29552\n",
      "Wolfgang and Henning: 56761\n"
     ]
    }
   ],
   "source": [
    "data = readData('henningsMetadata.json') # picked hennings file randonly\n",
    "henningsMethod = determineRepecHandle_HenningsMethod()\n",
    "wolfgangsMethod = determineRepecHandle_WolfgangsMethod()\n",
    "\n",
    "henningsSet = set()\n",
    "wolfgangsSet = set()\n",
    "numDocs = len(data['hits']['hits'])\n",
    "\n",
    "for i, item in enumerate(data['hits']['hits']):\n",
    "    try:\n",
    "        eBId = item['id']\n",
    "    except TypeError:\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            henningsId = henningsMethod(eBId)\n",
    "            wolfgangsId = wolfgangsMethod(eBId)\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            if henningsId != None:\n",
    "                henningsSet.add(henningsId)\n",
    "            if wolfgangsId != None:\n",
    "                wolfgangsSet.add(wolfgangsId)\n",
    "        \n",
    "print '\\nRESULT:\\nWolfgang without Henning: {}\\nHenning without Wolfgang: {}' \\\n",
    ".format(str(len(wolfgangsSet.difference(henningsSet))), str(len(henningsSet.difference(wolfgangsSet))))\n",
    "print'Henning: {}\\nWolfgang: {}\\nWolfgang and Henning: {}'.format(str(len(henningsSet)), str(len(wolfgangsSet)), str(len(wolfgangsSet.union(henningsSet))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RePec crawler\n",
    "The third way to obtain RePec handles is based on RePec's search engine called [IDEAS](https://ideas.repec.org/). It's obviouly build to interface humans, but it can also be used by robots, as we do it.\n",
    "This methods produces a RePec handle given the title of a document.\n",
    "It mimics a human user that queries the search engine with the title of a document. It \"clicks\" the first match (if there is a match) and extract the desired information from the detail page.\n",
    "It should be noted that this method is fragile and error-prone. In case the layout of the website changes, the corresponding xPath's need to be adapted appropriately. Moreover, this method can produce false values. There is guarantee that the RePec handle is the one you where looking for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib2\n",
    "import json\n",
    "from lxml import etree\n",
    "import unicodedata\n",
    "\n",
    "def determineRepecHandle_ideasCrawler(query):\n",
    "    numResults = '1'\n",
    "    xpathFirstResult = '//*[@id=\"content-block\"]/dl/dt/a'\n",
    "    xpathRepecHandle = '//*[@id=\"biblio-body\"]/table/tr[4]/td[2]'\n",
    "    \n",
    "    \n",
    "    # normalize text \n",
    "    #    replaces e.g. Ã¤ with a\n",
    "    unicodedata.normalize(\"NFKD\", query).encode(\"ascii\", \"ignore\").decode(\"utf8\")\n",
    "    #    remove everthing that's not alphanumeric\n",
    "    query = re.sub(r'[^A-Za-z0-9 ]*', '', query)\n",
    "    \n",
    "    # percentage encoding\n",
    "    queryPercentageEncoded = urllib.quote_plus(query)\n",
    "    htmlParser = etree.HTMLParser()\n",
    "    # Request result list\n",
    "    ideasHdl = urllib2.urlopen('http://ideas.repec.org/cgi-bin/htsearch?ul=&q=' + queryPercentageEncoded + '&cmd=Search%21&wf=4BFF&s=R&dt=range&db=&de=&m=all&fmt=long&sy=1&ps=' + numResults)\n",
    "    # parse received page\n",
    "    tree = etree.parse(ideasHdl, htmlParser)\n",
    "    # find first match in result list\n",
    "    match = tree.xpath(xpathFirstResult)\n",
    "    \n",
    "    # is there a match?\n",
    "    if len(match) > 0:\n",
    "        urlsDetailPages = match[0].values()\n",
    "    else:\n",
    "        raise Exception('Your query produced no result.')\n",
    "    \n",
    "    for url in urlsDetailPages:\n",
    "        if validateURL(url):\n",
    "            detailPageHdl = urllib2.urlopen(url)\n",
    "            detailsPageTree = etree.parse(detailPageHdl, htmlParser)\n",
    "            match = detailsPageTree.xpath(xpathRepecHandle)\n",
    "            if len(match) > 0:\n",
    "                return match[0].text\n",
    "\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build a function that combines Wolfgang's and Henning's methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetchRepecHandlerByEBId():\n",
    "    henningsMethod = determineRepecHandle_HenningsMethod()\n",
    "    wolfgangsMethod = determineRepecHandle_WolfgangsMethod()\n",
    "    \n",
    "    def lookup(id):\n",
    "        # hennings method\n",
    "        repecHdl = henningsMethod(id)\n",
    "        if repecHdl != None:\n",
    "            return repecHdl\n",
    "        # wolfgangs method\n",
    "        repecHdl = wolfgangsMethod(id)\n",
    "        if repecHdl != None:\n",
    "            return repecHdl\n",
    "\n",
    "        return None\n",
    "    \n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "Now that we have all components in place, let's glue them together. \n",
    "What the code roughly does, for every item in the meta data file, is:\n",
    "1. Download the corresponding PDF-file\n",
    "2. Save the corresponding meta data into a separate file\n",
    "3. Determine the RePec handle\n",
    "  1. Try Henning's method\n",
    "  2. If the previous method failed, try Wolfgang's method\n",
    "  3. If the previous method failed, try scraping RePec\n",
    "4. Fetch citation count informaton form citEc and save it to the file create in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:845337718.pdf skipped. already downloaded.\n",
      "INFO:root:Unfortunately you have been blacklisted by the citec-API. Stopping execution.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "citec service unavailable",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m citec service unavailable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib2\n",
    "import logging\n",
    "import re\n",
    "import xmltodict\n",
    "import sys\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "wd = os.getcwd() + os.sep + '..' + os.sep + 'data'\n",
    "metadataFile = henningsMetadataFile\n",
    "failedPath = 'failedToDownload.json'\n",
    "lookupRepecHdl = fetchRepecHandlerByEBId()\n",
    "\n",
    "with open(metadataFile, \"r\") as data_file:\n",
    "    data = json.load(data_file)\n",
    "    if data.has_key(\"hits\") and data[\"hits\"].has_key(\"hits\"):\n",
    "        data = data[\"hits\"][\"hits\"]\n",
    "    else:\n",
    "        raise Exception(\"unknown Datastructure\")\n",
    "\n",
    "# create directories if not existing\n",
    "pdfDir = wd + os.sep +  u'pdf'\n",
    "jsonDir = wd  + os.sep + u'json'\n",
    "failDir = wd + os.sep + os.sep + u'failed'\n",
    "for f in (pdfDir, jsonDir, failDir):\n",
    "    mkDir(f)\n",
    "    \n",
    "u = \"\"\n",
    "failedDownloads = []\n",
    "for item in data:\n",
    "    url = item[\"identifier_url\"][0]\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    \n",
    "    # download the pdf file\n",
    "    try:\n",
    "        if not os.path.exists(pdfDir + os.sep + filename):\n",
    "            u = urllib2.urlopen(url)\n",
    "            with open(pdfDir + os.sep + filename, 'w') as f:\n",
    "                f.write(u.read())\n",
    "            logging.log(logging.INFO, filename + \" successfully downloaded.\")\n",
    "        else:\n",
    "            logging.log(logging.INFO, filename + \" skipped. already downloaded.\")        \n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.log(logging.INFO, url + \" couldn't be opened.\") \n",
    "        failedDownloads.append(item)\n",
    "        logging.error(logging.ERROR, e)\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        # write meta data to json file\n",
    "        with open(jsonDir + os.sep + filename + '.json', 'w+') as f:\n",
    "            try:\n",
    "                itemFromFile = json.load(f)\n",
    "            except ValueError:\n",
    "                itemFromFile = {}\n",
    "            \n",
    "        if itemFromFile.has_key('citedBy') and \\\n",
    "           itemFromFile.has_key('cites'):\n",
    "            logging.log(logging.INFO, filename + u'.json skipped. Has citations counts already')\n",
    "            continue\n",
    "        else:\n",
    "            citeCount = None\n",
    "            # try to obtain repec handle\n",
    "            repecHdl = lookupRepecHdl(item['id'])\n",
    "            \n",
    "            if repecHdl == None:\n",
    "                # no handle so far. maybe we can find one on repec\n",
    "                title = \"\"\n",
    "                for s in item['title']:\n",
    "                    title += s + ' '\n",
    "                title = title.strip()\n",
    "                \n",
    "                try:\n",
    "                    repecHdl = determineRepecHandle_ideasCrawler(title)\n",
    "                except Exception:\n",
    "                    logging.log(logging.INFO, \"Couldn't obtained RePec Handle for \" + title)\n",
    "                \n",
    "                if repecHdl != None:\n",
    "                    logging.log(logging.INFO, \"Obtained RePec Handle from ideas:\" + unicode(repecHdl))\n",
    "            \n",
    "            if repecHdl != None:\n",
    "                # Fetch citation count figures\n",
    "                try:\n",
    "                    citeCount = citationCount(repecHdl)\n",
    "                except IOError as e:\n",
    "                    logging.log(logging.INFO, \"Unfortunately you have been blacklisted by the citec-API. Stopping execution.\")\n",
    "                    sys.exit('citec service unavailable')\n",
    "                \n",
    "            if citeCount == None:\n",
    "                citeCount = {'citedBy': None, 'cites': None}\n",
    "                \n",
    "            itemFromFile.update(citeCount)\n",
    "            itemFromFile.update(item)\n",
    "            \n",
    "            with open(jsonDir + os.sep + filename + u'.json', 'w+') as jf:\n",
    "                json.dump(itemFromFile, jf)\n",
    "                logging.log(logging.INFO, filename + u'.json updated')\n",
    "\n",
    "if len(failedDownloads) > 0:\n",
    "    handler = open(failDir + os.sep + failedPath, \"w\")\n",
    "    handler.write(json.dumps(failedDownloads))\n",
    "logging.log(logging.INFO, \"Downloads complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
