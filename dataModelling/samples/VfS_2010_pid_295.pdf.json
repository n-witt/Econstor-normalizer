{"lang": "en", "identifier_url": ["http://www.econstor.eu/bitstream/10419/37467/1/VfS_2010_pid_295.pdf"], "title": ["Case-Based Belief Formation under Ambiguity"], "plaintext": "CaseBased Belief Formation under Ambiguity Jcid:252rgen Eichberger University of Heidelbergand Ani Guerdjikova Cornell University 23 October 2009This version: October 23, 2009Abstract In this paper, we consider a decisionmaker who tries to learn the distribution of outcomes frompreviously observed cases. For each observed database of cases the decisionmaker predicts aset of priors expressing his beliefs about the underlying probability distribution. We impose aversion of the concatenation axiom introduced in BILLOT, GILBOA, SAMET, AND SCHMEIDLER 2005 which ensures that the sets of priors can be represented as a weighted sum of theobserved frequencies of cases. The weights are the uniquely determined similarities betweenthe observed cases and the case under investigation. The predicted probabilities, however, mayvary with the number of observations. This generalisation of BILLOT, GILBOA, SAMET, ANDSCHMEIDLER 2005 allows one to model learning processes.JEL Classicid:2cation: D81, D83Keywords:casebased decision theory, ambiguity, multiple priors, learning, similarity Adress for Correspondence: Ani Guerdjikova, Cornell University, Department of Economics, 462 Uris Hall, ag 334cornell.edu. We would like to thank Larry Blume, Alain Chateauneuf, David Easley, Itzhak Gilboa, Joe Halpern, JeanYves Jaffray, Marcin Peski, Clemens Puppe and David Schmeidler as well as the participants of the Conferenceon Risk, Uncertainty and Decisions in Tel Aviv 2007, the ESEM in Budapest 2007 and seminar participants at Heidelberg, CergyPontoise, Cornell, Melbourne, and the University of Queensland for helpful comments andsuggestions. Financial support from the DFG SFB 504 and from the Center for Analytic Economics at Cornellis gratefully acknowledged.11 Introduction How will existing information incid:3uence probabilistic beliefs? How do data enter the inductiveprocess of determining a prior probability distribution? KEYNES 1921 discusses in greatdetail the epistemic foundations of probability theory. In particular, in Part III of his A Treatiseon Probability, he critically reviews most of the then existing inductive arguments for thisprobabilitygenerating process.Randomized statistical experiments with identically repeated trials represent an ideal method ofdata collection. In this case, decision makers can aggregate information directly into a probability distribution over unknown states. In most reallife decision problems, however, decisionmakers do not have available data derived from explicitly designed experiments with sufcid:2cientlymany identical repetitions. Usually, they face the problem to predict the outcome of an actionbased on a set of data which may be more or less adequate for the decision problem under consideration. This requires aggregating data with different degree of relevance. The casebaseddecision making approach of GILBOA AND SCHMEIDLER 2001 offers a systematic way todeal with this information aggregation problem: to evaluate an action, the outcomes of pastobservations are summed up, weighted by their perceived degree of relevance, their similarityto the current decision situation.In a recent paper, BILLOT, GILBOA, SAMET, AND SCHMEIDLER 2005, henceforth BGSS2005, show that, under few assumptions, a probability distribution over outcomes can be derived as a similarityweighted average of the frequencies of observed cases. Moreover, GILBOA,LIEBERMAN, AND SCHMEIDLER 2006 demonstrate how one can estimate the similarityweights from a given database.The casebased approach in BGSS 2005 associates a database with a single probability distribution. Furthermore, the probability distribution depends only on the frequency of observationsin the data, but not on the length of the database. This approach appears satisfactory if thedatabase is large and if the cases recorded in the database are clearly relevant for the decision 2problem under consideration. Indeed, BGSS 2005 note also that this approach  might be unreasonable when the entire database is very small. Specicid:2cally, if thereis only one observation,   However, for large databases it may be acceptable to assignzero probability to a state that has never been observed. BGSS 2005, p. 1129In particular, this approach restricts the decision maker to being a frequentist, but allows theweights assigned to the frequencies to depend on the perceived relevance of the cases. Twoimportant aspects of the decision situation are, however, neglected. First, even if the decisionmaker is able and willing to assign a probability distribution to each database, this distributionmight vary both with the frequency and the length of the database, as for instance in Bayesianupdating. Second, in the face of ambiguity, the decision maker might cid:2nd himself unable topinpoint a unique probability distribution.In this paper, we modify the approach of BGSS 2005 in two ways. First, we allow theprediction of the decision maker to depend both on the frequency and on the length of thedatabase. This allows us to capture the idea that, controlling for the frequency, longer databasescontain more precise information and to incorporate Bayesian updating as a special case of ouranalysis. Second, we allow the predictions to be represented by a convex set of probabilitydistributions to capture the idea that information can be ambiguous.OHAGAN AND LUCE 2003 describe the difcid:2culty of making and interpreting point predictions about probabilities as follows:The cid:2rst difcid:2culty we will face is that the expert will almost certainly not be an expertin probability and statistics. That means it will not be easy for this person to express herbeliefs in the kind of probabilistic form demanded by Bayes theorem. Our expert maybe willing to give us an estimate of the parameter, but how do we interpret this? Shouldwe treat it as the mean or expectation of the prior distribution, or as the median of thedistribution, or its mode, or something else?   We could go on to elicit from the expertsome more features of her distribution, such as some measure of spread to indicate hergeneral level of uncertainty about the true value of the parameter. pp. 6465.While decision makers might be unable to make point predictions about a prior distribution,they may be able to identify a range of possible probabilities, either directly as upper and lowerbounds of probabilities, or indirectly by a degree of concid:2dence expressed regarding a pointprediction. In the former case, a convex set of probabilities is suggested directly, in the latter 3case, one may view the set of probabilities as a neighborhood of an imprecise point prediction.The decision makers ambiguity can be related to the length of the database insufcid:2cient numberof observations or to the content of the data observations which do not exactly correspond tothe case for which a prediction has to be made. The cid:2rst type of ambiguity is relevant evenin the perfect case of randomized statistical experiments. Consider, e.g., a decision makerobserving random draws with replacement from an urn containing black and white balls inunknown proportions. After one white and two black balls have been drawn out of the urn,the decision maker might entertain a set of priors describing his beliefs about the constitutionof the urn. This set might include the observed frequencycid:0 1other distributions, e.g.,cid:0 13cid:1, but it might also contain 2cid:1, if the decision maker considers the number of observations to 3 ; 22 ; 1be insufcid:2cient to generate an exact prediction. This type of ambiguity will disappear as thenumber of observations grows.The second type of ambiguity arises in situations in which the data contains relevant, but notidentical cases to the one, for which a prediction has to be made. A case often discussed in theeconometrics literature is the one of missing variables, see MANSKI 2000, as well as Example 1 in GONZALES AND JAFFRAY 1998. For instance, medical studies often contain large setsof data, but fail to record potentially important characteristics, such as the gender of the patients.MANSKI 2000 argues that in this case, the probability distribution over outcomes cannot bepointidenticid:2ed. Hence, the data is consistent with a set of probability distributions which willbe nondegenerate even as the number of observations becomes large.Our model provides a representation which allows for both types of ambiguity. To obtain this,we modify the main axiom of BGSS 2005, Concatenation, by restricting it to databases ofequal length, i.e., thus controlling for the ambiguity resulting from insufcid:2cient amount of data.In order to establish a connection between the predictions for databases of different lengths, weintroduce an additional axiom cid:151 Learning. It captures the idea that, as information accumulates, the changes in the forecast caused by additional concid:2rming observations get smaller andsmaller. If the perceived ambiguity is due to a limited number of observations, the set of pre4dictions converges to a singleton. However, the axiom does not exclude the case of persistentambiguity, when predictions converge to a nondegenerate set of beliefs. This distinction between ambiguity which vanishes with a sufcid:2ciently large number of observations and ambiguitywhich remains for any number of observations corresponds to a similar distinction in EPSTEINAND SCHNEIDER 2007.Despite these modicid:2cations, our approach obtains a similarity function which is unique andindependent of the content and the size of the databases. This property is a central featureof the representation in BGSS 2005. Hence, as in BGSS 2005, frequentism and kernelclassicid:2cation represent special cases of our representation. However, our approach captures abroader scope of rules, including Bayesianism and full Bayesian updating on a set of priors.Moreover, it allows us to model persistent ambiguity arising from missing or inadequate data.There exist several approaches as to how a decision maker, whose forecast consists of a setof probability distributions, can select a prior from this set 1. Among these, the maxmin rulesuggested by GILBOA AND SCHMEIDLER 1989 and MANSKI 2000 is by far the mostpopular. It selects the probability distribution which results in minimal expected utility for thespecicid:2c action. Our approach allows us to establish a connection between the set of priorsand the selection of a single probability distribution used to evaluate a specicid:2c act. We showthat whenever the sets of priors which the decision maker associates with different databasessatisfy the axioms of our model, so do the probability distributions determined according to themaxmin rule.As in BGSS 2005, the question remains open which decision criterion one should use giventhe decision makers beliefs. In order to obtain a decision rule together with a multiple priorrepresentation one may embed these ideas in a behavioral model in the spirit of GILBOA,SCHMEIDLER, AND WAKKER 2002 or derive decision criteria recid:3ecting degrees of optimismor pessimism in the face of ambiguity as in the work of COIGNARD AND JAFFRAY 1994 and GONZALES AND JAFFRAY 1998. We propose such a behavioral approach in EICHBERGER1 For instance, a Bayesian might assign a prior on the set of possible probability distributions and takeexpectations with respect to this prior. Alternatively, one can use the center of the set of probability distributionsthe Steiner point as a focal probability distribution, see GAJDOS, HAYASHI, TALLON, AND VERGNAUD 2007.5AND GUERDJIKOVA 2008. We believe, however, that a characterization of the mapping Hfrom databases to probabilities over outcomes is desirable in its own right. It opens up the possibility to study the optimal use of data for the derivation of a set of prior distributions and tomodel databased learning rules.The remainder of the paper is organized as follows. Section 2 reviews the related literature.Section 3 outlines the model and Section 4 provides some motivating examples. In Section 5,we state the axioms. Section 6 presents the main result. In Section 7, we collect some exampleswhich illustrate our approach and show that it is compatible with an array of popular statisticalmethods. Section 8 concludes the paper. All proofs are collected in the Appendix.2 Related Literature There are several ways to model ambiguity of a decision maker in the literature. A representation of ambiguous beliefs by means of capacities was introduced by SCHMEIDLER 1989.For convex capacities, this approach coincides with the multiple prior approach advanced in GILBOA AND SCHMEIDLER 1989. BEWLEY 1986 derives a set of probability distributions from incomplete preferences. These multipleprior approaches were developed furtherby GHIRARDATO, MACCHERONI, AND MARINACCI 2004 and Chateauneuf, Eichberger,and Grant 2007. KLIBANOFF, MARINACCI, AND MUKERJI 2005 model ambiguity attitudes by a secondorder probability distribution over a set of probability distributions. All thesemultipleprior approaches represent ambiguity by a set of probability distributions which a decision maker considers when evaluating her expected utility. In the spirit of these models, wemodel ambiguity by a set of probability distributions over outcomes. The degree of ambiguitycan be measured by set inclusion. The smaller the set of probability distributions over outcomes,the less ambiguous the prediction.In GILBOA AND SCHMEIDLER 1989 the set of priors is purely subjective. In contrast, several recent papers, AHN 2008, GAJDOS, HAYASHI, TALLON, AND VERGNAUD 2007,STINCHCOMBE 2003, provide a framework to analyze decisions in situations in which the 6set of priors is objectively given. This allows them to distinguish objectively given Knightian uncertainty from the subjective attitude towards ambiguity. In our framework, a similardistinction is achieved differently. The decision maker associates with each database a set ofprobability distributions, which take into account both the objective information contained inthe data i.e. the nature and frequency of cases observed, as well as the number of observationsand the subjective degree of ambiguity. Thus, our approach provides a method to characterizesets of subjective priors related to the datagenerating process.MARINACCI 2002 and EPSTEIN AND SCHNEIDER 2007 analyze statistical learning in thecontext of ambiguity. MARINACCI 2002s work provides conditions under which ambiguityalmost surely fades away as data accumulate. In contrast, EPSTEIN AND SCHNEIDER 2007distinguish two types of scenarios: one in which it is possible to learn the objective probabilitydistribution and another where ambiguity is persistent. They study the effect of prior information on the learning process in the context of statistical experiments in the spirit of ELLSBERG1961. If information about the colors of a given number of balls in an urn is obtained fromsampling with replacement, such learning will reveal the proportions of colors in the longrun. In contrast, if the composition of the color in the urn is changing over time, e.g., in Scenario 3 of EPSTEIN AND SCHNEIDER 2007 p. 1279 because a certain number of ballsis replaced by an administrator in every period, then learning by sampling with replacementcannot reveal the true proportions of colors and ambiguity will prevail even in the long run.EPSTEIN AND SCHNEIDER 2007 show also how these types of ambiguity induce differentinvestment behavior in a portfolio choice model.Our framework uses the notion of similarity to distinguish between controlled statistical experiments, and situations in which relevant, but not completely identical cases have been observed.If the observed cases are identical to the case under consideration, as in a controlled statisticalexperiment, e.g., in Scenarios 1 and 2 of EPSTEIN AND SCHNEIDER 2007, then the decisionmaker will be able to learn the objective probability distribution satisfying the ergodicity property. When, however, the observed cases are distinct from the situation under consideration, as 7in Scenario 3, then ambiguity may persist in form of a limit set of probabilities, even if a largenumber of data has been collected. Yet, sampling may still provide information, though thedecision maker has to judge its relevance based on some presumption about the administratorsbehavior.GONZALES AND JAFFRAY 1998 model preferences over Savagetype acts for a given set of,possibly imprecise, data. They derive a representation of preferences in form of a linear combination of the maximal and the minimal potential outcome of an act and its expected utilitywith respect to the observed frequency of states. The weights attached to the maximal and minimal outcomes can be interpreted as degrees of optimism and pessimism. They decrease overtime relative to the weight attached to the expected utility part of the representation. Becauseobservations may be imprecise a decision maker associates with a set of data a set of priorscentered around the observed frequency. The size of the set of probabilities depends negativelyon the amount of data. While we do not derive a decision rule from behavior, our approachencompasses a richer class of situations which is not restricted to the case of controlled statistical experiments considered in both COIGNARD AND JAFFRAY 1994 and GONZALES ANDJAFFRAY 1998.3 The Model The basic element of a database is a case which consists of an action taken and the outcomeobserved together with information about characteristics which the decision maker considersas relevant for the outcome. We denote by X a set of characteristics, by A a set of actions,and by R a set of outcomes. All three sets are assumed to be cid:2nite. A case c x; a; r is anelement of the cid:2nite set of cases C X cid:2 A cid:2 R. A database of length T is a sequence ofcases indexed by t 1:::T :D x 1; a 1; r 1 ; :::; x T ; a T ; r T 2 C T .The set of all databases of length T is denoted by DT : C T . Finally, D : Tcid:211set of databases of arbitrary length.DT denotes the 8Consider a decision maker with a given database of previously observed cases, D, who wantsto evaluate the uncertain outcome of an action a 0 2 A given relevant information about theenvironment described by the characteristics x 0 2 X: Based on the information in the database D; the decision maker will form a belief about the likelihood of the outcomes. We will assumethat the decision maker associates a set of probability distributions over outcomes R,H D j x 0; a 0 cid:26 cid:1j Rjcid:01;with the action a 0 in the situation characterized by x 0 given the database D 2 D:Formally, H : Dcid:2Xcid:2A ! cid:1j Rjcid:01 is a correspondence which maps Dcid:2Xcid:2A into nonempty,compact and convex subsets of cid:1j Rjcid:01: As usual, the convex combination of two sets of probability distributions H and H0 is decid:2ned by cid:21H1 cid:0 cid:21 H0 fcid:21h 1 cid:0 cid:21 h 0 j h 2 H and h 0 2 H0g.Elements of this set are denoted by h D j x 0; a 0 and we write hr D j x 0; a 0 for the probability assigned to outcome r by the probability distribution h D j x 0; a 0.We interpret H D j x 0; a 0 as the set of probability distributions over outcomes which the decision maker takes into consideration given the database D.4 Motivating Examples The following examples illustrate the broad cid:2eld of applications for this framework. They willalso highlight the important role of the decision situation x 0; a 0:The cid:2rst example is borrowed from BGSS 2005.Example 4.1 Medical treatment A physician must choose a treatment a 0 2 A for a patient. The patient is characterized by aset of characteristics x 0 2 X, e.g., blood pressure, temperature, gender, age, medical history,etc. Having observed the characteristics x 0, the physician evaluates a treatment a 0 based onthe assessment of the probability distribution over outcomes r 2 R. A set of cases D observed 2in the past may serve the physician in this assessment of probabilities over outcomes.2 The observations of cases are not restricted to personal experience. Published reports in scienticid:2cjournals, personal communications with colleagues and other sources of information may also provideinformation about cases.9A case c xt; at; rt is a combination of a patient ts characteristics xt, treatment assignedat and outcome realization rt recorded in the database D: Given the database D, the physicianconsiders a set of probabilities over outcomes, H D j x 0; a 0 cid:26 cid:1j Rjcid:01, as possible. Theseprobability distributions represent beliefs about the likelihood of possible outcomes after choosing a treatment a 0 for the patient with characteristics x 0:In general, the physician will form his beliefs based on cases in which characteristics potentially different from x 0 and actions potentially different from a 0 were observed. E.g., OHAGANAND LUCE 2003PP. 6264 discuss how information from different studies about the effectiveness of similar, but not identical, drugs can be combined into a prior distribution.Two problems can prevent the physician from specifying a unique probability distribution fora specicid:2c treatment. First, he might have few observations, and, therefore, doubt that the observed frequencies are representative of the population as a whole. Second, the observationsmight not be identical to the case at hand e.g., the physician might have a vast amount of dataon patients with cid:3u symptoms, which allows him to evaluate different treatments, however, hemight consider all these cases to be of only limited relevance when faced with the symptomsof swine cid:3u. While in the cid:2rst situation, collecting more data of the same type would reducethe ambiguity, in the second, the ambiguity is due to incomplete understanding of the relationbetween different cases.Consider, e.g. a situation in which some of the characteristics contained in X were not recorded.As in MANSKI 2000, suppose that in a study that contains the outcomes of a specicid:2c treatment, the gender of the patient is not recorded 3. Suppose that the treatment resulted in successfor exactly 50% of all cases. A physician who has to assign a treatment to a woman will notbe able to infer from the database which of the following scenarios corresponds to the truth:i the treatment is always effective for men, but never for women; ii the treatment is alwayseffective for women, but never for men; iii the treatment is successful in 50% of cases, forboth genders of course many more intermediate cases are possible. Even after observing avery large database, the physician will be completely ignorant of the probability of success, andhis prediction will be represented by the interval 0; 1. Here, the fact that the cases in the data 3value of cid:22xi to denote that characteristic i has not been recorded for the specicid:2c observation.If patients characteristics are represented by a vector x cid:0x 1:::x Icid:1, we could use a default 10are not completely identical to the case at hand gives rise to sets of probability distributions.cid:4As a second application we will consider classic statistical experiments where the decisionmaker bets on the color of the ball drawn from an urn.cid:15cid:15cid:15Example 4.2 Lotteries Consider three urns with black and white balls. There may be different information about thecomposition of these urns. For example, it may be known thatthere are 50 black and 50 white balls in urn 1,there are 100 black or white balls in urn 2,there is an unknown number of black and white balls in urn 3.We will encode all such information in the number of the urn, x 2 X f 1; 2; 3g.In each period a ball is drawn from one of these urns. A decision maker can bet on the colorof the ball drawn, f B; Wg: Assume that a decision maker knows the urn x 0 from which the ballis drawn, when he places his bet a 0. An action is, therefore, a choice of lottery a 2 A :f 1B0; 1W 0g, with the obvious notation 1E0 for a lottery which yields r 1 if E occurs andr 0 otherwise.Suppose the decision maker learns after each round of the lottery the color of the ball that wasdrawn. Since there are only two possible bets a 1B0 or a 0 1W 0 we can identify casesc x; a; r by the urn x and the color drawn B or W: Hence, there are only six cases C f1; B; 1; W ; 2; B; 2; W ; 3; B; 3; W g:Note that for a given urn x, the observation of a case, allows the decision maker to observethe outcome of the actually chosen action, but also to infer the counterfactual outcome of thelottery he did not choose. This is a specicid:2c feature of this example, which distinguishes it from Example 4.1.Suppose that, after T rounds, the decision maker has a database D 1; B; 3; W ; :::; 2; B 2 C T .With each database D, one can associate a set of probability distributions over the color ofthe ball drawn f B; Wg or, equivalently, over the payoffs f 1; 0g given a bet a: Suppose a decision maker with the information of database D learns that a ball will be drawn from urn 211and places the bet a 0 1B0, then he will evaluate the outcome of this bet based on the set ofprobability distributions H D j 2; a 0 : This set should recid:3ect both the decision makers information contained in D and the degree of concid:2dence held in this information. For example, asin statistical experiments, the decision maker could use the relative frequencies of B and Wdrawn from urn 2 in the database D and ignore all other observations in the database. Depending on the number of observations of draws from urn 2, say T 2, recorded in the database D of length T , the decision maker may feel more or less concid:2dent about the accuracy of theserelative frequencies. Such ambiguity could be expressed by a neighborhood of the frequenciesf D2; B; f D2; W of black and white balls drawn from urn 2 according to the records inthe database D: The neighborhood will depend on the number of relevant observations T 2,e.g.,H D j 2; a 0 cid:26h W ; h B 2 cid:11 j f D2; W cid:01This set of probabilities over outcomes H D j 2; a 0 may shrink with an increasing number ofrelevant observations. cid:4T 2 cid:20 h W cid:20 f D2; W T 2cid:27 .Example 4.2 illustrates how information in a database may be used and how one can modelambiguity about the probability distributions over outcomes. In this example, we assumed thatthe decision maker ignores all observations which do not relate to urn 2 directly. If there islittle information about draws from urn 2, however, a decision maker may also want to considerevidence from urn 1 and urn 3, possibly with weights recid:3ecting the fact that these cases are lessrelevant 4 for a draw from urn 2. The representation derived in the next section allows for thispossibility.5 Axioms In this section, we will take the decision situation x 0; a 0 as given. We will relate the frequencies of cases in a database D 2 DT ,f D c : jfct 2 D j ct cgj;4 Part III of KEYNES 1921 provides an extensive review of the literature on induction from casesto probabilities.12TThe set of probability distributions over outcomes P cjx 0;a 0to sets of probabilities over outcomes HD j x 0; a 0. In particular, let HT D j x 0; a 0 be therestriction of HD j x 0; a 0 to databases of length T . We will impose axioms on the set ofprobability distributions over outcomes H D j x 0; a 0 which will imply a representation of thefollowing type: for each T cid:21 2 and each database of length T ,HT D j x 0; a 0 cid:26Pc 2D s c j x 0; a 0 f D c pc Pc 2D s c j x 0; a 0 f D cT times! is observed. This set may depend on the numberof observations. It may be large for small numbers and may shrink as more concid:2rming databecome available. The weighting function s c j x 0; a 0 represents the relevance of a case c forthe current situation x 0; a 0 and can be interpreted as the perceived similarity between c andx 0; a 0.The axioms we introduce below imply that s cid:1 j x 0; a 0 is unique up to a normalization andare determineddoes not depend on T , while the sets of probability distributions P cjx 0;a 0maker when the database D c:::czdenotes the beliefs of the decisioncid:27 .TTj pc T 2 P cjx 0;a 0TTuniquely. This result generalizes the main theorem of BGSS 2005 to the case in which beliefsdepend on the number of observations and can be expressed as sets of probability distributionsover outcomes.In the following discussion, x 0; a 0 is assumed constant. Hence, we suppress notational reference to it and write HD, hD, P c T and s c instead of H D j x 0; a 0, h D j x 0; a 0, P cjx 0;a 0and s c j x 0; a 0, respectively. It is important to keep in mind, however, that all statements ofaxioms and conclusions do depend on the relevant reference situation x 0; a 0. In particular,the similarity weights, deduced below, measure similarity of cases relative to this reference Tsituation.In order to characterize the mapping HD we will impose axioms which specify how beliefsover outcomes change in response to additional information. In general, it is possible that theorder in which data become available conveys important information. We will abstract herefrom this possibility and assume that only data matter for the probability distributions overoutcomes.13Axiom A1Invariance Let cid:25 be a onetoone mapping cid:25 : f 1:::Tg ! f 1:::Tg, then Hcid:16ctTt1cid:17 Hcid:16cid:0ccid:25tcid:1Tt1cid:17 .According to Axiom A1, the set of probability distributions over outcomes is invariant withrespect to the sequence in which data arrive. Hence, each database D is uniquely characterizedby the tuple f D; T , where f D 2 cid:1j Cjcid:01 denotes the vector of frequencies of the cases c 2 Cin the database D and T is the length of the database, i.e. D 2 DT .Remark 5.1 By Axiom A1, we can identify every database D ctTsponding multiset 5 nctTt1o, in which the number of appearances of every case c exactlycorresponds to the number of its appearances in D. We will denote the database and its corresponding multiset by the same letter. In particular, when we write D D0, we mean equalityof the multisets corresponding to the databases D and D0.t1 with the correIn line with BGSS 2005, we call the combination of two databases a concatenation.Decid:2nition 5.1 Concatenation For any T , T 0 2 N, and any two databases D ctTD0 c 0tT 0t1, the databaset1 andis called the concatenation of D and D0.D cid:14 D0 cid:16ctTt1 ; c 0tT 0t1cid:17By Axiom A1, concatenation is a commutative operation on databases. The following notational conventions are useful.Notation Dk D cid:14 ::: cid:14 Dza database consisting of ktimes the same case c can be written as ck:ktimesdenotes k concatenations of the same database D: In particular,Imposing the following Concatenation Axiom, BGSS 2005 obtain a characterization of afunction h mapping D into a single probability distribution over outcomes.Axiom BGSS 2005Concatenation For every D, D0 2 D,hD cid:14 D0 cid:21hD 1 cid:0 cid:21hD05 On multisets see, e.g., BLIZARD 1988.14for some cid:21 2 0; 1:This axiom can be easily adapted to our framework:Axiom BGSS cid:150 Multiple PriorsConcatenation with multiple priors For every D, D0 2 DH D cid:14 D0 cid:21H D 1 cid:0 cid:21 H D0for some cid:21 2 0; 1.Both versions of the axiom imply that, for any k, the databases D and Dk map into the sameset of probability distributions over outcomes, HD HDk: Hence, two databases D cand D0 c 10000 will be regarded as equivalent. This seems counterintuitive. Ten thousandobservations of the same case c x; a; r are likely to provide stronger evidence for the outcome r in situation x; a than a single observation. Suppose, that we restrict the prediction tobe singlevalued, e.g., because the decision maker is a Bayesian. Unless, the decision makersprior assigns a probability of 1 to outcome r, this decision maker will assign a higher probabilityto outcome r under D0 than under D. If, in contrast, the decision maker considers the situationto be ambiguous, we could argue that the database c 10000 provides a strong evidence for a probability distribution concentrated on the outcome r; hrc 10000 1. Based on a single observationx; a; r, however, it appears quite reasonable to consider a set of probability distributions Hcwhich also contains probability distributions hc with hr 0 c 2 0,1 for all r 0: In particular,based on the information contained in D c, a decision maker may not be willing to excludethe case of all outcomes being equally probable, i.e., hD with hr 0D 1for all r 0 2 R: Itj Rjappears perfectly reasonable to include h in H c but not in H c 10000.Since we would like to capture the fact that concid:2dence might increase as the number of observations grows, we cannot simply apply the Concatenation Axiom of BGSS 2005 to concatenations of arbitrary databases D and D0. Restricting the axiom to databases with equal lengthwill provide sufcid:2cient cid:3exibility for our purpose.To illustrate the idea, consider two cases c 1 and c 2 and databases with two observations ofthese cases, say D1 c 1; c 1; D2 c 2; c 2; and F c 1; c 2: Due to Axiom A1, one 15can write these databases in terms of frequencies and numbers of observations as F f F ; 2;D1 f D1; 2; and D2 f D2; 2: Since D1cid:14D2 c 1; c 1; c 2; c 2 F cid:14F holds, the frequencyof cases in F must be a mixture of the frequencies of D1 and D2;f F 12f D1 12f D2:Whatever the predictions HD1 and HD2; which the decision maker expresses based on thedatabases D1 and D2; the prediction for the database F c 1; c 2 should in some sense liebetween HD1 and HD2: Formally, we will require the existence of a cid:21 2 0; 1 such thatcid:21H D1 1 cid:0 cid:21H D2 H F .Axiom A2 generalizes this idea: for any n databases of equal length T that can be concatenated to an nfold of a database F of length T , we postulate that any probability distributionover outcomes predicted on the basis of database F can be expressed as a convex combinationof probability distributions over outcomes associated with the databases Di.Axiom A2Concatenation restricted to databases of equal length Consider databases F 2DT and D1:::Dn 2 DT for some n 2 N, such that D1cid:14 :::cid:14 Dn F n: Then, there exists a vectorcid:21 2 int cid:1ncid:01 such thatcid:21i H Di H F .n Xi1In spirit, Axiom A2 is very similar to the Concatenation Axiom introduced by BGSS 2005.It has the following intuitive interpretation 6: if a decision maker cannot exclude a certain probability distribution h after observing the evidence in any of the databases D1:::Dn, then heshould not be able to exclude it after observing the evidence in a database of the same length,F , the frequency of which is a mixture of the frequencies of D1:::Dn. The main difference tothe Concatenation Axiom of BGSS 2005 is that we restrict the axiom to databases of equallength.6 Note that the Axiom does not have the following behavioral implication: if action a is preferred toa 0 under all databases D1:::Dn, then it is also preferred under F . To understand this, consider the case of n 2. Leta %D1 a 0 and a %D2 a 0. Suppose also that the evidence contained in database D1 is more relevant for a, while theevidence contained in D2 is more relevant for a 0. Suppose that, at the same time, the decision maker values a 0higher given the relevant evidence contained in D2 than he values a, given the relevant evidence for this action, D1.In this case, combining the evidence contained in the two databases D1 and D2 into F might lead to a reversal ofpreferences, i.e., a 0 %F a. The same argument applies also for the Concatenation Axiom of BGSS 2005.16The restriction to sets of equal length is important for our approach since databases with identical frequencies, but different length may give rise to different sets of probabilities over outcomes. In particular, depending on some learning rule e.g. full Bayesian updating, see Section 7, it may be reasonable to assume that the set of probabilities is nondegenerate, but convergestowards the observed frequency of outcomes as more observations of the same cases becomeavailable. Intuitively, the decision maker becomes more concid:2dent that the observed frequencies recid:3ect the actual datagenerating process for the database Dk1 than for Dk. In contrast,applying the Concatenation Axiom of BGSS 2005, we would have to conclude that for somecid:21 2 intcid:0cid:1kcid:1,Hcid:0Dk1cid:1 Hcid:0Dk cid:14 Dcid:1 cid:211HDk 1 cid:0 cid:211HD cid:211HDkcid:01 cid:212H D 1 cid:0 cid:211 cid:0 cid:212HDk1Xi1cid:21i H D H D .for all k 2 N: Thus, imposing BGSS 2005s Concatenation Axiom, the set of probabilitydistributions over outcomes would necessarily be independent of the number of observations.Our weaker Axiom A2, however, implies in this case only Pk1i1 cid:21i H D H D, which istrivially satiscid:2ed for any set D.Axiom A2 allows us to identify the similarity function. In general, however, similarity willdepend on the length of the database. In order to prevent this, we impose Axiom A3 Constant Similarity Consider the databases F 2 DT and D1:::Dn 2 DT for somen 2 N, such that D1 cid:14 ::: cid:14 Dn F n: If for some cid:21 2 int cid:1ncid:01,cid:21i Hcid:0Dkholds for some k 2 N, then it holds for any k 2 N.Independence of the similarity function from the number of observations is justicid:2ed if oneicid:1 Hcid:0F kcid:1n Xi1assumes that the similarity of cases is determined by some primitive knowledge about the cases,which is not based on the information contained in the database 7. We discuss this axiom and its 7 Compare also the discussion of structural priors in OHAGAN AND LUCE 2003 PP. 6768.17implications in Section 7.4.The following axiom requires learning processes to be stable. If the number of observationsof the same case c case increases, beliefs about the outcome of x 0; a 0 will react less to eachadditional observation and will eventually settle on a possibly singleton set of probabilitydistributions.Axiom A4 Learning For every c 2 C, the sequence of sets HTcid:0c Tcid:1 converges 8.We will use the notation H1 c for the limit of the sequence lim T!1 HTcid:0c Tcid:1. Since all sets HTcid:0c Tcid:1 are nonempty, compact and convex subsets of cid:1j Rjcid:01, the limit H1 c inherits theseproperties.Under Axiom A4, ambiguity may persist or vanish in the limit depending on the similarityof cases to the situation under consideration. E.g., if c represents a statistical experiment w.r.t.the action of interest, i.e., c x 0; a 0; r for some r 2 R, then it appears reasonable to assumethat H1 c fcid:14rg, where cid:14r is the Dirichlet distribution putting mass 1 on r. However, if cincludes the observation of an action distinct from a 0, say, a 0, there is no reason to suggest thatthe decision maker will be able to eliminate all ambiguity about the performance of action aeven after observing an incid:2nite sequence of realizations of a 0. Hence, in general, the limit setwill contain more than one element.The next axiom requires those elements of H1 cc 2C which are singletons or segments to benoncollinear.Axiom A5 Noncollinearity No three of the sets H1 c of dimension 0 or 1 are collinear.Axiom A5 replaces the Axiom Noncollinearity in BGSS 2005. While BGSS 2005 require that there are at least three noncollinear vectors in the set h DD2D, our restriction isimposed on the limit sets H1 c. To understand the restrictions imposed by A5, it is useful tocid:2rst look at its implications in the setting of BGSS 2005. If h is a function and the Concate8 For the decid:2nition of set convergence, see Decid:2nition 4.1 in ROCKAFELLAR AND WETS 2004. Sincethe sets Hcid:0c Tcid:1 are subsets of the j Rj cid:0 1dimensional simplex, it follows that in our model, thisnotion of convergence coincides with convergence with respect to the PompeiuHausdorff distance,see Example 4.13 in ROCKAFELLAR AND WETS 2004.18nation Axiom of BGSS is satiscid:2ed, we know that h 1 c exists and h 1 c hcid:0c Tcid:1 h cfor all c 2 C. This means that no three of the predictions related to the basic cases are collinear.Intuitively, this excludes the possibility that the set of basic cases C can be reduced by takingthe evidence from a given case c to be exactly equivalent to the evidence of a database containing observations of two different cases, c 0 and c 00 in a certain proportion. This requirement issatiscid:2ed, for controlled randomized experiments, i.e., for the cases of the type x 0; a 0; r, forwhich ambiguity vanishes and the limit prediction can be reasonably assumed to be cid:14r.However, ambiguity need not vanish for cases in which characteristics distinct from x 0 anda 0 have been observed. In this case, the only restriction imposed by Axiom A5 concernsthose sets H1 c which are segments. We require that they are not collinear to any othertwo segments or points in the set H1 cc 2C. No assumptions are imposed on those sets inH1 cc 2C with a dimension 2 or higher.6 Representation Theorem The following theorem guarantees a unique similarity function for databases of arbitrary length.Theorem 6.1 Let H be a correspondence H : D ! cid:1j Rjcid:01 the images of which are nonempty convex, and compact sets and which satiscid:2es the Axioms A4 Learning and A5 Noncollinearity. Let HT D be the restriction of H to DT . Then the following two statements areequivalent:i H satiscid:2es the Axioms A1 Invariance, A2 Concatenation restricted to databases ofequal length, and A3 Constant Similarity.ii There exists a unique, up to multiplication by a positive number, functionand a unique correspondences : C ! RP : f 2; 3:::g cid:2 C ! cid:1j Rjcid:0119HT D cid:26Pc 2C s c pc Pc 2C s c f D cT f D cTcid:27 .T 2 P cj pcsuch that for all T cid:21 2 and any D 2 DT ,The proof of the Theorem is relegated to the Appendix. Note, however, how the axioms affectthe representation. Axiom A1 ensures that the prediction associated with a database dependsonly on the number and the frequency of the observations, but not on the order in which theyarrive. Axiom A2 implies the existence of weights s T c such that the predicted probabilitydistributions associated with a database D 2 DT can be expressed as a weighted average ofthe predictions of the individual cases in this set. Without imposing further restrictions, theweights s T c will depend on the length of the database T and be nonunique. Axiom A3yields independence of the similarity weights of the number of observations. Axioms A4 andA5 ensure uniqueness of the representation.As in BGSS 2005, uniqueness cannot be guaranteed unless some of the predictions corresponding to cases in C are noncollinear. In contrast to the framework of BGSS 2005, inwhich the predictions from databases consisting of observations of a single case c are independent of the number of observations T; here the predictions depend on the number of observations. Hence, in order to deduce a unique similarity function, one could require noncollinearityfor every value of T . An alternative, and in our opinion more intuitive approach, which is chosen here, is to ensure that the predictions from databases of increasing length converge to somelimit set, Axiom A4 ; and to guarantee noncollinearity of limit sets which are singletons orsegments, Axiom A5 : Then, for sufcid:2ciently large databases, there exists a selection of at leastthree predictions which are not collinear and which allow us to identify the similarity functionuniquely.Note that replacing Axioms A2 and A3 by the Axiom BGSScid:150Multiple Priors in Theorem 6.1 would imply that the correspondence P ; and hence the predictions H D ; would be independent of the length of the database.For some applications, the forecast of a decision maker about the outcome of a given action maybe a unique probability distribution. Using the lowercase letter h to indicate the special case 20where H is a function rather than a correspondence, it is straightforward to rewrite our axiomsfor this special case.In particular, Axiom A4, Learning, now reduces to the requirementthat for each c 2 C, the sequence h Tcid:0c Tcid:1 converges to some probability distribution h 1 c.Noncollinearity, Axiom A5, now ensures that no three of these limit vectors will be collinear.Hence, we obtain the following corollary to Theorem 6.1:Corollary 6.2 Let h be a function h : D ! cid:1j Rjcid:01 which satiscid:2es Axioms A4 and A5 andlet h T D be the restriction of h to DT . Then the following two statements are equivalent:i h satiscid:2es the Axioms Invariance A1, Concatenation restricted to databases withequal length A2 and Constant Similarity A3.ii There exists a unique, up to multiplication by a positive number, functionand a unique functions : C ! Rsuch that for all T cid:21 2 and any D 2 DT ,p T : f 2; 3:::g cid:2 C ! cid:1j Rjcid:01h T D Pc 2C s c f D c pc Pc 2C s c f D cT.Allowing the predicted probability distribution to depend on the length of a database, Corollary 6.2 generalizes the result of BGSS 2005. The timedependency of this representation allowsus to model learning processes. For example, with increasing numbers of observations the predicted probability distribution may become less sensitive to new additional data. The followingsection provides examples from statistical models.7 Examples and Applications A special case of our approach are predictions based on homogenous databases which containthe same characteristics and actions in all observations. Hence, all data have the same similarity.Homogenous databases result typically from controlled statistical experiments. In this context,it appears natural to assume that ambiguity decreases as new data concid:2rm past evidence.21In this section we show by examples that several statistical procedures satisfy the axioms of Theorem 6.1. Moreover, we discuss situations where constant similarity appears natural andillustrate how our method can be used to select a probability distribution from a set of priors.7.1 Frequentism Consider a decision maker who observes the outcome of a statistical experiment, where the setof possible cases is given by C x 0; a 0; rr 2R. After observing a database D of length Tand frequency f, the decision makers beliefs about the outcome of action a 0 are described byh T D f. It is easy to check that this rule satiscid:2es all the axioms. This prediction rule hasthe special property that the prediction does not depend on the length of the database, but onlyon the observed frequency of cases.Should the set of cases include also pairs of characteristics and actions different from x 0; a 0then the decision maker predicts a probability distribution px;a;r over outcomes of action a 0 incircumstances x 0 the for each observed case x; a; r : The similarity weight s x; a; r describesthe relevance of this case x; a; r for the prediction about x 0; a 0. This is the case axiomatizedby BGSS 2005.7.2 Bayesianism and Full Bayesian Updating Bayesian updating is one of the most prominent statistical learning rules. Its generalizationto full Bayesian updating incorporates learning with multiple priors, see MARINACCI 2002.In both cases, predictions depend on the observed frequency as well as on the length of thedatabase. Hence, neither of these rules satiscid:2es the Concatenation Axiom formulated by BGSS2005. Here, we show that both Bayesianism and full Bayesian updating constitute specialcases of our approach.As in MARINACCI 2002, consider a decision maker who is trying to learn the probabilitydistribution over the outcomes in a statistical experiment where sampling takes place with replacement.The set of possible cases is given by C x 0; a 0; rr 2R. Let D be a database of length T: Then T f Dr is the number of observations of r in D: Suppose that the decision makers 22prior information is recid:3ected by an initial set of priors P, consisting of Dirichlet distributionson cid:1j Rjcid:01. Then P can be described by the strictly positive parameters of these distributions,cid:0cid:111; :::; cid:11j Rjcid:1. In particular, for a Dirichlet distribution with parameter cid:11k, the expectedprobability of outcome r in absence of any observations is given bydistributions H0 is given by:. The initial set of The Bayesian update of a Dirichlet probability distribution on cid:1j Rjcid:01 with parameterscid:0cid:111:::cid:11j Rjcid:1after observing a database D is another Dirichlet distribution with parameters cid:111Pj Rjk1 cid:11k; :::;cid:11r Pj Rjk1 cid:11kcid:11j Rj Pj Rjk1 cid:11k! jcid:0cid:111:::cid:11j Rjcid:1 2 P .cid:0cid:111 T f D r 1 ; :::; cid:11j Rj T f Dcid:0rj Rjcid:1cid:1 .Hence, full Bayesian updating on the set P implies that the decision maker updates each of thepriors according to the Bayesian rule,H D cid:111 T f D r 1Pj Rjk1 cid:11k T; :::;Pj Rjk1 cid:11k T ! jcid:0cid:111; :::; cid:11j Rjcid:1 2 P .cid:11j Rj T f Dcid:0rj Rjcid:1Standard Bayesian updating obtains as a special case where P is a singleton and H D is theprobability distribution obtained by Bayesian updating.Note that the order in which information arrives does not affect the posterior, hence AxiomA1 is satiscid:2ed. Furthermore, let F , D1:::Dn be databases of length T such that There are strictly positive coefcid:2cients cid:13ini1 summing to 1 such that F n D1 cid:14 ::: cid:14 Dn.f F cid:13if Di.n Xi1These cid:13ini1 are independent of T . Hence, we have; :::;H F cid:111 T f F r 1Pj Rjk1 cid:11k T cid:111 TPn Pj Rjk1 cid:11k Tcid:13i cid:111 T f Di r 1Pj Rjk1 cid:11k TPj Rjk1 cid:11k T ! jcid:0cid:111; :::; cid:11j Rjcid:1 2 H0cid:11j Rj T f Fcid:0rj Rjcid:1i1 cid:13if Dicid:0rj Rjcid:1cid:11j Rj TPn Pj Rjk1 cid:11k T ! jcid:0cid:111; :::; cid:11j Rjcid:1 2 H0 cid:11n T f Dicid:0rj Rjcid:1Pj Rjk1 cid:11k Tn Xi1cid:21i H Di ,with cid:21i : cid:13i for i 2 f 1:::ng. Full Bayesian updating, therefore, satiscid:2es Axiom A2. Since! jcid:0cid:111; :::; cid:11j Rjcid:1 2 H0i1 cid:13if Di r 1n Xi1; :::;; :::;23i1, and hence also cid:21ini1 ; do not depend on T , Axiom A3 is satiscid:2ed asthe coefcid:2cients cid:13inwell.Since the parameters cid:111:::cid:11n are strictly positive, it follows that all priors contained in H0assign strictly positive probabilities to all outcomes in R. Therefore, each of the sequences Hcid:16x 0; a 0; rTcid:17 will converge to the unit vector assigning probability 1 to outcome r. Hence,Axiom A4 ; is satiscid:2ed as well. Finally, since the limit sets coincide with the unit vectors incid:1j Rjcid:01, Axiom A5 holds.7.3 Kernel Density Classicid:2cation In contrast to the previous examples, kernel methods are not restricted to databases generatedfrom the same statistical experiment. Therefore, similarity plays a role. Consider the standardkernel density classicid:2cation model 9.There are j Rj classes and a set of objects each of which can be described by a vector of characteristics x 2 X. For instance, a physician might want to divide his patients into classesaccording to their reaction to a certain type of drug 10. The physician observes cases of the formc x; r, in which a patient of type x has been classicid:2ed into class r. The physician entertains a notion of closeness between the patients described by a similarity function s : C ! R.Suppose that the reaction of a patient of type x 0 has to be classicid:2ed. The information of thephysician is given by a database D of length T .The kernel density classicid:2cation proceeds as follows 11. Given the database D which containsthe cases in which patients of different types have been classicid:2ed, one needs to determine therelevance of these cases to the case at hand, i.e., the similarity of an observed patient x to x 0.Weighting the frequencies of cases in which outcome r has been observed by their similarities 9 See, e.g., HASTIE, TIBSHIRANI, AND FRIEDMAN 2001P. 184.10 The classicid:2cation may also be actiondependent, e.g., one might be interested in classifyingthe patients relative to their risk of contracting a specicid:2c desease, conditional on a treatment theyhave undergone e.g. vaccination.11 A similar problem showing that kernel density methods can be represented in terms of similarityweighted frequencies can be found in GILBOA 2009.24and normalizing, one obtains the probability that patient x 0 will show the reaction r,Prfr j x 0g Px 2X s x 0; x f D x; rPr 02RPx 2X s x 0; x f D x; r 0 Px;r 02Xcid:2R s x 0; x f D x; r 0 px;r 0 rPx;r 02Xcid:2R s x 0; x f D x; r 0,2with px;r cid:14r the Dirichlet distributions concentrated on r. Note that px;r does not dependon the length of the database T .Expression 2 implicitly assumes that, when collecting the data, no classicid:2cation errors haveoccurred. Under this assumption, it is sensible to exclude all cases in which a patient has beenassigned to a class different from r. Moreover, once a patient has been classicid:2ed with a reactionr, the decision maker trusts that this classicid:2cation is correct and assigns probability of 1 topatients of this type belonging to class r.In practice such assumptions are hard to justify, since classicid:2cations may be biased. Supposefor example, that the data come from classicid:2cations made by different experts. Each expertobserves only patients of a certain type and has the task to record their reaction. Assume that theexpert dealing with class x classicid:2es the patients correctly with probability 1 cid:0 cid:15x and assigns If mistakes acrossthem mistakenly with probabilityj Rjcid:01 to any of the remaining classes.experts are independent and if, in absence of better evidence, one assumes a uniform prior overcid:15xthe classes, then the physician will derive the following modicid:2ed probability distribution,Prfr j x 0g Px;r 02Xcid:2R s x 0; x f D x; r 0 px;r 0Px;r 02Xcid:2R s x 0; x f D x; r 0TTr;where the probability distributions px;rare now posterior distributions based on T observations,px;rTpx;rTr Prnr j x 0; x; rTo 1 cid:0 cid:15xT ;r 0 Prnr 0 j x 0; x; rTo 1 cid:0 1 cid:0 cid:15xTj Rj cid:0 1for all r 0 6 r. In this case, Axiom A1 trivially holds. We have already shown that Bayesianupdating implies Axiom A2 : Since the similarity function is independent of T , axiom A3T cid:14r.applies as well. It is straightforward to check that Axiom A4 holds, i.e., lim T!1 px;rAmbiguity may become relevant if the physician is uncertain about the magnitude of the mistake 25of the experts. For example, if he believes that the probability of misclassifying a type x lies inthe interval cid:15x; cid:22cid:15x cid:18 0; 1 ; he immediately has to deal with a set of probability distributionsP x;r,TP x;rT8:0B1 cid:0 1 cid:0 cid:15xTj Rj cid:0 1 1CA j cid:15x 2 cid:15x; cid:22cid:15x9;;1 cid:0 1 cid:0 cid:15xTj Rj cid:0 1; :::1 cid:0 1 cid:0 cid:15xTj Rj cid:0 1;1 cid:0 1 cid:0 cid:15xTj Rj cid:0 1:::; 1 cid:0 cid:15xTzrthpositionobtained by full Bayesian updating with respect to the probabilities in the set cid:15x; cid:22cid:15x. Full Bayesian updating is consistent with Axioms A2 and A3 : Since lim T!1 fcid:14rg,Axiom A4 holds.P x;rTHence, for an arbitrary database D of length T , the set of probability distributions HT D j x 0 Px;r 02Xcid:2R s x 0; x f D x; r 0 px;r 0Px;r 02Xcid:2R s x 0; x f D x; r 0TT 2 P x;r 0j px;r 0T3describes the ambiguous beliefs of the physician about the classicid:2cation of the patient of typex 0 given the observations in D. Note that ambiguity vanishes, as more data become availableand the physician learns the true classicid:2cation scheme.7.4 Constant Similarity So far, we implicitly assumed that the similarity function is independent of the length and content of the observed data. In statistical analysis, however, the kernel width is usually chosendepending on the size of the database. The set of cases considered to be relevant for the classicid:2cation of a case x 0 shrinks as the number of cases increases. In our model, this corresponds toa similarity function s T x 0; x which depends on T . With this modicid:2cation, the representationin 3 would satisfy Axioms A1, A2 and A4, but would violate Axiom A3. Our Axuniquely andioms would still allow us to determine the sets of probability distributions P x;rto derive similarity values s T x 0; x for each x and T: For small values of T , however, thesesimilarity values would no longer be uniquely identicid:2ed TReducing the kernel width as data accumulates recid:3ects the assumption that large databases aremore representative of the distribution of x, and, therefore, contain a larger fraction of highlyrelevant or even identical cases. However, if the number of observations increases withoutaffecting the relative frequencies of cases, then there is no reason to adjust the similarity relation 26between cases. E.g., a physician with a long practice may encounter symptoms which he hasnever observed before. Consequently, he may cid:2nd it hard to associate these new cases withthose in his long memory. In such a situation, it does not appear reasonable to require thesimilarity function to converge to the identity, even for long databases. Hence, if unexpectedcases are likely to occur, Axiom A3 may be viewed as a sensible cid:2rst approximation.A nonconstant similarity function may also recid:3ect a decision makers effort to learn aboutcorrelation between outcomes conditional on the characteristics. Consider, once again the classicid:2cation problem discussed before and assume that two characteristics x and x 0 are very similar. If the physician has a database in which patients of types x and x 0 are associated with thesame outcome, then this database will concid:2rm the initial similarity perception. In contrast, ifhe observes that most patients of type x are classicid:2ed as r, whereas most patients of type x 0 areclassicid:2ed as r 0, it would be sensible to revise the similarity function. In such a case, the similarity function would depend not only on the length of the database, but also on the observedfrequency of cases. Hence, both Axioms A2 and A3 would fail. Modelling an adjustmentprocess of the similarity function according to the type and quantity of data is complicated bythe fact that, to our knowledge, there is little systematic information in the literature about howpeople assess the relevance of observations.7.5 Missing Data and Persistent Ambiguity In contrast to BGSS 2004, we assume that decision makers experience ambiguity about theirforecasts. GONZALES AND JAFFRAY 1998 attribute such ambiguity to missing data. In orderto illustrate this possibility and how it can be incorporated in this approach, consider a physicianwho has to decide whether to prescribe a specicid:2c treatment a to a patient with characteristicsx. He has a database in which the outcomes success, r 1; or failure, r 0; of treatmenta, have been recorded for two types of patients with characteristics x and x 0. Suppose thatthe physician has reasons to believe that the outcomes of treatment a are perfectly negativelycorrelated for patients with the two characteristics x and x 0 and, for simplicity, that all cases areequally relevant.27For long databases in which only outcome r 1 has been recorded for type x, D x; a; r 1T ; fcid:141g. The fcid:141g ; would obtain if the observed database were given by D0 it is reasonable to expect that the treatment a will lead to success, P x;a;1same prediction, P x 0;a;0TTx 0; a; r 0T . Hence, for large databases, the probability of success can be assessed as:Prfr 1 j Dg f D x; a; 1 f D x 0; a; 0 .Suppose that for some of the observations in the database the value of the characteristic, x or x 0,has not been recorded. cid:22x is used to denote the fact that information about the characteristic ismissing. In the extreme case of a database D cid:22x; a; r 1T the physician cannot unambiguously determine the probability of success for the patient to be treated. Since the characteristicx or x 0 has not been recorded, one cannot rule out that all patients in the database D are of typex, nor that all patients are of type x 0. In the cid:2rst case, he would assign a probability of 1 to asuccess, in the second, a probability of 0. The ignorance about the distribution because of theimprecise data can be modelled by sets of priors P cid:22x;a;1T cid:11 and P cid:22x;a;0T cid:11. Combiningdatabases in which the characteristic has been recorded with such in which data are missinggives rise to multiple priors HT D Xx 2fx;x 0;cid:22xgr 2f 0;1gf D x; a; r P x;a;rT.Such indeterminacy does not depend on the length of the database and will not disappear aslong as there are imprecise data.7.6 Selection of a Prior by the MaxMin Rule Most of the literature on ambiguity deals with decision rules based on preferences over acts.Following ELLSBERG 1961 ambiguity aversion has become the dominant assumption aboutbehavior under ambiguity. GILBOA AND SCHMEIDLER 1989 axiomatize a preference representation where the decision maker evaluates acts by the lowest expected utility of the act overall probabilities in a set of priors. Such conservative behavior is also recommended by the precautionary principle in environmental economics. Whatever the justicid:2cation for the minimumrule, it selects a particular probability distribution from the set of priors. We will show by ex28ample that, whenever the set of priors of a the decision maker satiscid:2es the axioms proposed inthis paper, then the probability distributions selected by the minimum principle will also obeythese axioms as well 12.Reconsider the case of Example 4.2, in which the decision maker observes only draws from Urn 2. He wants to predict the outcome of a bet on a black ball drawn from this urn, action a 0.Suppose that the decision makers beliefs are represented by a set of probability distributions asgiven by a correspondence H satisfying Axioms A1 cid:151 A5, e.g., the correspondence H in Equation 1.Let the utility of the decision maker from a black ball been drawn from Urn 2 be 1, and 0otherwise. Hence, u 0; 1 is the utility vector associated with action a 0. Consider themaxmin rule, which selects the single probability distributionhmin D j x 0; a 0 arg minfh cid:1 u j h 2 H 2; a 0 j Dgfrom the set H D j x 0; a 0. If a decision maker uses this probability distribution hmin to formthe expected utility of a 0; then his behavior will be governed by the maxmin rule, as describedin GILBOA AND SCHMEIDLER 1989 and MANSKI 2009. We now demonstrate that theselection hmin also satiscid:2es our Axioms.Since H satiscid:2es Axiom A1, Invariance, so does hmin. Consider databases F , D1:::Dn ofj1 cid:13j 1j1 cid:13jf Dj f F holds. By Axiom, A2, we havelength T such that D1cid:14:::cid:14Dn F n. Then there exist positive coefcid:2cients cid:13j with Pnfor which Pnfor positive coefcid:2cients 13 cid:21j such that Pnj1 cid:21j 1. Note thatcid:21j minfh cid:1 u j h 2 H 2; a 0 j Djg minfh cid:1 u j h 2 H 2; a 0 j F g H 2; a 0 j F cid:21j H 2; a 0 j Djn Xj1n Xj112 Similar results can be established for the rule which selects the Steiner point of each set, as well as for themore general cid:11maxmin rule, see GHIRARDATO, MACCHERONI, AND MARINACCI 2004 and CHATEAUNEUF, EICHBERGER, AND GRANT 2007.13for his evaluation, as, e.g., in the case of Equation 1. Hence, cid:21j cid:13j can be assumed for all j 2 f 1:::ng.It appears reasonable to assume that the decision maker in this example perceives all cases to be equally relevant 29Furthermore,n Xj1cid:21j minfh cid:1 u j h 2 H 2; a 0 j Djg jcid:1 ucid:1 n Xj1cid:21jcid:0hmin minfh cid:1 u j h 2 H 2; a 0 j F g hmin cid:1 u,where hminj : hmin 2; a 0 j Dj note that for u 0; 1, these values are unique. Hence,n Xj1cid:21jhminj hmin 2; a 0 j F ,implying that hmin satiscid:2es the Axiom A2.Since the weights cid:21j are independent of T , Axiom A3 is satiscid:2ed as well. For T ! 1,HTcid:162; a 0 j 2; BTcid:17 ! 0; 1 and HTcid:162; a 0 j 2; W Tcid:17 ! 1; 0. It follows that the associated probability distributions in hmin will also converge to these values, implying that Learning,Axiom A4 is satiscid:2ed. Since we have assumed only two outcomes, 0 and 1, Axiom A5,Noncollinearity, is trivially satiscid:2ed.8 Concluding Remarks Most of the literature on ambiguity takes the degree of ambiguity as a personal subjective characteristic. In particular, there is no formal reference to the information available to the decisionmaker. The amount of data is, however, likely to incid:3uence both the forecast made by the decision maker and his concid:2dence in this forecast. In this paper, we provided an approach whichcombines this intuition with the similarityweighted frequency approach of BGSS 2005. Werelax the Concatenation Axiom of BGSS 2005 by restricting it to databases of equal length.We show that the main result of BGSS 2005, namely that the similarity function is unique,remains true if one imposes consistency on the weights across databases of different size. Thisconsistency is essential for the uniqueness of the similarity weights.If one views the perception of similarity as an imperfect substitute for knowledge about therelevance of underlying data, then a decision maker has to cid:2nd out which characteristics arepayoffrelevant. Hence, the database may provide not only information about the distribution 30of payoffs, but also about the similarity of alternatives. One may conjecture that the more observations a database contains, the more precise the perception of similarity may become. PESKI2007 suggests a possible approach. He describes a learning process, in which the decisionmaker tries to assign objects optimally to categories in order to make correct predictions. Onemay interpret this approach as learning similarity where similarity values are restricted to zeroor one. A more general model would consider a continuum of similarity values.A further important research question concerns the derivation from preferences of a casebasedmultipleprior representation of beliefs jointly with a decision rule. Combining axioms fromcasebased decision making and from the literature on decision making under ambiguity, itappears possible to cid:2nd a representation of preferences over acts and a set of probabilities overoutcomes conditional on a database. We pursue this issue in EICHBERGER AND GUERDJIKOVA2008.Appendix A. Proofs To prove Theorem 6.1, we proceed as follows.In a cid:2rst step, Lemma A.1 establishes thenecessity of Axioms A1, A2 and A3 for the representation.The second step of the proof, Proposition A.2 consists in showing the result of Theorem 6.1for the special case where predictions are singlevalued, i.e. where beliefs are described by afunction h : D ! cid:1j Rjcid:01 satisfying Axioms A1 cid:151 A5. Using Axioms A4 and A5,Lemma A.3 shows that for sufcid:2ciently large values of T , no three of the vectors h Tcid:0c Tcid:1 arecollinear. Lemma A.4 uses Axiom A3 to demonstrate that similarity weights are independentof the length of the database. In particular, if the representation holds for a given D 2 D, then itholds for all databases with the same frequency f D, regardless of their length. In Lemmas A.5cid:151 A.7, we apply the construction of BGSS 2005 to determine the similarity function and showthat the representation holds for large values of T . Lemma A.4 implies that the representationholds for all values of T if the sets HTcid:0c Tcid:1 are singletons HTcid:0c Tcid:1 cid:8h Tcid:0c Tcid:1cid:9.In the third step of the proof, we show that a correspondence H satisfying the Axioms A1 cid:15131A5 can be represented as a union of functions h 2 H with the following properties: i allof these functions satisfy Axioms A1 cid:151 A5; ii for all of these functions the coefcid:2cientscid:21 specicid:2ed in Proposition A.2 are the same and coincide with the coefcid:2cients cid:21 for the correspondence H specicid:2ed in Axiom A2; iii the sets HTcid:0c Tcid:1 are given bycid:8h Tcid:0c Tcid:1cid:9h 2Hproperty i, a representation exists for each of the functions h. Property ii implies that all of. Bythese representations feature the same similarity function s. Property iii means that we canidentify P c T with HTcid:0c Tcid:1 and use the similarity function s to obtain a representation for H.Lemma A.1 The Axioms A1, Invariance, A2, Concatenation for databases of equal lengthand A3, Constant similarity are necessary for H to have a representation of the type HT D cid:26Pc 2C s c pc Pc 2C s c f D cT f D cTcid:27 .T 2 P cj pc Proof of Lemma A.1:It is obvious that for a given D 2 DT ,HT D cid:26Pc 2C s c pc Pc 2C s c f D cT f D cTcid:27T 2 P cj pcdoes not depend on the order of cases observed in D, but only on their frequency and the lengthof D, T , hence Axiom A1 is satiscid:2ed. To see that Axiom A2 is satiscid:2ed, cid:2rst note that forall c 2 C and all T 2 f 2; 3:::g,Hence,Let F n D1 cid:14 ::: cid:14 Dn for some n 2 N and some sets D1:::Dn 2 DT , then TT .1ns cf F f Di.j pc T 2 P c Tcid:27 P c HTcid:0c Tcid:1 cid:26 s c pcn Xi1Pc 2C s c f F cn Xi1 Pc 2C s c pc Pc 2CPnn Xi1HT F Pc 2C s c pcs c pc Ti1 s c f Di cs c pc Pc 2CPn Pc 2C s c f Di cT f Di c1ns c f Di cns c pc T f Di cT f F cf Di cHT Di ;321i1n Xi1 Pc 2CPc 2CPnn Xi1Xc 2Cn Xi1cid:21i Xc 2CT f Di ci1 s c f Di cwith.Note further that cid:21 does not depend on the length T of the databases D1:::Dn and F , but onlycid:21i Pc 2C s c f Di cPni1Pc 2C s c f Di cSince s c 0 for all c 2 C,cid:0cid:21icid:1ni1 2 int cid:1ncid:01 and, therefore, Axiom A2 is satiscid:2ed.on their frequencies. Hence, if HTcid:0F kcid:1 Pnicid:1 for some k 2 N, it holds for anyi1 cid:21i HTcid:0Dkk 2 N, implying that Axiom A3 holds.cid:4Denote by Qj Cj cid:1j Cjcid:01 the set of rational probability vectors of dimension j Cj. The possiblefrequency vectors which can be generated by a database of length T are given by the set:Qj Cj T 8:f 2 cid:1j Cjcid:01 j f c kc Tfor some kcj Cjc1 2 f 0; 1:::Tgj Cj with Obviously, for each T 2 f 2; 3:::g, Qj Cj T cid:18 Qj Cj cid:1j Cjcid:01. Let f j be the jth unit vector incid:1j Cjcid:01. f j denotes the jth component of the frequency vector f. The following propositionestablishes the result of the Theorem for the special case, in which H is a function:j Cj Xc1.kc T9;Proposition A.2 Assume that a class of functions h T : Qj Cj T ! cid:1j Rjcid:01 for T cid:21 2 satiscid:2es:exists a cid:21 2 0; 1 such that:for all distinct f, f 0 and f 00 such that cid:13f 1 cid:0 cid:13 f 0 f 00 for some cid:13 2 0; 1, thereicid:21h T f 1 cid:0 cid:21 h T f 0 h T f 00holds for all T such that f, f 0 and f 00 2 Qj Cj T ;iiiiithe sequencescid:16h Tcid:16 f jcid:17cid:17T2f 2;3:::gthe setcid:16h 1cid:16 f jcid:17cid:17j 2f 1:::j Cjgconverge for all j 2 f 1:::j Cjg;contains no three collinear vectors.Then, there are positive numbers fsjgj Cjj1, which are unique up to a multiplication by a positive Tcid:9j Cjnumber, and, for each T cid:21 2, there are unique probability vectorscid:8pjj1, such that for each T cid:21 2 and each f 2 Qj Cj T ,.A1h T f Pj Cjj1 sjf j pj Pj Cjj1 sjf jTProof of Proposition A.2:We start with the following Lemma:33Lemma A.3 Assume that conditions ii and iii of Proposition A.2 hold. For any three distinct i, j and k 2 f 1:::j Cjg there exists a cid:2nite cid:22T fi;j;kg such that the vectorscid:16h Tcid:16 f lcid:17cid:17l 2fi;j;kgare noncollinear for all T cid:21 cid:22T fi;j;kg.Proof of Lemma A.3:Let d denote the distance between the point h 1cid:16 f icid:17 and the line through the two pointsh 1cid:16 f jcid:17 and h 1cid:16 f kcid:17. Since the three points are noncollinear, d 0. Since the sequencesh Tcid:16 f lcid:17 are converging for l 2 fi; j; kg, we know that for each l, there exists a cid:22Tl suchthat for all T cid:21 cid:22Tl, h Tcid:16 f lcid:17 is contained in a ball with a center in h 1cid:16 f lcid:17 and with a ra3, denoted by h Tcid:16 f lcid:17 2 Bh 1 f l d3. Let cid:22T fi;j;kg : maxl 2fi;j;kgcid:8 cid:22Tlcid:9. Take anydius dtwo points xj 2 Bh 1 f j d3 and xk 2 Bh 1 f k d3 and note that the line which connects these two points must be at a distance at least d 3 from any point xi 2 Bh 1 f i d3.Hence, xi, xj and xk cannot be collinear. Since for every T cid:21 cid:22T fi;j;kg, and every l 2 fi; j; kg,h Tcid:16 f lcid:17 2 Bh 1 f l d3, the three vectors cannot be collinear.cid:4Letcid:22T : max For each T , decid:2ne pj T : h Tcid:16 f jcid:17. Since each of the sequencescid:16h Tcid:16 f jcid:17cid:17T2f 2;3;:::gffi;k;lgcid:18Cgcid:8 cid:22T fi;k;lgcid:9and no three limit vectors are collinear, the sequencescid:0pj We have to show that there are positive numbers fsjgj Cjj1 such that A1 holds. The next Lemmademonstrates that if such weights can be used to represent h T f for some f 2 Qj Cj T , then thesame weights can be used to represent h T 0 f for any T 0 such that f 2 Qj Cj T 0 .Tcid:1T2f 2;3;:::ginherit these properties.converges Lemma A.4 For j Cj cid:21 3, let fsjgj Cjj1 be a collection of similarity weights. For any T cid:21 2 andany f 2 Qj Cj T , decid:2ne the function g T f byg T f Pj Cjj1 sjf j pj Pj Cjj1 sjf jTSuppose that for some f 2 Qj Cj T , we can show that h T f g T f Pj Cjj1 sj f jpj Pj Cjj1 sj f jh T 0 f g T 0 f for all T 0 such that f 2 Qj Cj T 0 .34T. Then,cid:21j pj Tf,h Tf f hk Tf f then, by property i in Proposition A.2,iff T k Tf for some k 2 N. If Proof of Lemma A.4:Let Tf be the smallest T such that f 2 Qj Cj T . Then f 2 Qj Cj Tcid:21 2 cid:1j Cjcid:01 with cid:21j 0 iff f j 0 satiscid:2escid:21jh Tfcid:16 f jcid:17 j Cj Xj1j Cj Xj1cid:21jhk Tfcid:16 f jcid:17 j Cj Xj1j Cj Xj1cid:21jh Tcid:16 f jcid:17 j Cj Xj1j Cj Xj1for i 2 f 1; 2; :::j Cjg. Since gk Tf f Pj Cjj1 sj f jpjPj Cjj1 cid:21j pjimplying cid:21i sif iPj Cjj1 sj f jPj Cjj1 sj f jhk Tf f , we have the desired result.cid:4We now prove the result of Proposition A.2 for the case j Cj 3. For this case, decid:2ne fcid:3 :P33 cid:22T . Let s 1, s 2 and s 3 be the unique up to a T Pj Cjj1 sjf j pj Pj Cjj1 sjf jmultiplication by a positive number solution of the equation:In particular, g T f ,h T f cid:21j pjcid:21j pjk Tf.k Tf 13j1f j and consider T 3 cid:22T . Obviously, fcid:3 2 Q3h 3 cid:22T fcid:3 P3P3T , decid:2ne g T f : P3P3For any T cid:21 2 and any f 2 Q3Lemma A.4 then implies.j1 sj pj 3 cid:22Tj1 sjj1 sj f jpj Tj1 sj f jTk Tf. Obviously, g 3 cid:22T fcid:3 h 3 cid:22T fcid:3.g 3k fcid:3 h 3k fcid:3for all k 2 N.In order to state our next Lemma, we decid:2ne the cid:2rst simplicial partition of Qj Cj cid:1j Cjcid:01 byf jcid:17 for i 6 j.the four triangles separated by the segments connecting the three pointscid:16 1The second simplicial partition is obtained by partitioning each of the simplicial triangles intof i 122simplicial triangles, the kth simplicial partition is decid:2ned recursively. The simplicial points ofelements of the cid:2rst simplicial partition are in Q3the kth simplicial partition are all the vertices of the triangles in this partition. Note that all 2. The centers of gravity of these triangles are 4, while their centers of gravity 6. All elements of the second simplicial partition are in Q3in Q335are in Q312, etc. Our next result shows that, for j Cj 3, the functions h and g coincide on theset of simplicial points.13i1f ik ; f 2k ; f 3Lemma A.5 Let convcid:16 f 1and let fcid:3k P3kcid:17 g T0cid:16 f i If h T0cid:16 f icial triangle convcid:16 f 1lcid:17 g 12T0cid:16 f ih 12T0cid:16 f ikcid:17 be a simplicial triangle from the kth simplicial partitionk be its center of gravity. Let T0 cid:21 cid:22T be such thatn f iko 3and fcid:3k 2 Q3T0.kcid:17 for all i 1:::3 and h T0 fcid:3k g T0 fcid:3k , then, for every simplikcid:17,with a center of gravity fcid:3l P3lcid:17 for all i 1:::3 and h 12T0 fcid:3l g 12T0 fcid:3l holds.lcid:17 of convcid:16 f 1k ; f 3l ; f 2k ; f 2l ; f 3l ,f ii1i113k ; f 2Proof of Lemma A.5:Observe that if f 1; f 2 and f 3; f 4 are noncollinear segments in Q3T0 with the property thath T0 fi g T0 fi for all i 2 f 1:::4g, then for f f 1; f 2 f 3; f 4, h T0f f g T0f f ,Tg.where T0f minf T cid:21 T0 j f 2 Q3kcid:17 be a simplicial triangle from the kth simplicial partition In particular, let convcid:16 f 1k ; f 3kcid:17 for all i 1:::3 and h T0 fcid:3k g T0 fcid:3k . By Lemma A.4,kcid:17 g T0cid:16 f iand let h T0cid:16 f ikcid:17 for all i 1:::3 and h 12T0 fcid:3k g 12T0 fcid:3k . Note that for any two i,kcid:17 g 12T0cid:16 f ih 12T0cid:16 f ij 2 f 1; 2; 3g, i 6 j and n f 1; 2; 3gnfi; jg,k cid:16 f ikcid:17 are noncollinear, and, hence,cid:16h 12T0cid:16 f ikcid:17 and h 12T0cid:16 f 3Since h 12T0cid:16 f 1andcid:16h 12T0cid:16 f nkcid:17 and h 12T0cid:16 1Since both g 12T0cid:16 1kcid:17 h 12T0cid:16 1it follows that g 12T0cid:16 1kcid:17 ; h 12T0cid:16 f jkcid:17, h 12T0cid:16 f 2kcid:17cid:17kcid:17 ; h 12T0 fcid:3k cid:17 are noncollinear as well, they have a unique intersection point.kcid:17 must coincide with this intersection point,kcid:17.Now consider the centers of gravity of the four subtriangles. For the trianglekcid:17 cid:16 f nf ik 12f ik 12f ik 12f ik 12k ; fcid:3kcid:17 .f ik k; f j 1212f jf jf jf jf j 222212f 2k ;12f 1k 12f 3k ;12f 2k 12f 3kcid:192f 1k convcid:18 1lcid:17 with f 1kcid:17 12cid:16 1the center of gravity is fcid:3k and, hence, satiscid:2es the condition. Consider, therefore, w.l.o.g.,k . Firstf 3f 2k , f 2f 3k and f 3f 3f 1k 12l 12l ; f 2l ; f 3f 3f 1k 12l f 3f 2k 12f 2k 12f 1k 12l 12k ; 122the triangle convcid:16 f 1note that sincecid:16 12cid:16 12kcid:17 andkcid:17cid:17 is the intersection ofcid:16 f 3362212f 31122f 312f 2f 3f 3f 2k f 1k f 1k f 1k f 3k ; 12f 1k 12Similarly,The point 122f 3k 1f 2k 1212kcid:17, we have:cid:16 1h 12T0cid:18 1kcid:19 2cid:18 12cid:18 1h 12T0cid:18 12cid:18 12cid:18 1kcid:19 2cid:16 1kcid:17 3cid:18 12cid:18 1kcid:19 2cid:18 1kcid:17. Hence, h 12T0cid:16 3andcid:16 f 2convcid:18 f 3is the intersection ofcid:16 f 312f 3k 1412f 2kcid:19cid:19 g 12T0cid:18 12cid:18 1kcid:19cid:19 g 12T0cid:18 12cid:18 1kcid:19cid:19 ;cid:18 12cid:18 1kcid:17 g 12T0cid:16 3kcid:17 andcid:16cid:16 112f 1k 12kcid:17 ;cid:16 3f 2k 1212f 3k 142f 3k 142k ; f 312f 312f 2f 1k 12f 2k f 1k f 1k f 1k f 2k k ; 12f 3k ;f 3f 3f 2k ;12212f 34442f 1k 122f 3kcid:19 kcid:19 2cid:18 12f 32f 1k 12k is on the intersection off 21212cid:18 12cid:18 121f 2k f 1k 1212f 3kcid:19cid:19 .kcid:19cid:19f 212f 2f 3f 1k kcid:19 kcid:19cid:19kcid:17. The center of gravity fcid:3l ofkcid:19f 312f 3k 14f 24kcid:17cid:17, and, hence, h 12T0 fcid:3l g 12T0 fcid:3l .cid:4Applying the claim inductively and using the result of Lemma A.4, we conclude that the functions h and g coincide on the set of all simplicial points.To complete the proof of Proposition A.2 for the case of j Cj 3, it remains to show that thefunctions h and g coincide on the set of all rational points.Lemma A.6 h T f g T f for all f 2 Q3T and for all T cid:21 2.T1 ; f 21 ; f 3k , f 2k and f 3k are in Q3. Form a sequence T 1:::T k::: with T 1 T . For some T cid:21 cid:22T , let f 2 Q3Proof of Lemma A.6:Take an arbitrary f 2 Q36 T and T k 6T kcid:01 and take a sequence of simplicial trianglescid:16 f 1kcid:17 :::k ; f 3k ; f 2such that for each k, f 1k f forf iall i 2 f 1; 2; 3g. It is obvious that this construction is possible for every f. We want to showthat h T k f g T k f for all k.First note that if f 2 convcid:16 f 1Since for all n 2 f 1; 2; 3g, limk!1 pnkcid:17, then by the decid:2nition of g,kcid:17 ; g T kcid:16 f 3kcid:17cid:17 :T k h 1cid:16 f ncid:17 2 cid:12, we have that for all r 2 R and allg T k f 2 convcid:16g T kcid:16 f 11cid:17 :::cid:16 f 1kcid:17 and limk!1T k, f 2 convcid:16 f 1kcid:17 ; g T kcid:16 f 2k ; f 3k ; f 2k ; f 2k ; f 337j 2 f 1; 2; 3gpnlim T k rT k r!n1 sn limk!1k n!n1 snf n pnn1 snf nP3T k r snf nf jk n pnf jn1 snk nf jk nsnf jn1 snk!1 P3cid:0P3P33Xn1n1 snf n cid:0P3P3n1 snf n! 0,h 1cid:16 f ncid:17 r P3n1 snf n cid:0 P3i1 sif iP3P3T k r and h 1cid:16 f ncid:17 r denote the rthcomponents of the vectores pnkcid:17cid:13cid:13cid:13 0k!1cid:13cid:13cid:13g T k f cid:0 g T kcid:16 f j 3Xn1i1 sif ilimwhere pnrespectively. Hence,for all j 2 f 1; 2; 3g.Property i in Proposition A.2 implies that there exists a vector cid:21 2 cid:12, independent of T k,such that T k and h 1cid:16 f ncid:17,, i.e. that there exists an cid:15 0 such that:cid:21nh T kcid:16 f ncid:17 .3Xn1h T k f Suppose that for some i 2 f 1; 2; 3g, cid:21i 6P3n1 snf ncid:13cid:13cid:13cid:13cid:13P3Since h T k f 2 convcid:16h T kcid:16 f 1cid:17 ; h T kcid:16 f 2cid:17 ; h T kcid:16 f 3cid:17cid:17 andcid:13cid:13cid:13cid:13cid:13cid:21i cid:0n1 snf nsif isif i cid:15 0.convcid:16h T kcid:16 f 1cid:17 ; h T kcid:16 f 2cid:17 ; h T kcid:16 f 3cid:17cid:17 convcid:16g T kcid:16 f 1cid:17 ; g T kcid:16 f 2cid:17 ; g T kcid:16 f 3cid:17cid:17we have thatlimk!1convcid:16h T kcid:16 f 1cid:17 ; h T kcid:16 f 2cid:17 ; h T kcid:16 f 3cid:17cid:17nfg T k f g ?.Hence, it must be that:which is equivalent to:which reduces tolimlimsif ik!1kh T k f cid:0 g T k f k 0,T kcid:13cid:13cid:13cid:13cid:13k!1cid:13cid:13cid:13cid:13cid:13n1 snf n! pi 3Xi1 cid:21i cid:0P3cid:13cid:13cid:13cid:13cid:13n1 snf n! h 1cid:16 f icid:17cid:13cid:13cid:13cid:13cid:133Xi1 cid:21i cid:0P3sif i38 0, 0,A2A3sif iT .cid:4P3n1 snf nare not collinear. Hence, A3 can be satiscid:2edfor all i 2 f 1; 2; 3g. It follows that h T k f g T k f for all k. Lemma By presumption, the vectorscid:16h 1cid:16 f icid:17cid:17i 2f 1;2;3gonly if cid:21i A.4 then implies that h T f g T f for all T such that f 2 Q3Lemma A.6 completes the proof of Proposition A.2 for the case of j Cj 3.Now consider the case of j Cj 3. To decid:2ne the similarity function for this case, choose threedistinct j, k, l cid:20 j Cj. Decid:2ne ffj;k;lg : Pn 2fj;k;lgf n and let sfj;k;lg be the unique up to ah 3 cid:22Tcid:0ffj;k;lgcid:1 Pn 2fj;k;lg Pn 2fj;k;lgf : Pn 2fj;k;lg Pn 2fj;k;lgmultiplication by a positive number solution of:T convn f j; f k; f lo. Note that gfj;k;lg h T on this set. We cid:2rst showdetermined in this way do not depend on the choice of j, k andsfj;k;lgpnn 3 cid:22Tsfj;k;lgnsfj;k;lgnsfj;k;lgnf n pn Tgfj;k;lg TDecid:2nef n13.T:, since gfj;k;lg Tf gfj;k;l 0g Tf h T f for all f 2 convcid:16 f j; f kcid:17.for all T and all f 2 Q3that the similarity values sfj;k;lgl: note that sfj;k;lgjsfj;k;lgk Hence, decid:2nensfj;k;l 0gsfj;k;l 0gkjcid:13jk :sfj;k;lgjsfj;k;lgksfj;k;l 0gjsfj;k;l 0gkfor all l, l 0 2 f 1:::j Cjg. Note that cid:13jk is welldecid:2ned for all j, k cid:20 j Cj, since no three vectors 3 cid:22T h 3 cid:22Tcid:16 f jcid:17, pkpj 3 cid:22T h 3 cid:22Tcid:16 f lcid:17 are collinear. Furthermore,3 cid:22T h 3 cid:22Tcid:16 f kcid:17 and plcid:13jkcid:13klcid:13lj sfj;k;lgjsfj;k;lgksfj;k;lgksfj;k;lglsfj;k;lglsfj;k;lgj 1.Decid:2ne s 1 : 1 and sj : cid:13j 1 for all j 2 f 2:::j Cjg. We wish to show that for all triples j, k and l,nsfj;k;lgsk 1.is proportional to fsj; sk; slg. This follows from cid:131jcid:13jkcid:13k 1 1on 2fj;k;lgsj;k;ljsj;k;lksjn Given s sjj Cjj1, decid:2neg T f : Pj Cjj1 sjf j pj Pj Cjj1 sjf jTfor all T and all f 2 Qj Cj T .We know that for j Cj 3, g T f h T f . We wish to prove that the same is true for anyj Cj cid:21 3. We proceed by induction. Suppose that the claim is true for all j Cj cid:20 N and takej Cj N 1. We prove the following claim by induction:39Lemma A.7 For every subset K cid:18 f 1:::j Cjg, h T f g T f holds for every T cid:21 2 andevery f 2 Qj Kj T convcid:16n f j j j 2 Kocid:17.Proof of Lemma A.7:We know that the claim is true for j Kj 3, so we assume that it is true for j Kj N and provethat it will hold for j Kj N 1.By property iii of Proposition A.2, no three of the vectors h 1cid:16 f icid:17 are collinear. By theinduction argument, for every m 2 K, h T f g T f holds for every T cid:21 2 and everyf 2 QNLet T cid:21 cid:22T and consider cid:2rst f 2 intcid:16QN 1 convcid:16n f j j j 2 Kocid:17cid:17. f can be expressedas: f PN 1f l for some rational coefcid:2cients cid:13l 0. For every m 2 K, let fm be the point T convcid:16n f j j j 2 Knfmgocid:17.l1 cid:13lin Tconvcid:16cid:16n f l j l 2 Knmocid:17 QNT 0cid:17that is on the line connecting f and f m, i.e. fm PN 1f l, where T 0 is the smallestnumber of observations, for which f and fm 2 QNT 0 for all m 2 K. Such a T 0 exists, sincecid:13l are rational coefcid:2cients. We have h T 0cid:16 f mcid:17 g T 0cid:16 f mcid:17 and, by the induction argument,h T 0 fm g T 0 fm. Property i of Proposition A.2 impliesl1l 6m 1cid:0cid:13mcid:13lh T 0 f 2 cid:16h T 0cid:16 f mcid:17 ; h T 0 fmcid:17h T 0 f 2 cid:16h T 0cid:16 f m 0cid:17 ; h T 0 fm 0cid:17for all m and m 0 2 K.Now we wish to show that not all of these intervals are collinear. This follows from the fact thatno three of the vectors h 1cid:16 f mcid:17 are collinear, and hence, by Lemma A.3, the correspondingvectors h T 0cid:16 f mcid:17 are also noncollinear for any T 0 cid:21 T . Hence, there are two distinct intervalscid:16h T 0cid:16 f mcid:17 ; h T 0 fmcid:17 and cid:16h T 0cid:16 f m 0cid:17 ; h T 0 fm 0cid:17 which do not lie on the same line, and,by property i of Proposition A.2, have h T 0 f as an intersection point. Since, g T 0 f is byconstruction also an intersection point of the two intervals, it follows that h T 0 f g T 0 f .40Lemma A.7 completes the proof of Proposition A.2.f 2 intcid:16convcid:16n f j j j 2 Kocid:17 QN 1T cid:17 ,By Lemma A.4, we conclude that h T f g T f holds for all T cid:21 2 and allas well as for all f 2 convcid:16n f j j j 2 Knmocid:17 QNLemma A.8 Under Axiom A5, it is possible to select vectors h 1 c 2 H1 c for eachc 2 C such that no three vectors in the set h 1 cc 2C are collinear. Furthermore, for each c,there exists a Tc 2 N and a sequence of vectors h Tcid:0c Tcid:1 2 HT cT for T cid:21 Tc such that T for m 2 K, thus establishing the result.cid:4lim T!1h Tcid:0c Tcid:1 h 1 c .Proof of Lemma A.8:Denote the set Cp to be the set of all cases c 2 C, such that H1 c is of dimension p 2f 0; 1; :::j Rj cid:0 1g. To show that a selection of vectors h 1 c with the stated properties exists,cid:2rst set h 1 c to be the unique elements of each of the sets H1 cc 2 C0. No three of theseare collinear by Axiom A5. Take a case c 2 C1. For a given segment e; f , decid:2ne l e; f tobe the line containing the segment. Consider the set Lc fl h 1 c 0 ; h 1 c 00gc 0, c 002 C0 cid:0l H1 cc 2 C1 nl H1 ccid:1This is the set of all lines connecting any two singleton sets, as well as the collection of linesdecid:2ned by the segments in f H1 cgc 2C, excluding the line containing H1 c itself. Choose apoint h 1 c 2 H1 c such that h 1 c 62 Lc. That this can be done is ensured by Axiom A5.We now show that no three of the sets fh 1 cg for c 2 C0 fcg and H1 c for c 2 C1 arecollinear. First consider the combination of h 1 c with any two points h 1 c 0 and h 1 c 00with c 0, c 00 2 C0. Since h 1 c 62 l h 1 c 0 ; h 1 c 00, these are noncollinear.Second, consider the combination of h 1 c with a point h 1 c 0, c 0 2 C0, and a segment,H1 c 00, c 00 2 C1. If h 1 c 0 and H1 c 00 are collinear, then h 1 c 62 l H1 c 00 and, hence,the three sets are not collinear. If h 1 c 0 and H1 c 00 are not collinear, then neither is the tripleh 1 c, h 1 c 0 and H1 c 00.Last, consider the combination of h 1 c with two segments H1 c 0 and H1 c 00 c 0, c 00 2 C1.Axiom A5 excludes the case in which all three of the sets H1 c, H1 c 0 and H1 c 00 lie 41on the same line. Hence, at least one of these two segments, say H1 c 0 is noncollinear to H1 c. It follows that h 1 c 62 l H1 c 0 ; which proves that the three sets h 1 c, H1 c 0and H1 c 00 are noncollinear. We have thus shown that the new set of limit predictions decid:2nedby:H 11H 11c fh 1 cg for all c 2 C0 fcgc H1 c for all c 62 C0 fcgsatiscid:2es Axiom A5.Using the same argument by induction, it is possible to choose points h 1 c for all c 2 C0 C1in such a way that no three of these points are collinear. This procedure generates a new set oflimit predictions:Hj C1j 1 c fh 1 cg for all c 2 C0 n C1o Hj C1j 1 c H1 c for all c 62 C0 n C1odecid:2ned in this way, construct the set of all lines Using the singleton setscid:18Hj C1j 1 ccid:19c 2 C0 C1connecting these points:L C0 C1cid:26lcid:18Hj C1j 1 c 0 ; Hj C1j 1 c 00cid:19cid:27c 0, c 002 C0 C1 fl h 1 c 0 ; h 1 c 00gc 0, c 002 C0 C1.Note that the intersection of each of the remaining Hj C1j 1 c with L C0 C1is a cid:2nite collection ofpoints and segments. Fix a case cid:22c 62 C0 C1. Since the set Hj C1j 1 cid:22c is of dimension 2 or higher,we can cid:2nd a point h 1 cid:22c 2 Hj C1j 1 cid:22cn L C0 C1, which is noncollinear to any pair of vectors in. Analogously, decid:2ne the set L C0 C1fcid:22cgh 1 cc 2 C0 C1and proceed by induction to complete the construction.The fact that a convergent sequencecid:16h Tcid:0c Tcid:1cid:17Tcid:21Tc l h 1 c 0 ; h 1 c 00c 0, c 002 C0 C1fcgof the limit of a sequence of sets, see ROCKAFELLAR AND WETS 2004, P. 109.cid:4Proof of Theorem 6.1:We show that we can represent the correspondence H as a collection of functions H :cid:8h : D ! cid:1j Rjcid:01cid:9 satisfying the following conditions:for any T and any three databases D, D0 and D00 2 DT such that cid:13f D 1 cid:0 cid:13 f D0 f D00exists follows directly from the decid:2nitioni42for some cid:13 2 0; 1,impliesfor every h 2 H;cid:21HT D 1 cid:0 cid:21 HT D0 HT D00cid:21h T D 1 cid:0 cid:21 h T D0 h T D00iiiiiivfor each h 2 H and c 2 C, the sequencecid:0h Tcid:0c Tcid:1cid:1Tcid:212 converges to some limit h 1 c;for each h 2 H, no three of the vectors in the set h 1 cc 2C are collinear;for each T cid:21 2, and any D 2 DT , h 2Hh T D HT D.h DA4where cid:210c are such that:To construct the set of functions H, proceed as follows: cid:2x a D 2 D. Let T be the lengthof the database D. Let h T D 2 HT D. If D cT for some c 2 C, pick an elementh Tcid:16c 0Tcid:17 2 HTcid:16c 0Tcid:17 for all c 0 6 c. For any D0 2 DT , letcid:210ch Tcid:0c Tcid:1 ,cid:210c HTcid:0c Tcid:1 .cid:21c HTcid:0c Tcid:1 .cid:21ch Tcid:0c Tcid:1 .T D0 Xc 2j Cj HT D0 Xc 2j Cj If D 6 c T for all c 2 C, let cid:21cc 2C be such that:HT D Xc 2j Cjh T D Xc 2j Cj It is then possible to choose vectors 2 HTcid:0c Tcid:1 for each c 2 C such that:Using the so chosen vectors h Tcid:0c Tcid:1, construct h T D0 as in A4 for all other sets D0 2 DT. Let TC maxf Tc j c 2 Cg, where Tc are as decid:2ned in the statement of Lemma A.8. For T 0 TC, T 0 6 T , pick any vectors h T 0cid:0c T 0cid:1 2 HT 0cid:0c T 0cid:1 and for any D0 2 DT 0, construct thevectors h T 0 D0 as in A4.To complete the construction for T 0 cid:21 TC, T 06 T , choose vectors h 1 c 2 H1 c suchthat no three of these vectors are collinear and sequencescid:16h T 0cid:0c T 0cid:1cid:17T 0cid:21TCsuch that for everyc 2 C and T cid:21 TC, T 0 6 T , h T 0cid:0c T 0cid:1 2 HT 0 cT 0 and lim T 0!1h T 0cid:0c T 0cid:1 h 1 c this can bedone by Lemma A.8. Set h T 0cid:0c T 0cid:1 h T 0cid:0c Tcid:1 for all T 0 cid:21 TC, T 0 6 T , and for any D0 2 DT 0,construct the vectors h T 0 D0 as in A4.43The same procedure can be repeated for any h T D 2 HT D, giving us a collection of functions HD for a specicid:2c database D 2 DT . Note that the same sequencescid:16h T 0cid:0c T 0cid:1cid:17Tcid:21TCareused in the construction of each of these functions. The union of the sets HD over all databasesin D, gives us H D2DHD. It is obvious that these functions satisfy all of the conditionslisted above.Take any two such functions, h 1 and h 2 2 H. Suppose that h 1 was constructed starting froma database D1 2 DT 1, while h 2 was constructed starting from a database D2 2 DT 2. Then,h 1T 0 h 2From Proposition A.2, we conclude, that for each of the functions h 2 H, we can cid:2nd a similarity function sh and probability vectors ph;c T 0 for all T 0 cid:21 maxf T 1; T 2; TCg.T h Tcid:0c Tcid:1 such thath T D Pc 2C sh c ph;c Pc 2C s c f D cT oh 2HT nph;c T f D cfor all T cid:21 2 and all D 2 DT . We set P c We now show that sh does not depend on the specicid:2c choice of the function h. Take any twofunctions h 1 and h 2 2 H. Let cid:22T 1 be the minimal value of T such that no three of the vectors Tcid:0c Tcid:1cid:1c 2C are collinear for T cid:21 cid:22T 1. From the proof of Lemma A.3, we know that cid:22T 1 iscid:0h 1cid:2nite. Decid:2ne cid:22T 2 analogously. Recall that the similarity functions s 1 and s 2 are decid:2ned bycid:8h Tcid:0c Tcid:1cid:9h 2H HTcid:0c Tcid:1.p 1;m 3 cid:22T 1p 2;m 3 cid:22T 2h 1h 2s 1;fj;k;lgms 1;fj;k;lgm 3 cid:22T 1cid:0ffj;k;lgcid:1 Pm 2fj;k;lg Pm 2fj;k;lg 3 cid:22T 2cid:0ffj;k;lgcid:1 Pm 2fj;k;lg Pm 2fj;k;lg Pm 2fj;k;lgp 1;m 3k cid:22T 1 cid:22T 2s 1;fj;k;lgms 2;fj;k;lgms 2;fj;k;lgmwhere j, k and l is any triplet of cases in C. Since for all T 0 cid:21 maxf T 1; T 2; TCg, h 1there exists a k such that 3k cid:22T 1 cid:22T 2 cid:21 maxf T 1; T 2; TCg and, hence, the equations 3k cid:22T 1 cid:22T 2cid:16c 3k cid:22T 1 cid:22T 2s 1;fj;k;lgms 1;fj;k;lgmh 1h 1ms 1;fj;k;lgm T 0,T 0 h 2A5A6cid:17A7cid:17A83k cid:22T 1 cid:22T 2cid:0ffj;k;lgcid:1 Pm 2fj;k;lg Pm 2fj;k;lg 3k cid:22T 1 cid:22T 2cid:0ffj;k;lgcid:1 Pm 2fj;k;lg Pm 2fj;k;lgh 2Pm 2fj;k;lgs 2;fj;k;lgmh 2Pm 2fj;k;lgs 2;fj;k;lgmp 2;m 3k cid:22T 1 cid:22T 2s 2;fj;k;lgm Pm 2fj;k;lg 3k cid:22T 1 cid:22T 2cid:16c 3k cid:22T 1 cid:22T 2s 2;fj;k;lgmm 44are equivalent. By Lemma A.4, the similarity values in equations A5 and A7 are identical,and so are the similarity values in equations A6 and A8. Hence, both h 1 and h 2 give riseto identical similarity functions. sh is therefore independent of h.We conclude thatcid:4References HT D cid:26Pc 2C s c pc Pc 2C s c f D cT f D cTcid:27 .T 2 P cj pc AHN, D. 2008: cid:147Ambiguity Without a State Space,cid:148 Review of Economic Studies, 71, 3cid:15028.BEWLEY, T. F. 1986: cid:147Knightian Decision Theory: Part 1,cid:148 Discussion Paper 807, Cowles Foundation.BILLOT, A., I. GILBOA, D. SAMET, AND D. SCHMEIDLER 2005: cid:147Probabilities as SimilarityWeighted Frequencies,cid:148 Econometrica, 73, 1125cid:1501136.BLIZARD, W. D. 1988: cid:147Multiset Theory,cid:148 Notre Dame Journal of Formal Logic, 301, 36cid:15066.CHATEAUNEUF, A., J. EICHBERGER, AND S. GRANT 2007: cid:147Choice Under Uncertainty withthe Best and the Worst in Mind: NeoAdditive Capacities,cid:148 Journal of Economic Theory, 137,538cid:150567.COIGNARD, Y., AND J.Y. JAFFRAY 1994: cid:147Direct Decision Making,cid:148 in Decision Theory and Decision Analysis: Trends and Challenges, ed. by S. Rios, Boston. Kluwer Academic Publishers.EICHBERGER, J., AND A. GUERDJIKOVA 2008: cid:147CaseBased Expected Utility: Preferences over Actions and Data,cid:148 Discussion paper, Cornell University.ELLSBERG, D. 1961: cid:147Risk, Ambiguity, and the Savage Axioms,cid:148 Quarterly Journal of Economics, 75, 643cid:150669.EPSTEIN, L., AND M. SCHNEIDER 2007: cid:147Learning Under Ambiguity,cid:148 Review of Economic Studies, 74, 1275cid:1501303.GAJDOS, T., T. HAYASHI, J.M. TALLON, AND J.C. VERGNAUD 2007: cid:147Attitude Towards Imprecise Information,cid:148 Journal of Economic Theory, 1401, 27cid:15065.GHIRARDATO, P., F. MACCHERONI, AND M. MARINACCI 2004: cid:147Differentiating Ambiguity abnd Ambiguity Attitude,cid:148 Journal of Economic Theory, 118, 133cid:150173.GILBOA, I. 2009: Theory of Decision under Uncertainty, Econometric Society Monographs. Cambridge University Press, Cambridge.GILBOA, I., O. LIEBERMAN, AND D. SCHMEIDLER 2006: cid:147Empirical Similarity,cid:148 Review of Economics and Statistics, 88, 433cid:150444.GILBOA, I., AND D. SCHMEIDLER 1989: cid:147Maxmin Expected Utility with a NonUnique Prior,cid:148Journal of Mathematical Economics, 18, 141cid:150153.GILBOA, I., AND D. SCHMEIDLER 2001: A Theory of CaseBased Decisions. Cambridge University Press, Cambridge, UK.GILBOA, I., D. SCHMEIDLER, AND P. WAKKER 2002: cid:147Utility in CaseBased Decision Theory,cid:14845Journal of Economic Theory, 105, 483cid:150502.GONZALES, C., AND J.Y. JAFFRAY 1998: cid:147Imprecise Sampling and Direct Decision Making,cid:148Annals of Operations Research, 80, 207cid:150235.HASTIE, T., R. TIBSHIRANI, AND J. FRIEDMAN 2001: The Elements of Statistical Learning. Data Mining, Inference, and Prediction, Springer Series in Statistics. Springer, New York.KEYNES, J. M. 1921: A Treatise on Probability. Macmillan, London.KLIBANOFF, P., M. MARINACCI, AND S. MUKERJI 2005: cid:147A Smooth Model of Decision Making Under Ambiguity,cid:148 Econometrica, 736, 1849cid:1501892.MANSKI, C. 2000: cid:147Identicid:2cation Problems and Decisions under Ambiguity: Empirical Analysisof Treatment Response and Normative Analysis of Treatment Choice,cid:148 Journal of Econometrics, 95, 415cid:150432.MANSKI, C. 2009: cid:147Diversicid:2ed Treatment under Ambiguity,cid:148 International Economic Review, p.forthcoming.MARINACCI, M. 2002: cid:147Learning about Ambiguous Urns,cid:148 Statistical Papers, 43, 143cid:150151.OHAGAN, A., AND B. R. LUCE 2003: cid:147A Primer on Bayesian Statisitics in Health Economicsand Operations Research,cid:148 Centre for Bayesian Statistics in Health Economics.PESKI, M. 2007: cid:147Learning through Patterns,cid:148 Discussion paper, University of Chicago.ROCKAFELLAR, R. T., AND R. WETS 2004: Variational Analysis. Springer, Heidelberg.SCHMEIDLER, D. 1989: cid:147Subjective Probability and Expected Utility without Additivity,cid:148 Econometrica, 57, 571cid:150587.STINCHCOMBE, M. 2003: cid:147Choice and Games with Ambiguity as Sets of Probabilities,cid:148 Discussion paper, University of Texas, Austin.46", "filename": "VfS_2010_pid_295.pdf", "person": ["J\u00fcrgen Eichberger", "Eichberger, J\u00fcrgen", "Ani Guerdjikova", "Guerdjikova, Ani"], "date": ["2010"]}