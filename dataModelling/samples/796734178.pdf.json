{"lang": "en", "identifier_url": ["http://www.econstor.eu/bitstream/10419/103900/1/796734178.pdf"], "title": ["Strongly Symmetric Equilibria in Bandit Games"], "plaintext": "Strongly Symmetric Equilibriain Bandit Games Johannes H orner Nicolas Klein Sven Rady This version: August 17, 2014Abstract This paper studies strongly symmetric equilibria SSE in continuoustimegames of strategic experimentation with Poisson bandits. SSE payoffs can bestudied via two functional equations similar to the HJB equation used for Markovequilibria. This is valuable for three reasons. First, these equations retain thetractability of Markov equilibrium, while allowing for punishments and rewards:the best and worst equilibrium payoff are explicitly solved for. Second, theycapture behavior of the discretetime game: as the period length goes to zeroin the discretized game, the SSE payoff set converges to their solution. Third,they encompass a large payoff set: there is no perfect Bayesian equilibrium in thediscretetime game with frequent interactions with higher asymptotic efficiency.Keywords: TwoArmed Bandit, Bayesian Learning, Strategic Experimentation, Strongly Symmetric Equilibrium.JEL Classification Numbers: C73, D83.We thank seminar participants at Aalto University Helsinki, Collegio Carlo Alberto Turin, Frankfurt, Mc Master University, Microsoft Research New England, Paris S eminaire Roy and S eminaire Parisien de Th eorie des Jeux, Toulouse, Warwick, the 2012 International Conference on Game Theory at Stony Brook, the 2013 North American Summer Meeting of the Econometric Society, the 2013Annual Meeting of the Society for Economic Dynamics, the 2013 European Meeting of the Econometric Society, the 4th Workshop on Stochastic Methods in Game Theory at Erice, the 2013 Workshopon Advances in Experimentation at Paris II, the 2014 Canadian Economic Theory Conference, the 8th International Conference on Game Theory and Management in St. Petersburg, and the SING 10Conference in Krakow for their comments and suggestions. Part of this paper was written during avisit to the Hausdorff Research Institute for Mathematics at the University of Bonn under the auspicesof the Trimester Program Stochastic Dynamics in Economics and Finance. Financial support fromthe Cowles Foundation, Deutsche Forschungsgemeinschaft SFBTR 15, and the Fonds de Recherchedu Qu ebec Soci et e et Culture is gratefully acknowledged.Yale University, 30 Hillhouse Ave., New Haven, CT 06520, USA, johannes.horneryale.edu.Universit e de Montr eal and CIREQ. Mailing address: Universit e de Montr eal, D epartementde Sciences Economiques, C.P. 6128 succursale Centreville; Montr eal, H3C 3J7, Canada,kleinnicyahoo.com.University of Bonn, Adenauerallee 2442, D53113 Bonn, Germany, radyhcm.unibonn.de.1Introduction There is a troubling disconnect between discretetime and continuoustime game theory.With few exceptions, games in discrete time use either subgameperfect equilibrium or,if there is incomplete information, perfect Bayesian equilibrium as a solution concept.With few exceptions, games in continuous time are concerned with Markov equilibriaonly. The technical reasons for this divide are wellknown: defining outcomes, strategiesand equilibrium in continuous time raises serious mathematical difficulties; restrictingattention to Markov strategies bypasses these. Conceptually, however, the discontinuityis artificial and deeply unsatisfactory.This paper suggests a middle ground.It examines strongly symmetric equilibriaSSE. These are equilibria in which all players use a common continuation strategy,on and off path. However, this common continuation strategy can depend on theentire history, not only its payoffrelevant component. As we show, strongly symmetric equilibria retain the tractability of Markov perfect equilibria MPE. Markovperfect equilibrium payoffs can be studied via a wellknown functional equation, the HamiltonJacobiBellman or Isaacs equation. Similarly, the set of strongly symmetricequilibrium payoffs is characterized by a pair of coupled functional equations. At thesame time, unlike Markov equilibrium, strongly symmetric equilibrium allows for patterns of behavior that are both empirically compelling and theoretically fundamental:punishments and rewards.We confine our analysis to a particular class of models, the socalled twoarmedbandit model, which has been extensively studied both in discrete and in continuoustime; see, in particular, Keller et al. 2005 and Keller and Rady 2010. More specifically, the setup is as in Keller and Rady 2010. The motivation for this restriction istwofold. First, as will become clear, the characterization of the appropriate boundarycondition for strongly symmetric equilibria hinges on fine details of the setup as isalso the case for MPE. We only know how to perform such an analysis within theconfines of a specific model. Second, restricting attention to such a wellstudied modelallows us to provide a closedform for the equilibrium payoff set, a concrete illustrationof how a slight weakening of the solution concept from MPE to SSE dramaticallyalters behavior and payoffs.Strongly symmetric equilibria are not new. They have been studied in repeatedgames at least since Abreu 1986. They are known to be restrictive. To begin with,they make no sense if the model itself fails to be symmetric. But as Abreu 1986already observes for repeated games, they are i easily calculated, being completelycharacterized by two simultaneous equations; ii more general than static Nash, or 1even Nash reversion; and even iii without loss in terms of total welfare, at least insome cases. See also Abreu, Pearce and Stacchetti 1986 for optimality of symmetricequilibria within a standard oligopoly framework, and Abreu, Pearce and Stacchetti1993 for a motivation of the solution concept based on a notion of equal bargainingpower. A more general analysis for repeated games with perfect monitoring is carriedout by Cronshaw and Luenberger 1994 showing how the set of SSE payoffs is obtainedby finding the largest scalar solving a certain equation. Properties iiii generalizeto stochastic games, with Markov perfect replacing Nash in statement ii.Our first step involves establishing the rather straightforward functional analoguesof the equations derived by Abreu, and Cronshaw and Luenberger, for a discretizedversion of our game in which all players can adjust actions on a common, equally spacedtime grid only. This in turn motivates the coupled functional equations and boundarycondition in continuous time that we put forth as the tool for analyzing stochasticgames such as our bandit model. In our second step, we then provide a formal limitingresult: as players are allowed to adjust actions more and more frequently, the upperand lower boundaries of the set of SSE payoffs of the discretized game converge to theunique solution of the functional equations subject to the boundary condition.To be sure, we can and do in our third step directly construct strongly symmetricequilibria in continuous time and show that their payoff functions solve the functionalequations. But given that, to the best of our knowledge, this paper is the first attemptat studying these coupled equations in continuoustime games, we view it as useful andreassuring to check that they capture precisely the strategic elements of the discretetime game with frequent interactions in this particular instance. This is by no meansa foregone conclusion: there are wellknown examples in which the continuoustimedefinition of Markov equilibrium yields a set of payoffs that does not coincide with thelimit of the set of Markov equilibrium payoffs for the discretetime approximation. Infact, one corollary of our analysis is that the infiniteswitching equilibria in Keller etal. 2005 have no counterpart in discrete time, no matter how small the time intervalbetween consecutive choices;1 see also Heidhues, Rady and Strack 2012.While proving this limit result requires some care, actually solving the continuoustime equations is a straightforward exercise in the case of the bandit model. This iswhere the analytical convenience of continuous time comes into play, yielding simpleand exact solutions that admit intuitive interpretations. The resulting equilibriumpayoff correspondence is rich: the symmetric Markov equilibrium is neither the lowestnor the highest selection. In fact, we show that the restriction to SSE is without loss 1To be more precise, they have no counterpart provided one discretizes the game as we do. Alternative discretizations might yield different boundary conditions and different predictions.2in terms of joint payoffs: as we take the length of the time intervals to zero, there is nosequence of pure or mixed perfect Bayesian equilibria in the discretetime game whoselimit sum of payoffs or experimentation rates would be higher than in the best SSE.The same holds true regarding the worst SSE joint payoff, which equals the singleagentpayoff.Both the best and the worst equilibrium are of the cutoff type, so that playersexperiment if and only if the belief exceeds a certain threshold. This contrasts with thenonexistence of such equilibria within the set of Markov equilibria; see Proposition 3 of Keller and Rady 2010. Surprisingly, firstbest can be attained for some parameters.Whether or not this is possible hinges on a simple comparison: does a success thearrival of a lumpsum at the cooperative threshold take the posterior belief above orbelow the singleagent threshold? If the posterior lies below the singleagent threshold,the cooperative solution can be implemented. Roughly speaking, this is because thepunishment applied when a nondeviant player has a success is most effective inthis case, giving a deviant player the lowest possible continuation payoff that ofeverybody giving up on experimentation forever. By contrast, if a success makes theplayers very optimistic, the opponents threat to stop experimenting has little impacton a deviant players payoff.We provide comparative statics regarding the cutoff in the best equilibrium and theassociated payoff. In particular, the larger the number of players, the lower the cutoff,and hence the larger the amount of experimentation.Section 2 introduces the model. Section 3 presents the main results regardingequilibrium payoffs and strategies both in the discrete and the continuoustime game.Section 4 contains the construction of SSE in the discretetime game which underliesour main results. Section 5 concludes.2 The Model The basic setup is that of Keller et al. 2005 and Keller and Rady 2010. Timet 0, is continuous. There are N 2 players, each facing the same twoarmedbandit problem with one safe and one risky arm.The safe arm S generates a known expected payoff s 0 per unit of time. The riskyarm R generates lumpsum payoffs that are independent draws from a timeinvariantdistribution on IR0 with a known mean h 0. These lump sums arrive at the jumptimes of a standard Poisson process whose intensity depends on an unknown state of 3the world, 0, 1. If 1, the intensity is 1 0 for all players; if 0, theintensity is 0 for all players with 0 0 1. These constants are again known tothe players. Conditional on , the Poisson processes that drive the payoffs of the riskyarm are independent across players.In the discretetime versions of the experimentation game, players may only changetheir action at the times t 0, , 2, . . ., for some fixed 0. The action is binaryusing the risky or safe arm. We refer to this game as the discrete game althoughit is cast in continuous time, to be contrasted with the analysis that we perform inthe continuoustime game see Section 3.3. While arguably natural, our discretizationremains nonetheless ad hoc, and other discretizations might possibly yield other results.Not only is it well known that limits of the discretetime models might differ fromthe continuoustime solutions, but the particular discrete structure might matter; see,among others, M uller 2000, Fudenberg and Levine 2009, Sadzik and Stacchetti2013, and H orner and Samuelson 2013.2The expected discounted payoff increment from using S for the length of time 0 r er t s dt 1 s with er , where r 0 is the common discountrate. Conditional on , the expected discounted payoff increment from using R isis R Eh R 0 r er t h d N,ti where N,t is a standard Poisson process with intensity ; as N,t t is a martingale, this simplifies to R 0 r er t h dt 1 h. We assumethat 0h s 1h, so each player prefers R to S if R is good 1, and prefers Sto R if R is bad 0.Players start with a common prior belief about . Thereafter, they observe eachothers actions and outcomes, so they hold common posterior beliefs throughout time.With p denoting the subjective probability that 1, the expected discounted payoffincrement from using R conditional on all available information is 1 ph withp p 1 1 p0. This exceeds the payoff increment from using S if and only ifp exceeds the myopic cutoff beliefpm s 0h1 0h.To derive the law of motion of beliefs, consider one of the intervals of length on which the players actions k 1, . . . , k N 0, 1N are fixed, with kn 1 indicating 2In H orner and Samuelson 2013, for instance, there are multiple solutions to the optimalityequations, corresponding to different boundary conditions, and to select among them it is necessaryto investigate in detail the discretetime game see their Lemma 3. But the role of the discretizationgoes well beyond picking the right boundary condition; see Sadzik and Stacchetti 2013.4that player n uses R, and kn 0 indicating that she uses S. With K PNn1 knplayers using the risky arm, the probability in state of a total of j 0, 1, 2, . . . lumpsums during this time interval is Kje K by the sum property of the Poissondistribution. Given the belief p held at the beginning of the interval, therefore, theprobability assigned to J lump sums arriving within the length of time isj!J,Kp K J JJ!cid:2p J1 K1 1 pJ0 K0 cid:3 ,with e, and the corresponding posterior belief is BJ,Kp p J1 K1p J1 K1 1 pJ0 K0.For K 0, the absence of a lumpsum payoff over the length of time makes 0,Kp p whenever p 0. Throughout the paper, we 0 . This guarantees that successesplayers more pessimistic: Bshall assume small enough that 1Nalways make players more optimistic: B1 0NJ,Kp p for all J 1, K 0 and p 1.For any bounded function w on 0, 1 and any K 0, 1, . . . , N, we define abounded function E K w by E K wp XJ0J,Kp wBJ,Kp.This is the expectation of w with respect to the distribution of posterior beliefs whenthe current belief is p and K players use R for a length of time .A history of length t , 2, . . . is a sequenceht cid:0kn,0Nn1, jn,Nn1, . . . , kn,tNn1, jn,tNn1cid:1 ,such that kn, 0 jn, 0. This history specifies all actions kn, 0, 1 takenby the players, and the resulting number of realized lumpsums jn, IN0. We write Ht for the set of all histories of length t, set H0 , and let H SIn addition, we assume that players have access to a public randomization device inevery period, namely, a draw from the uniform distribution on 0, 1, which is assumedto be independent of and across periods. Following standard practice, we omit itsrealizations from the description of histories.t0,,2,  Ht.Along with the prior belief p 0, each profile of strategies induces a distribution over H. Given a history ht, we can recursively define the beliefs p, p 2, . . . , pt through 5p Bn1 kn, .3J ,K p , where J PNn1 jn, and K PNA behavioral strategy n for player n is a sequence n,tt0,,2, , where n,t is amap from Ht to the set of probability distributions on 0, 1; a pure strategy takesvalues in the set of degenerate distributions only. A pure or behavioral strategy is a Markov stationary strategy if it depends on ht only through the posterior belief pt.A Markov strategy profile is symmetric if this map is the same for all players.Player n seeks to maximize the average discounted expected payoff By the law of iterated expectations, this equals1 E Xl01 E Xl0ln1 kn,ls kn,lho .ln1 kn,ls kn,lplho .Nash equilibrium, perfect Bayesian equilibrium and Markov perfect equilibrium of thegame with period length are defined in the usual way.4Our focus is on strongly symmetric equilibria. By definition, a strongly symmetricequilibrium SSE is a perfect Bayesian equilibrium in which all players use the samestrategy: nht nht, for all n, n and ht H. This implies symmetry of behaviorafter any history, not just on the equilibrium path of play.5 For 0 0, we shallactually restrict ourselves to purestrategy SSE; as we shall see, this entails no loss interms of equilibrium payoffs when we take the period length to 0.6 Endowing theset of histories with the product topology, the set of SSE outcomes for a given initialbelief is compact, and so is the set of SSE payoffs. If nonempty, this set is simply aninterval in IR. Its characterization is the subject of the next section.3Anticipating on the solution concept, this requires Bayes rule to be applied offpath as well. Asthe game has observable actions, this raises no particular difficulty.4While we could equivalently define this Bayesian game as a stochastic game with the commonposterior belief as a state variable, no characterization or folk theorem applies to our setup, as the Markov chain over consecutive states does not satisfy the sufficient ergodicity assumptions; see Dutta1995 and H orner, Sugaya, Takahashi and Vieille 2011.5Note that any symmetric Markov perfect equilibrium is a strongly symmetric equilibrium.6When 0 0, there exists no purestrategy SSE. The equilibria we construct in this scenarioinvolve mixed actions over a range of beliefs that vanishes as 0, so that the resulting outcome incontinuous time is achieved by a purestrategy automaton as defined in Section 3.3.63 Main Results In this section, we present the main results, discuss the intuition behind them andsketch the strategy of proof.3.1 SSE Payoffs in the Discrete Gamep and W p denote the supremum and infimum,Fix 0. For p 0, 1, let Wrespectively, of the set of payoffs over purestrategy strongly symmetric equilibria,given prior belief p.If such an equilibrium exists, these extrema are achieved, and Wp W p.Proposition 1 Suppose that Wsolve the functional equations W . The pair of functions w, w W, W wp wp max Kp;w,wn1 1 s ph E N wpo ,k0,1n1 1 ks kph E Kp;w,wmaxminN 1kwpo ,where Kp; w, w 0, 1 denotes the set of all such that1 1 s ph E N wp maxk0,1n1 1 ks kph E N 1kwpo .123Moreover, W w w Wfor any solution w, w of 13.The proof of this result, which is given in Appendix C, relies on arguments thatare familiar from Cronshaw and Luenberger 1994. The above equations can be understood as follows. The ideal conditions for a given symmetric action profile to beincentive compatible is that, if each player conforms to it, the continuation payoff isthe highest possible, while a deviation triggers the lowest possible continuation payoff. These actions are precisely the elements of Kp; w, w, as defined by equation 3.Given this set of actions, equation 2 gives the recursion that characterizes the constrained minmax payoff under the assumption that, if a player were to deviate to hismyopic bestreply to the constrained minmax action profile, the punishment would berestarted next period, resulting in a minimum continuation payoff. Similarly, equation1 gives the highest payoff under this constraint, but here, playing the best actionwithin the set is on the equilibrium path.7Our next step is to study the system 13 as the reaction lag vanishes.3.2 SSE Payoffs in the Continuous Limit As tends to 0, equations 12 transform into differentialdifference equations involving terms that are familiar from Keller and Rady 2010. A formal Taylor approximation shows that for any 0, 1 and K 0, 1, . . . , N,1 1 s ph E K wp wp rn1 s ph K bp, w wpo o,whereandbp, w prwjp wp 1 0rp1 p wp,jp 1pp.As in Keller and Rady 2010, we can interpret bp, w as the expected benefit ofplaying R when continuation payoffs are given by the function w. It weighs a discreteimprovement in the overall payoff after a single success, with the belief jumping upfrom p to jp, against a marginal decrease in the absence of such a success.7Applying this approximation to 12, cancelling the terms of order 0 in , dividing through by , letting 0 and using the notationcp s phfor the opportunity cost of playing R, we obtain the coupled differential equationswhich are at the heart of the following result.Proposition 2 As 0, the pair of functions Wp to a pair of functions w, w solving, W converges uniformly inwp s max Kpwp s min Kp Nbp, w cp ,N 1 bp, w maxk0,1k bp, w cp ,457As the belief is updated downward in the absence of a success we can compute bp, w wheneverw possesses a lefthand derivative at p.8whereand Kp 0, 1 for p p,for p p,0pcid:2Nwjp N 1wjp scid:3 rcp.67Proposition 2 will be proved jointly with our next result; more details are providedbelow.Equation 7, which characterizes the threshold below which no experimentationtakes place, admits a simple interpretation. An instant before all players switch tothe safe arm in the absence of a success, they afford the risky arm one last chance.The righthand side of 7 is each players instantaneous opportunity cost of usingthe risky arm rather than the safe one. The lefthand side is the longterm benefitfrom doing so, which only accrues in the event that a lumpsum arrives in the nextinstant. Because it is exceedingly unlikely that more than one player is successful,we may decompose this benefit into two terms, according to who receives the lumpsum.If ones own experimentation succeeds an event of instantaneous probabilityp, but no one elses, the continuation payoff wjp results; because everyoneelses experimentation fails, deviating to safe would result in a continuation payoff ofs, as the belief would necessarily drop below the threshold. If instead someone elsesexperimentation succeeds an event of instantaneous probability N 1p, thecommon belief would jump up to jp, and deviating to the safe arm would triggera punishment, lowering the continuation payoff from wjp to wjp. Adding upthese two benefits of conforming to the equilibrium strategy yields the lefthand sideof 7.Plainly, the system given in Proposition 2 appears more tractable than the onegiven in Proposition 1. Therein lies the benefit of continuous time. In fact, we nowderive an explicit solution for the unknowns w, w and p that appear in Proposition 2.Taking the threshold p and associated correspondence K as given at first, we canuse results from Keller and Rady 2010 to solve the equations 45. Adopting thesame notation as there, let V N be the Nplayer cooperative value function in continuoustime. It satisfies V N p s for p p N , where N , and V N p s for p pp N N s 0hN 11h s N s 0h9and N is implicitly defined as the unique positive root ofr N1cid:19N 0 N 1 0 0cid:180.On p N , 1, we havewith V N p ph N cp N ; N upup; N ,p cid:19up; 1 pcid:181 p.V N is once continuously differentiable, so that Nbp, V difference has a single zero at pleft. Setting N 1, we obtain the singleagent value function V cutoff pmin0,1N 1 bp, V Bellman equation for V N cp is continuous in p. This N , being positive to the right of it and negative to the 1 and corresponding 1 0 everywhere, we have 1 0, and 5 with this minimum set to zero is just the 1 always solves 5. In fact, as bp; V N , then 4 is trivially solved by V N .1 . Moreover, if p p 1. Clearly, V Next, we define a continuous function VN,p by setting VN,pp ph cpup; N up; N for p p, and VN,pp s otherwise. From Keller and Rady 2010, we know that VN,pis the players common payoff function in continuous time when all N of them use therisky arm on p, 1 and there is no experimentation otherwise; in particular, VN,pp s Nbp, VN,p cp on p, 1. For p p N , this is again the cooperative value function N on p N . For p p V N , 1, and VN,p is continuously differentiableexcept for a convex kink at p, which implies a discontinuity in Nbp; VN,p cp:this difference is positive on p, 1, approaches zero as p tends to p from the right,is positive at p itself, and then decreases monotonically as p falls further, eventuallyassuming negative values. All this implies that VN,p solves 4 when p p N . It nowremains to pin down p.N , we have VN,p V Proposition 3 The unique solution to the system 47 is w, w, p VN,p, V where p is the unique belief in p 1 satisfying N , p 1 , pp NVN,pjp N 1V 1 jp s rcp.Moreover, p p N if and only if jp N p 1, and p p 1 if and only if 0 0.10Figure 1 illustrates the cooperative continuoustime payoff V N , the lowest SSE limitpayoff V 1 , as well as the highest SSE limit payoff VN,p, which lies in between.1. Second, Section 4.1 shows that the functions VN,p and V As alluded to before, Propositions 2 and 3 will be proved together. First, Lemma B.1 in Appendix B establishes the existence of a unique belief p satisfying the definingidentity in Proposition 3 and proves the conditions under which this belief equals p Nor p 1 constitute upper andlower bounds, respectively, on SSE payoffs in the discrete game as vanishes. Third,Sections 4.24.3 construct SSE of the discrete game which in the limit get as closeto these bounds as one wishes, so that Section 4.4 can establish uniform convergence W1 , and thus the validity of Propositions 2 and 3. VN,p and W V A remarkable implication of Proposition 3 is that for a range of parameters, firstbest experimentation can be achieved in the limit. Furthermore, the necessary andsufficient condition for this to be the case is simply that a jump in the belief whena success is observed, starting from the cooperative threshold p N , does not take thecommon belief above the singleplayer threshold p 1. This is because, in such a configuration, there is no benefit from freeriding at or right above the threshold p N : failingto partake in the cooperative effort leads to the singleplayer continuation payoff of s,whether or not another player experiences a success or not. On the other hand, whenjp N which is specified when another playerhas a success is not enough: the deviating player can still secure a payoff above the safearms return, which depresses his incentives to experiment. Nonetheless, for 0 0,this punishment is not entirely ineffective, and helps push the experimentation threshold below the threshold that would prevail in the symmetric Markov equilibrium.8 Wehave the following result, proved in Appendix C as are the two results that follow it.1, the punishment for deviating at p N p Corollary 1 For 0 0, the cutoff p is strictly lower than the belief at which allexperimentation stops in the symmetric MPE of the continuoustime game.The unique symmetric Markov perfect equilibrium in Keller and Rady 2010 exhibits a doublebarrel inefficiency. Not only is the overall amount of experimentationtoo small, i.e. there is an inefficiently high probability of never finding out the truestate of the world in the long run; the speed of experimentation is inefficiently slow toboot. Strongly symmetric equilibria do better along both dimensions.98When 0 0, there is no difference between the best and worst continuation payoffs after asuccess: both equal 1h. This is the reason that experimentation cannot be sustained below p 1.9This holds even though the action set used in the continuoustime game defined by Keller and Rady 2010 is larger an action is a fraction allocated to the risky arm and there is no requirementthat the symmetric MPE be the limit of a sequence of discretetime equilibria.11w 1.51.41.31.21.110.20.30.40.50.60.70.80.91.0p Figure 1: Payoffs V 1, 1, 1.5, 1, 0.2, 5, implying p 1 dotted, V N solid and VN,p dashed. Here, r, s, h, 1, 0, N N , p, p 1 .27, .40, .45.It is also instructive to consider what happens when the players become infinitelyimpatient or patient. If players are myopic, they will not react to future rewards andpunishments. It is therefore no surprise that in this case the cooperative solution cannotbe sustained in equilibrium. By contrast, if players are very patient, the plannerssolution can be sustained provided the number of players is large enough.Corollary 2 For 0 0,andlimrjp N p 11hslimr 0jp N p 11N0,.Finally, in the case 0 0, the more players participate in the game the moreexperimentation can be sustained. Recall that for 0 0, the threshold belief p isindependent of N.Corollary 3 For 0 0, p is decreasing in N.This corroborates the comparative statics of the symmetric Markov equilibrium in Keller and Rady 2010: experimentation and payoffs increase in the number of players.12However, there are two important differences with the SSE that we construct below:first, the symmetric MPE is necessarily inefficient; second, behavior in the MPE is notof the cutoff type.3.3 SSE Strategies in the ContinuousTime Game Markov equilibria allow a simple characterization of the set of equilibrium payoffs viadynamic programming as well as a description of the corresponding strategies in amanner that unambiguously defines the equilibrium outcome, by requiring them to bemeasurable functions of the state variable. A similar description can be given here.Instead of considering a single measurable function, we must describe play by twofunctions depending upon whether the continuation payoff is maximum or minimum and a point process that acts as public randomization device. Given these threeelements, we define a twostate automaton which unambiguously defines an outcome.We can then provide a definition of SSE in continuous time, relative to this class ofstrategies. As with MPE, a player cannot gain from deviating to any other adaptedprocess as long as the other players follow the equilibrium strategy.To be more formal, let L be the set of all mappings from 0, 1 0, 1 to 0, 1that are leftcontinuous with respect to the second argument on the open unit intervaland switch between the two possible actions only finitely many times as the secondargument increases from 0 to 1. Any f L is considered as an augmented Markovstrategy, with f t, pt being the action taken at time t. A twostate automaton withthe reward state 1 and the punishment state 0 is determined by the initial statebelief pair 1, p, a strategy L and a measurable function : 0, 1 0, which governs the rate of transitions from the punishment to the reward state whenall players conform to the punishment action 0, pt; transitions in the other directionoccur instantaneously at the time of a unilateral deviation from the suggested commonaction 1, pt. More precisely, given , and any profile of strategies knNn1 in LN ,the joint dynamics of the state and the belief are defined through a random number ofstages as in Murto and V alim aki 2013; the details of this construction are presentedin Appendix A.Given an automaton , , each initial statebelief pair , p and profile knNn1of strategies is associated with welldefined payoffs Ecid:20Z 0rertn1 knt, pts knt, ptpthocid:12cid:12cid:12cid:12 0, p 0 , pcid:21n 1, . . . , N.Let u,, pk denote a players payoff from using strategy k L when all other 13players use the strategy .Definition 1 The automaton , is an equilibrium if u,, p u,, pk forall k L and all statebelief pairs , p. The pair of functions u,1, , u,0, are then called the equilibrium payoffs.This definition encompasses symmetric MPE as the special case in which the function does not depend on its first argument. Note that it does not require the strategyprofile to be a limit of equilibria of the discrete game.We now turn to the equilibrium that achieves the extreme payoffs determined in Proposition 3. While this equilibrium can be understood as the pointwise limit of the SSE of the discrete game that we construct to prove Propositions 23, working directlyin continuous time again results in a significantly cleaner description.Proposition 4 The twostate automaton defined by1, p 1pp,0, p 1p1and p 1p 1p1r bp, V VN,pp V 1 cp1 pis an equilibrium of the continuoustime game with payoffs VN,p, V 1 .The proof of this result can be found in Appendix A. In fact, it is shown there thatall pairs VN,p, V 1 including the first best are equilibrium payoffsin the sense of Definition 1. Yet, by Propositions 23, only those with p p can beapproximated by SSE payoffs of the discrete game.1 with p p N , p 3.4 Limit PBE Payoffs How restrictive are purestrategy SSE? Ones intuition suggests that it might be easierto reward only one player for playing risky with some positive probability than itis to give incentives to all the players to do so. Similarly, it might be more effectiveto punish just a single player who deviates unilaterally than to impose a punishmentphase on all players.However, as our next result shows, the restriction to strongly symmetric equilibriais without loss when it comes to the players average payoff and hence, to the rangeof beliefs at which experimentation is possible.14Proposition 5 Fix a prior p. In the limit as 0, the best and worst average payoffper player over all perfect Bayesian equilibria is achieved by an SSE. If 0 0, these SSE are in pure strategies.This implies, in particular, that for any o 0, there is a o 0 such that for all 0, o, the set of beliefs at which experimentation can be sustained in a perfect Bayesian equilibrium of the discrete game with period length is contained in theinterval p o, 1.The proof, presented in Appendix C, relies on an argument that is very similar to,but technically more complex than the one provided for SSE in Section 4.1.Loosely speaking, this result relies on the linearity of transition probabilities andpayoffs in the players actions: averaging actions over the set of players does not affectthe sum of instantaneous payoffs or the probability of a lump sum occurring. And thelinearity also ensures that averaging actions over the set of players preserves incentivecompatibility.4 SSE in the Discrete Game 4.1 Upper and Lower Bounds on Equilibrium Payoffs For 0, let p be the infimum of the set of prior beliefs at which the experimentation game with period length admits a strongly symmetric equilibrium with payoffexceeding s. Let p lim inf 0 p. For small o 0, consider the problem of maximizing the average of the players payoffs in the discretized setting subject to symmetryof actions at all times and no use of R at beliefs p p o. Denote the correspondoffs in any strongly symmetric equilibrium, and hence Wto this constrained optimization problem is feasible for the unconstrained planner in N for all 0, and henceing value function by f W ,o. By definition of p, there exists a o 0 such that for 0, o, the functionf W ,o provides an upper bound on the players common pay f W ,o. As the solutioncontinuous time, we have f W ,o V N . Lemma D.3 in the Appendix establishes that f W ,o VN,po uniformly as p p 0, where po max p o, p N .10N , implying W V As any player can choose to ignore the information contained in the other players 1 of a single agent experimenting inexperimentation results, the value function W 10The proof of this convergence result relies on the safe action being imposed on a closed interval.This is the reason why we work with the interval 0, p o and then take o 0.15isolation constitutes an obvious lower bound on a players payoff in any not juststrongly symmetric equilibrium, and so we have W W 1 . Lemma D.4 applied for p 1 establishes uniform convergence W 1 as 0.1 V Now, fix o 0 and consider a sequence of s smaller than o and converging to 0 such that the corresponding beliefs p converge to p. For each in this sequence,choose p p such that B0,N p p as well. If theplayers start at the belief p, therefore, and N 1 or all of them use R for unitsof time without success, then the posterior belief ends up below p and there is nofurther experimentation in equilibrium. Now, playing R at p against N 1 playerswho do so yields at most 0,N 1p p, and hence B1 ph r ph 1 rcid:261 Nps 0,N ps XJ1J,N pf W ,oBJ,N p Npf W ,oB s nr ph s N pVN,poj p so o,1 s J,N 1p r s 1 rcid:261 N 1ps J,N 1pW 0,N 1ps XJ11 B1,N pcid:27 owhile playing S yields at least N 1p W 1 B1,N 1pcid:27 o s nN 1 pV 1 j p so o.Incentive compatibility of R at p for small requires Letting o 0, we have po p and thus pcid:2NVN,poj p N 1V pcid:2NVN, pj p N 1V 1 j p scid:3 rc p 0.1 j p scid:3 rc p 0.By Lemma B.1, this means p p, which in turn implies the following result.16Proposition 6 For any o 0, there is a o 0 such that for all 0, o, the set ofbeliefs at which experimentation can be sustained in a strongly symmetric equilibriumof the discrete game with period length is contained in the interval p o, 1. Inparticular, lim sup 0 Wp VN,pp for all p.Proof: The statement about the range of experimentation follows immediately fromthe fact established at the start of this section that for o, we have Wand hence Wf W ,o,f W ,o s on 0, p o 0, p o. f W ,o for o, convergence f W ,o VN,po as 0, convergenceequality WVN,po VN, p for o 0, and the inequality VN, p VN,p.The statement about the supremum of equilibrium payoffs follows from the inIn addition, we obviously have lim inf 0 W p V 1 p for all p. In the followingsubsections, we show constructively that these bounds on the range of experimentationand the best and worst equilibrium payoffs are tight, that is, p p and, for all p,1 p. Our construction depends uponlim 0 Wwhether 0 0 or 0 0. Accordingly, we divide the analysis into two parts.p VN,pp and lim 0 W p V 4.2 The NonRevealing Case 0 0The equilibrium construction for 0 0 is inspired by the first part of the proof of Proposition 1. For sufficiently small 0, we shall exhibit a strongly symmetricequilibrium that can be summarized by two functions, and , which will not dependon . The equilibrium strategy is characterized by a twostate automaton.In thegood state, play proceeds according to and the equilibrium payoff satisfieswp 1 1 ps pph E N pwp,while in the bad state, play proceeds according to and the payoff satisfieswp maxk n1 1 ks kph E N 1pkwpo .89That is, w is the value from a players best response to all other players following .A unilateral deviation from in the good state is punished by a transition to the If therebad state in the following period; otherwise we remain in the good state.is no unilateral deviation from in the bad state, a draw of a public randomizationdevice determines whether the state next period is good or bad and guarantees thatthe payoff is indeed given by w; otherwise we remain in the bad state.17With continuation payoffs given by w and w, the common action 0, 1 canbe sustained at a belief p if and only if1 1 s ph E N wp 1 s 1 ph E N 11wp.10The functions and define an SSE, therefore, if and only if 10 holds for pand p at all p.The probability p of a transition from the bad to the good state in the absenceof a unilateral deviation from p is then pinned down by the requirement thatwp 1 1 ps pph11 np E N pwp 1 p E N pwpo .If k p is optimal in 9, we simply set p 0. Otherwise, 9 and 10 imply E N pwp wp 1 1 ps pph E N pwp,p so 11 holds withwp 1 1 ps pph E let p 1pp and p 1p p. Note that punishment and reward strategies agree We specify the functions and as follows: given p p, p 1 and p pm, 1,N pwpE N pwp E N pwp 0, 1.outside of p, p and that the strategies in Proposition 4 are obtained upon letting p pand p 1. The continuoustime payoff function associated with the common Markovstrategy is VN,p; we write V1, p for the continuoustime payoff function obtained from abest response against the opponents common strategy . In Appendix D, we establishuniform convergence w VN,p and w V1, p as 0, and V1, p V 1 as p 1.Proposition 7 For 0 0, there are beliefs p p, p 1 and p pm, 1 such thatfor all p p, p and p p, 1, there exists 0 such that for all 0, ,the twostate automaton with functions and defines a strongly symmetric perfect Bayesian equilibrium of the experimentation game with period length .Proof: See Appendix C.184.3 The Fully Revealing Case 0 0In the case 0 0, we were able to provide incentives in the potentially last round ofexperimentation by threatening punishment conditional on there being a success. Thisoption is no longer available in the case 0 0. Indeed, now any success takes us to aposterior of 1, so that everyone will play risky forever in any equilibrium. This meansthat irrespective of whether a success occurs or not, continuation strategies will beindependent of past behavior, conditional on the players belief about the state of theworld. This raises the possibility of unravelling. If we cannot support incentives justabove the candidate threshold below which play proceeds according to the symmetric Markov equilibrium, will the actual threshold not shoot up?To settle whether unravelling occurs or not requires us to study the discrete gamein considerable detail.11 Because the optimality equations for the discrete game areless tractable than their continuoustime analogue, their detailed analysis is relegatedto the Appendix.12First, we show that there is no perfect Bayesian equilibrium with any experimentation at beliefs below the singleagent cutoff p 1 infp : W 1 p s.Lemma 1 Let 0 0. Fix 0 and any prior belief p p Bayesian equilibrium outcome specifies that all players play safe in all periods.131 . Then the unique perfect Proof: See Appendix E.Lemma 1 already rules out the possibility that the asymmetric equilibria of Kelleret al. 2005 with an infinite number of switches can be approximated in discrete time.The highest payoff that can be hoped for, then, involves all players experimentingabove p 1 .11As already mentioned, we do not claim that the specific choice of the discrete game is innocuous:it might well be that requiring players to move in alternate periods, for instance, would yield differentconclusions.12These difficulties are already present in the study of symmetric Markov equilibria in discrete time.Unlike in the continuoustime limit, in which an explicit solution is known see Keller et al. 2005,the symmetric MPE in discrete time does not seem to admit an easy characterization. In fact, thereare open sets of beliefs for which there are multiple symmetric Markov equilibria in discrete time,no matter how small . It is not known whether these discretetime equilibria all converge in somesense to the symmetric equilibrium of Keller et al. 2005; in fact, it is not known whether somediscretetime MPE converges to it.13This does not extend to offpath behavior, of course. If a player deviates by pulling the risky armand obtains a success, players all switch to the risky arm from that point on.19Unlike for the case 0 0 see Proposition 7, an explicit description of a twostateautomaton implementing strongly symmetric equilibria whose payoffs converge to theobvious upper and lower bounds appears elusive. Partly, this is because equilibriumstrategies are necessarily mixed for beliefs that are arbitrarily close to but above p 1 ,as it turns out.The proof of the next Proposition establishes that the length of the interval ofbeliefs for which this is the case is vanishing as 0. In particular, for higher beliefsexcept for beliefs arbitrarily close to 1, when playing R is strictly dominant, bothpure actions can be enforced in some equilibrium.Proposition 8 For 0 0, and any beliefs p and p such that pthere exists 0 such that for all 0, , there exists 1 p pm p 1, a strongly symmetric equilibrium in which, starting from a prior above p, allplayers experiment on the path of play as long as the belief remains above p, andstop experimenting once the belief drops below p 1; a strongly symmetric equilibrium in which, given a prior in between p and p, theplayers payoff is no larger than their bestreply payoff against opponents whoexperiment if and only if the belief lies in p 1, p p, 1.Proof: See Appendix E.Intuitively, given that the interval p While this proposition is somewhat weaker than Proposition 7, its implicationsfor limit payoffs as 0 are the same.1, pcan be chosen arbitrarily small actually, of the order , as the proof establishes, itsimpact on equilibrium payoffs starting from priors above p is of order . This suggeststhat for the equilibria whose existence is stated in Proposition 8, the payoff converges,respectively, to the payoff from all players experimenting above p 1 and to the bestreplypayoff against none of the opponents experimenting. We now turn to proving this claimrigorously and establishing uniform convergence.4.4 Limit SSE Payoffsand W for the pointwise supremum and infiRecall that, for fixed , we write Wmum, respectively, of the set of strongly symmetric equilibrium payoff functions, which,by Proposition 5, in the limit coincide with the pointwise supremum and infimum ofaverage perfect Bayesian equilibrium payoffs, as the period length vanishes. The mainresult of this section is a characterization of the limit of Wand W .20Proposition 9 lim 0 W VN,p and lim 0 W V 1 , uniformly on 0, 1.Proof: For 0 0 and a given o 0, the explicit representation for VN,p in Section 3.2 and the uniform convergence V1, p V 1 as p 1 established in Lemma D.5allow us to choose 0, p p, p and p p, 1 such that k VN,p VN,pk o,k VN,p VN,pk o and k V1, p V 2, with k k denoting the supremum norm. Next,Proposition 7, Lemma D.7, Section 4.1 and Lemma D.4 imply the existence of a 0such that for all 0, , the twostate automaton defined by the cutoffs p and p constitutes an SSE of the game with period length and the following inequalities 1 k o. For 0, ,hold: w VN,p, Wwe thus have VN,p, kw V1, pk o 2 and k W 1 k o 1 V and VN,p o VN,p w W VN,p VN,p o V 1 o W 1 W w V1, p o 2 V 1 o,so that k W VN,pk and k W V 1 k are both smaller than o, which was to be shown.1, p For 0 0, the proof of Proposition 8 establishes that there exists a natural number M such that, given p as stated, we can take to be p p 1M. Equivalently,1 M p. Hence, Proposition 8 can be restated as saying that, for some 0,pand all 0, , there exists p p 1 M such that the two conclusions ofthe proposition hold with p p. Fixing the prior, let w, w denote the payoffs inthe first and second SSE from the proposition, respectively.14 Given that p p 1 and 1, p, it follows that we can pick 0, suchwp s, wp s for all p pthat for all 0, , w VN,po, and as before, W2 andk W 1 k o. The obvious inequalities follow as before, subtracting an additional oto the lefthand side of the first one; and the conclusion follows as before, using 2o asan upper bound. VN,p, kw V1, pk o 1 V 5 Conclusion This paper has characterized the strongly symmetric equilibrium payoffs in a standardmodel of strategic experimentation. As a proof of concept, our analysis demonstratesthat this solution concept offers a good compromise between two objectives: preserving the flexibility of dynamic programming, even in continuous time replacing the 14Hence, to be precise, these payoffs are only defined on those beliefs that can be reached given theprior and the equilibrium strategies.21HJB equation by a pair of coupled optimality equations, yet allowing for the rewardsand punishments that are the hallmark of dynamic games. Our point is not thatthis concept is necessarily preferable to either Markov equilibrium or perfect Bayesianequilibrium, if a model lends itself to systematic analysis. Each yields specific insights.Relative to the literature on strategic experimentation, the paper delivers threefindings. First, it validates some of the comparative statics of Markov equilibria: payoffs and experimentation increase with the number of players for 0 0, despite thefreeriding incentives. Second, and more importantly, in terms of behavior: the highestand lowest joint surpluses are achieved by equilibria in which players adhere to a simplecommon conduct; unlike in any Markov equilibrium, onpath play is of the cutoff type,with players experimenting at maximum rate until some threshold is reached.15 Third,in terms of efficiency: when information accrues at sufficiently moderate speed in thesense that lumpsums are not too informative, the best equilibrium achieves the firstbest.Obviously, some of these conclusions will not carry over to other applications. Forinstance, it is known that strongly symmetric equilibria are restrictive when actions areimperfectly monitored, at least if the monitoring structure permits statistical discrimination among deviations by different players; see Fudenberg, Levine and Takahashi2007. Clearly, the linearity and symmetry of both payoffs and transition probabilitiesin the players actions also play a role in our argument. Nonetheless, such features arecommon in applications; the model of Bolton and Harris 1999, for example, in whichthe players learn about the drift of a Brownian motion, shares them with our setup.It would be interesting to get more general sufficient conditions for the restriction tostrongly symmetric equilibria to be innocuous, just as it would be to apply the solutionconcept to specific applications where it is not.15Recall that in the symmetric MPE in Keller et al. 2005 and Keller and Rady 2010, playerschoose an interior level of experimentation at intermediate beliefs. More generally, Keller and Rady2010 show that there is no MPE in which all players use a cutoff strategy.22Appendix A TwoState Automata in Continuous Time Consider an automaton , as defined in Section 3.3 and any profile of strategies knNn1in LN . The first stage of the game is a reward stage with the initial statebelief pair 1, p.While this stage lasts, the state remains unchanged and the belief evolves according to Bayeslaw: pt 1 0pt 1 ptkn1, pt;NXn1see Keller and Rady 2010 for a derivation of this law of motion. The stage ends at the firsttime 0 at which there is a breakthrough on a risky arm or kn1, p 6 1, p for somen. In particular, the stage ends at time 0 if kn1, p 6 1, p for some n.If the first stage ends because of a breakthrough, another reward stage starts at time with the initial statebelief pair 1, jp . Play then proceeds exactly as in the first stage.If a reward stage ends because kn1, p 6 1, p for some n, a punishment stage startsat time with the initial statebelief pair 0, p . While this stage lasts, the state remainsunchanged and the belief evolves according to pt 1 0pt 1 ptkn0, pt.NXn1The stage ends at the random time min , where is the time of the first breakthroughon a risky arm in that stage and a random time in , with 1P t 1 exp Z t NYn1kn0,ps0,ps ps ds! .Conditional on no breakthrough occurring, therefore, the rate of transition out of the punishment stage is pt if all players act as prescribed by 0, , and zero otherwise. If ,another punishment stage starts at time with initial statebelief pair 0, jp . If ,a reward stage starts at time with the initial statebelief pair 1, p .The pair , is an equilibrium if and only if the payoff functions u u,1, ,u u,0, , the statecontingent strategies 1, and 0, and the transitionrate satisfy the following four conditions at all beliefs p:up s p N bp, u cp ,up uporup up s N 1pbp, u 1 p bp, u cpup s p N bp, u cp prup up ,up s N 1pbp, u 1 p bp, u cp .A.1A.2A.3A.423Equations A.1 and A.3 characterize a players payoffs when conforming to the strategysuggested by the automaton. Inequalities A.2 and A.4 state that there is no incentive todeviate from that strategy. In the reward state, such a deviation amounts to applying animpulse control that immediately moves the statebelief pair from 1, p to 0, p. Wheneverup up, the resulting discrete drop in continuation payoffs suffices to deter deviations. Ifup up, however, the rate at which continuation payoffs change when a player conformsmust be at least as large as when he deviates, hence the second part of A.2.If the action p suggested in the punishment state satisfies2p 1 bp, u cp 0,then A.4 holds for any nonnegative p; the harshest possible punishment is then generatedby setting p 0. If2p 1 bp, u cp 0,then we must have up up in A.2, and A.3 implies A.4 for anyr 1 2p bp, u cp.p up upIn this case, it is without loss of generality to set p equal to this lower bound. In eithercase, we see that conditions A.3A.4 can be replaced by the equation 1, the strategies 1pp and 1p1 can be supported in an equiup s N 1pbp, u maxk0,1For any p pk bp, u cp .A.5N , plibrium. In fact, the corresponding payoff functions u VN,p and u V 1 satisfy conditionsA.1 and A.5 by construction, and A.2 holds because bp, u cp on 0, p, u u onp, 1, and bp, u bp, u 0 cp at p 1.B An Auxiliary Result Lemma B.1 There is a belief p p N , p 1 such thatph N VN,pjp N 1V 1 jp si rcpis negative if 0 p p, zero if p p, and positive if p p 1. Moreover, p ponly if jp 1 if and only if 0 0.1, and p p N p N if and Proof: We start by noting that given the functions V uniquely determined by 1 and V N , the cutoffs p 1 and p N arep 1V 1 jp 1 s rcp 1B.624andrespectively.p N N V N jp N N s rcp N ,B.7Consider the differentiable function f on 0, 1 given byf p pN VN,pjp N 1V 1 jp s rcp.For 0 0, we have jp 1 and VN,pjp V 1 jp s rcp, which is zero at p p 1 jp 1h for all p, so f p 1, and negative 1 by B.6, positive for p ppV for p p 1.Assume 0 0. For 0 p p 1, we have VN,pp ph cpup; N up; N p cid:17with the function up; 1 pcid:16 1p 1 p s when p pwe have V otherwise. Using the fact that 1, and V which is strictly convex for 0. Moreover,1 p ph Cup; 1 with a constant C 0ujp; 01cid:19pcid:18 0up; ,1 jp is also linear in p; when jp pwe see that the term pN VN,pjp is actually linear in p. When jp p 1, the termpN 1V 1, the nonlinear part of this termsimplifies to N 1C111 . This shows that f is concave, and strictly concaveon the interval of all p for which jp p 1. As limp 1 f p 0, this in turn implies that fhas at most one root in the open unit interval; if so, f assumes negative values to the left ofthe root, and positive values to the right.up; 110As VN,p 1jp 1 V 1 jp 1, moreover, we havef p 1 p 1V 1 jp 1 s rcp 1 0by B.6. The potential root of f must thus lie in 0, pand 1. If jp N p 1, then V 1 jp N sf p N p N N V N jp N N s rcp N 0by B.7. If jp N p 1, then V 1 jp N s and f p N 0, so f has a root in p N , p 1.C Proofs Proof of Proposition 1: Fix a pair w, w that satisfies 13. Note that 12 implythat w w. Given such a pair, and any prior p, we construct two SSE whose payoffs arerespectively w and w. It then follows that W w w W. Let and denote aselection of the maximum and minimum of 12. The equilibrium strategies are describedby a twostate automaton, whose states are referred to as good or bad. The differencebetween the two equilibria lies in the initial state: w is achieved when the initial state is 25good, w when it is bad. In the good state, play proceeds according to ; in the bad state,according to . Transitions are as follows. If the state is good and all players play , playremains in the good state; otherwise, play shifts to the bad state. If after some history h,the state is bad and all players play , play switches from the bad state to the good statewith some probability p 0, 1 where p is the belief held after history h. This switch isdetermined by the public randomization device i.e., the switch is a deterministic function ofits realization. Otherwise, play remains in the bad state. The probability p is chosen sothatwp 1 1 ps pphC.8 np E N pwp 1 p E N pwpo ,It remains to show that Wwith 13 ensuring that p 0, 1. This completes the description of the strategies. Thechoice of along with 12 rules out profitable oneshot deviations in either state, so thatthe automaton describes equilibrium strategies, and the desired payoffs are obtained., W solve the functional equations whenever W W ., W . This Note that in any SSE, given p, the action p must be an element of Kp; Wis because the lefthand side of 3 with w Wis an upper bound on the continuationpayoff if no player deviates, and the righthand side with w W a lower bound on thecontinuation payoff after a unilateral deviation. Consider the equilibrium that achieves W.Then Wp max Kp;W,W n1 1 s ph E po ,N W, W and the continuation payoff is at most givenas the action played must be in Kp; W. Similarly, W must satisfy 2 with instead of . Suppose now that the by Wwere strict. Then we can define a strategy profile given prior p that i in period 0, plays themaximizer of the righthand side, and ii from t onward, abides by the continuation, W , this constitutesstrategy achieving Wan equilibrium; and it achieves a payoff strictly larger than Wp, a contradiction. Hence,1 must hold with equality for W. The same reasoning applies to W and 2.p. Because the initial action is in Kp; WProof of Corollary 1: Keller and Rady 2010 establish that in the unique symmetric Markov perfect equilibrium of the continuoustime game, all experimentation stops atthe belief p N implicitly defined by rc p N p N uj p N s, where u is the playerscommon equilibrium payoff function. The results of Keller and Rady 2010 further implythat VN, p N j p N uj p N V 1 j p N uj p N , and hence p p N by Lemma B.1.1 j p N , so that N VN, p N j p N N 1V Proof of Corollary 2: Simple algebra yieldsjp N p 110N11 11h s 1s 0hN 11h s 10N s 0h.26From the implicit definitions of 1 and N , we obtain limr 0 1 limr 0 N 0 so thatthe third fraction in the previous expression converges to 1 andlimr 01rcid:201 0 0 ln 01cid:211 N limr 0Nrimplyingby l Hopitals rule.limr 0N11NFurthermore, we note that we can write equivalentlyjp N p 1101 111h s s 0h1 1N1h s 10s 0h.As limr 1 limr N , we can immediately conclude thatlimrjp N p 11hs.Proof of Corollary 3: For the case that p p2010. Thus, in what follows we shall assume that p p N .N , this is shown in Keller and Rady Recall the defining equation for p from Lemma B.1,pN VN,pjp ps rcp N 1pV 1 jp.We make use of the closedform expression for VN,p to rewrite its lefthand side as N pjph N cp0 N 1 0 ps.Similarly, by noting that p pform expression for V 1 to rewrite the righthand side as N implies jp jp N p 1, we can make use of the closedN 1pjph N 1cp 1up; 1up 1; 1r 0 11 0.Combining, we havepjph N cp0 N 1 0 psN 1r 0 11 0cp 1up; 1up 1; 1.It is convenient to change variables to 01and y 101h ss 0hp 1 p.27The implicit definitions of 1 and N imply N 11 11 1N N 1 ,allowing us to rewrite the defining equation for p as the equation F y, N 0 with F y, 1 y 1 y 1 11 11 1 1 11 1111 111y 1.As y is a strictly increasing function of p, we know from Lemma B.1 that F , N admits aunique root, and that it is strictly increasing in a neighborhood of this root.A straightforward computation shows that F y, N 1 11 11 1 N 1 11 1N 2 y, N withy, 1 1 1y 1 1 1 y1 11 1 1 y ln.As p N p p 1, we have N1 N y 11 1,which impliesandy, 1 1 1y 1 1 11 log 0y, 11 y ln2 0for all N , 1. This establishes y, N 0.By the implicit function theorem, therefore, y is increasing in N . Recalling from Kellerand Rady 2010 that N is decreasing in N , we have thus shown that y and hence p aredecreasing in N .Proof of Proposition 5: For any given 0, let p be the infimum of the set of beliefsat which there is some possibly asymmetric perfect Bayesian equilibrium that gives a payoffwnp s to at least one player. Let p lim inf 0 p. By construction, p p.For any fixed o 0 and 0, consider the problem of maximizing the players averagepayoff subject to no use of R at beliefs p p o. The corresponding value function W ,ois the unique fixed point in the space of bounded functions on the unit interval of the 28contraction mapping given by T ,owp 1N max K0, ,N n1 Kph N Ks E 1 s wpK wpo if p p o,if p p o.Let po max p o, pments as in the proof of Lemma D.3.N . Uniform convergence W ,o VN, po follows from the same arguConsider a sequence of s converging to 0 such that the corresponding beliefs p convergeto p. For each in this sequence, select a perfect Bayesian equilibrium as well as a beliefp p starting from which a single failed experiment takes us below p. Let L bethe number of players who, at the initial belief p, play R with positive probability in theselected equilibrium. Let L be an accumulation point of the sequence of Ls. After selectinga subsequence of s, we can assume without loss of generality that player n 1, . . . , L playsn 0 at p, while player n L 1, . . . , N plays S; we can further R with probability assume that n1 converges to a limit nLn1 in 0, 1L.n LFor player n 1, . . . , L to play optimally at p, it must be the case thatn ph 1 Prwn, PrIJ,Kpw1 cid:2 1 s n scid:3 L1XK1 XIK, n 6ILXK1 XIKXJ0Prnwn, PrnIJ,Kpw,n,I,JXJ0n,I,Jwhere we write PrI for the probability that the set of players experimenting is I 1, . . . , L, PrnI for the probability that among the L 1 players in 1, , L nthe set of players experimenting is I, and wn,I,J for the conditional expectation of playerns continuation payoff given that exactly the players in I were experimenting and had Jn, is player ns continuation payoff if no one was experimenting. As Pr successes wn Prn Pr1 n, by itslower bound s. After subtracting 1 s from both sides, we then haven, the inequality continues to hold when we replace w Prs PrIJ,Kpw1 n cid:2ph scid:3 L1XK1 XIK, n 6Ins Pr LXK1 XIKXJ0XJ0n,I,JPrnIJ,Kpw.n,I,J29n1 n yields PrLs Summing up these inequalities over n 1, . . . , L and writing 1LPL1 L cid:2ph scid:3 n,I,J LXn1J,K p for all players n 1, . . . , N , and hence PLn,I,0 s whenever I 6 . For I K 0 and J 0, moreover,1 Bn,I,J J,Kp. So, for the preceding inequality to hold it is LXK1 XIKXJ0n,I,J W J,Kp N LW wn,I,JL1XK1 XIK, n 6IJ,Kpw XJ0J,KpLXn1LXn1ns n1 w PrIPrnI1 BPr.By construction, wwe have w N W ,oBnecessary that PrI0,K pLs J,Kp N LW 1 BJ,K pi PrLs LXK1 XIKJ,Kph N W ,oBL1XK1 XIK, n 6IXJ1nIPr PrnI0,K ps J,KpW 1 BJ,Kp.PrI1 L cid:2ph scid:3 XJ1LXK1 XIK LXn1LXn1L1XK1 XIK, n 6ILXn1LXK1 XIKPr ns Pr AsPrI 1 and LXK1 XIKPrIK L ,we have the firstorder expansions Pr PrI0,K pLXK1 XIK Pr LXK1 XIKPrIcid:01 Kpcid:1 o 1 L p oand LXK1 XIKPrI1,K p LXK1 XIKPrIKp o L p o,30so the lefthand side of the last inequality expands as Ls Lcid:26r ph s rs p N VN, poj p N LV 1 j p Lscid:27 owith lim 0 . In the same way, the identities Prn L1XK1 XIK, n 6IPrnI 1 and L1XK1 XIK, n 6IPrnIK L n Prn 0,K p L LL 1 p o1,K p LL 1 p o,and so the righthand side of the inequality expands as Comparing terms of order , dividing by L and letting o 0, we obtainimply LXn1and LXn1Pr PrnInIL1XK1 XIK, n 6ILXn1L1XK1 XIK, n 6ILs Ln rs L 1 p V n pcid:2N VN, pj p N 1V 1 ph 1 s L1XK0 XIK, n 6IPrnwn, PrnIXJ0L1XK1 XIK, n 6I1 j p so o.1 j p scid:3 rc po 0.n,I n,JXJ0J,KpwnIPr J,K1pw.n,I,J31By Lemma B.1, this means p p whenever 0.For the case that 0, we write the optimality condition for player n 1, . . . , L as As above, wover, we have w N W ,oBn, s, and wn,I,J W n,I,0 s whenever I 6 . For I K 0 and J 0, more1 Bn,I n,J J,K1p. So, for the optimality condition to hold, it J,K1p N 1W J,Kp, w 1 BJ,K1p and wn,I n,J W 1 BJ,K1p N 1W 1 BJ,K1pi Prns nI0,K psis necessary that1 ph L1XK0 XIK, n 6I 1 s Pr Now,L1XK0 XIK, n 6IXJ1nIPrnI0,K1ps J,K1ph N W ,oBL1XK1 XIK, n 6IXJ1nIPr PrL1XK1 XIK, n 6IL1XK1 XIK, n 6IJ,KpW 1 BJ,K p.PrnIK L n 0as vanishes. Therefore, the lefthand side of the above inequality expands ass cid:26r ph s p N VN, poj p N 1V 1 j p scid:27 o,and the righthand side as s o. Comparing terms of order , letting o 0 and using Lemma B.1 once more, we again obtain p p.Given that we have p p, therefore, the result follows directly from Proposition 9.Proof of Proposition 7: We take p as in Lemma D.8; Lemma D.9 ensures that p p.We fix a p p, p. By Lemma B.1,pN VN,pjp N 1V 1 jp s rcp 0on p, 1. As VN,pjp VN,pjp for p p, this impliespN VN,pjp N 1V 1 jp s rcp 0on p, 1. By Lemma D.5, there exists a belief p pm such that for all p p, infp :V1, pp s p, p 1 andpN VN,pjp N 1V1, pjp s rcp 0C.9on p, 1. We fix a p p, 1 and define p infp : V1, pp s.By Lemmas D.7 and D.8, there is a 0 0 such that w VN,p w on the unitinterval for all 0. For any such and any p 0, p, the common action p p 0 trivially satisfies the incentive constraint 10. In fact, since wp s, we have 321 s E 1 s E 0 wp 1 ph E 0 wp 1 ph E 1 wp by 9; as w w, this in turn implies 1 wp.For all 0 and p p, 1, moreover, the common action p p 1 satisfies N 1VN,pp the incentive constraint 10 because ph s and E E N 1wp, where the second of these inequalities follows from convexity of VN,p.N VN,pp E N wp E Now, let 1 0 be such thatpN VN,pjp N 1V1, pjp s rcp 1C.10for all p p, p. Such a 1 exists by C.9 and the continuity of its lefthand side in p. Fixp p, p such thatN p rh VN,pp si 13.C.11By Lemma D.4, there exists a 1 0, 0 such that for 1, wp s on 0, p. Bythe same argument as above, this implies that for these , the common action p 0satisfies the incentive constraint 10 on p, p as well.In the remainder of the proof, we simplify the notation by writing p KJ,Kp, theposterior belief starting from p when K players use the risky arm and J lumpsums arrivewithin the length of time .J for BFor p p, p and p 1, the lefthand side of the incentive constraint 10expands asr ph 1 rcid:8N p wp N0 cid:8rph N pwp N wp N1 1 N p wp N1 N p rwp Nand the righthand side asr s 1 rnN 1p wp N 11 nrs N 1pwp N 11 wp N 10 1 N 1p wp N 1 N 1p rwp N 10 cid:9 O20 cid:9 O2,o O2o O2.00For 1, we have wp N0righthand sides is no smaller than times 0 s wp N 1, so the difference between the lefthand andph N wp N1 N 1wp N 11plus terms of order 2 and higher. si rcp N p rcid:2wp N0 scid:31Let o 15N 1r . By Lemmas D.6 and D.4 as well as Lipschitz continuity of VN,pand V1, p, there exists 2 0, 1 such that for 2, kw VN,pk, kw V1, pk,maxppp VN,pp N V1, pjp are all smaller 1 VN,pjp and maxppp V1, pp N 1133than o. For 2, we thus havewp Nwp N 1wp N11 VN,pjp 2o, V1, pjp 2o,0 VN,pp N0 o,so that the expression displayed above is larger than 1 5N 2p ro 13 13by C.10, C.11 and the definition of o. This implies that there is a 3 0, 2 such thatfor all 3, the incentive constraint 10 holds for on p, p.As VN,p V1, p on p, 1, there exist 4 0, 3 and 2 0 such that VN,pp N 10 V1, pp N 10 2C.12for all 4 and p p, p. At any belief p in this interval, the difference between the lefthand and righthand sides of 10 for p 1 is wp NO. By Lemmas D.6 and D.4 and Lipschitz continuity of VN,p, there exists 4 0, 3 such that for 4,kw VN,pk, kw V1, pk and maxpp p VN,pp N are all smaller than 24.For 4 and p p, p, we thus have wp N 22and wp N 1 24, so that by C.12 the difference between the lefthandand righthand sides of 10 for p 1 is larger than 24 O. Thus, there is a 5 0, 4 such that for all 5, 10 holds for on p, p.0 VN,pp N 100 24 VN,pp N 10 VN,pp N0 wp N 1 V1, pp N 10000For p p, p and p 0, the difference between the lefthand and righthandsides of 10 is wp wp 10 O, and the same steps as in the previous paragraphyield existence of a 0, 5 such that for all , the incentive constraint 10 for is also satisfied on p, p.D Convergence and Comparison Results To establish uniform convergence of certain discretetime value functions to their continuoustime limits, we will need the following result.16Lemma D.1 Let T 0 be a family of contraction mappings on the Banach space W; kkwith moduli 0 and associated fixed points w0. Suppose that there is a constant 0 such that 1 o as 0. Then, a sufficient condition for w toconverge in W; k k to the limit v as 0 is that k T v vk o.Proof: Askw vk k T w vk k T w T vk k T v vk kw vk k T v vk,16To the best of our knowledge, the earliest appearance of this result in the economics literature isin Biais et al. 2007. A related approach is taken in Sadzik and Stacchetti 2013.34the stated conditions on and k T v vk implykw vk k T v vk 1 f gf gwith lim 0 f lim 0 g 0.In our applications of this lemma, we shall take W to be the Banach space of boundedfunctions on the unit interval, equipped with the supremum norm. The operators T willbe Bellman operators for certain optimal strategies in the experimentation game with periodlength ; the corresponding moduli will be er.The limit functions will belong to the set V of all continuous v W with the followingproperties: there are finitely many beliefs plLl0 with 0 p 0 p 1 . . . p L1 p L 1such that for all l 1, . . . , L, i the function v is once continuously differentiable withbounded derivative v on the interval pl 1, pl, ii limppl vp equals the lefthand derivativeof v at pl, and iii limppl 1 vp equals the righthand derivative of v at pl 1.In thefollowing, we shall always take vpl to mean the lefthand derivative at pl for l 1, andthe righthand derivative for l 0.With this convention, the termbp, v prvjp vp 1 0rp1 p vpis welldefined on the entire unit interval for any v V. We can now provide a firstorderexpansion for the discounted expectation E K that will appear in the Bellman operators ofinterest.17Lemma D.2 For K 0, 1, . . . , N and v V ,lim 01cid:13cid:13 E K v v rKb, v vcid:13cid:13 0.Proof: This follows from a straightforward Taylor expansion.Our first application of Lemmas D.1 and D.2 concerns the upper bound on equilibriumpayoffs introduced at the start of Section 4.1. Take p as defined there. Given 0, o 0and any bounded function w on 0, 1, define a bounded function e T ,ow bye T ,owp maxn1 ph E The operator e T ,o satisfies Blackwells sufficient conditions for being a contraction mapping N wp, 1 s wpo if p p o,with modulus on the Banach space W of bounded functions on 0, 1 equipped with the1 s wpif p p o.17Up to discounting, this is nothing but the computation of the infinitesimal generator of the processof posterior beliefs, of course.35considered in Section 4.1.supremum norm kk: monotonicity v w implies e T ,ov e T ,ow and discounting e T ,owc e T ,ow c for any real number c. By the contraction mapping theorem, e T ,o has aunique fixed point in W; this is the value functionf W ,o of the constrained planners problem From Keller and Rady 2010, we know that the corresponding continuoustime value N . It belongs to V and satisfies VN,pop ph N , moreover, ph N bp, VN,po s is zero at po andfunction is VN,po with po max p o, p N bp, VN,po s on po, 1. For po pnegative on 0, po.Lemma D.3 f W ,o VN,po uniformly as 0.Proof: To ease the notational burden, we write v instead of VN,po. Lemma D.2 then implies1 ph E N vp vp r ph N bp, v vp o,1 s vp vp r s vp o.Suppose first that po p o p N . For p p o, we have vp ph N bp, v s,Next, suppose that po p N p o. For p po, pfor small .N vp vp o for small .N , the same argument as in the previous N vp vp o for small . For p p and hence e T ,ovp 1 ph E paragraph yields e T ,ovp 1 ph E N , we have vp s ph N bp, v, which once more implies e T ,ovp vp oAs e T ,ovp s vp trivially on 0, po, we have established that ke T ,ov vk o.As the modulus of the contraction e T ,o is er 1 r o, uniform convergencef W ,o v now follows from Lemma D.1.The second application of Lemmas D.1 and D.2 concerns the payoffs in the bad state ofthe equilibrium constructed in Section 4.2. Fix a cutoff p pm, and let Kp N 1 whenp p, and Kp 0 otherwise. Given 0, and any bounded function w on 0, 1, definea bounded function T w by T wp maxn1 ph E Kp1wp, 1 s E Kpwpo .The operator T also satisfies Blackwells sufficient conditions for being a contraction mapping with modulus on W. Its unique fixed point in this space is the payoff function wintroduced in Section 4.2 from playing a best response against N 1 opponents who allplay risky when p p, and safe otherwise. For p 1, the fixed point is the singleagent valuefunction W 1 .In Section 4.2, we introduced the notation V1, p for the continuoustime counterpart to thispayoff function. The methods employed in Keller and Rady 2010 can be used to establishthat V1, p has the following properties. First, there is a cutoff p pm such that V1, p son 0, p, and V1, p s everywhere else. Second, V1, p V, being continuously differentiable 36everywhere except at p. Third, V1, p solves the Bellman equationvp maxnph Kp 1bp, v, s Kpbp, vo .Fourth, because of smooth pasting at p, the term ph bp, V1, p s is continuous in pexcept at p; it has a single zero at p, being positive to the right of it and negative to theleft. Finally, we note that V1, p V 1 and p p 1 for p 1.Let p, infp : wp s.Lemma D.4 w V1, p uniformly as 0, and lim inf 0 p, p.Proof: To ease the notational burden, we write v instead of V1, p.For p p, we have Kp N 1, and Lemma D.2 implies1 ph E 1 s E Kp1vp vp r ph N bp, v vp o,Kpvp vp r s N 1bp, v vp o.As vp ph N bp, v s N 1bp, v, we thus have T vp 1 ph E Kp1vp vp o for small .On p, p, we have Kp 0 and1 ph E Kp1vp vp r ph bp, v vp o,1 s E Kpvp vp r s vp o.As vp ph bp, v s, we again have T vp 1 ph E vp o for small .Kp1vp For p p, finally, we have Kp 0 and vp s, hence1 ph E Kp1vp s r ph bp, v vp o,1 s E Kpvp s.As vp s ph bp, v, this once more implies T vp vp o for small .We have thus shown that k T v vk o. Uniform convergence w v now followsfrom Lemma D.1.Turning to the second part of the lemma, we define p,0 lim inf 0 p,. For a sequenceof s converging to 0 such that the corresponding beliefs p, converge to p,0, choose p p, such that B0,1pand 1 ph E 0,1p p,. Along the sequence, we have wp s wB1 wp 1 s wp s. As1 ph E 1 wp r ph 1 rcid:261 ps p wcid:0B s nrp,0h s p,0vjp,0 so o,1,1pcid:1cid:27 o37we can conclude that p,0vjp,0 s rcp,0. As vp 0 and pvjp s rbp, v rcp for p p, this implies p,0 p. And since the inequality p,0 p wouldimply vp s lim 0 wp immediately to the right of p, we must have p,0 p.Our third uniform convergence result also concerns the continuoustime limits of equilibrium payoffs in the bad state. As it is straightforward to establish with the methods used in Keller and Rady 2010, we state it without proof.Lemma D.5 V1, p V p p implies V1, p V1, p on p : s V1, pp 1h.1 uniformly as p 1. The convergence is monotone in the sense that Our last result on uniform convergence concerns the payoffs in the good state of theequilibrium constructed in Section 4.2. Fix a cutoff p and consider the strategy profile whereall N players play risky for p p, and all play safe otherwise. As in Section 4.2, we write wfor the players common payoff function from this strategy profile when actions are frozenfor a length of time . The corresponding payoff function in continuous time is VN,p. Thefollowing result can be obtained from first principles; its proof does not rely on Lemmas D.1and D.2.Lemma D.6 w VN,p uniformly as 0.Proof: As wp VN,pp s for p p, there is nothing to show for these beliefs.Fix an initial belief p p, therefore, and consider the process of beliefs pt starting fromp 0 p that corresponds to N players using the risky arm. Let inft 0: pt p and inft , 2, 3, . . . : pt p. Then,VN,pp Ecid:20Z wp EZ 00rerth d N,t er scid:21 ,srerth d N,t er where N is a Poisson process with intensity . As almost surely, we havewp VN,pp EZ Ecid:20Z 0 1 er1h s, er srerth d N,t er rerth d N1,tcid:21 1 ersand hence lim 0 kw VN,pk 0 as claimed.The remaining auxiliary results needed for the proof of Proposition 7 are comparisonresults for w and w as becomes small. We start with equilibrium payoffs in the goodstate.38Lemma D.7 Let p p N . Then w VN,p for sufficiently small.In addition to the stopping times and introduced in the proof of Lemma D.6,Proof:we define inft 0: pt p N , which is the stopping time that an N player cooperative N , we have where 0 is thewould use in continuous time. As p pdeterministic length of time needed for the posterior belief to decay from p to p N when nolump sum arrives. For , therefore, we have , so yields anexpected payoff no smaller than ; that is, w VN,p.Turning to equilibrium payoffs in the bad state, we definewherep s 0h 11h s s 0h, N N 1r N 1 0.Lemma D.8 For p p and sufficiently small, w VN,p.Proof: To ease the notational burden, we write v instead of VN,p. It suffices to show that T v v for sufficiently small .Recall that for p p, vp ph Cup with up 1 pcid:16 1pp cid:17Nwhere theconstant C 0 is chosen to ensure continuity at p. It follows from Keller and Rady 2010that v is strictly increasing on p, 1.tation reveals that E further note that E The function u is strictly decreasing and strictly convex, and a straightforward compuN up for all 0, K 1, . . . , N and p 0, 1. We K up 1 KK p p for all K by the martingale property of beliefs.We define a belief p by requiring that B0,1p p. Starting from p p, whenone player experiments for a length of time without receiving a lump sum, the resultingposterior belief remains above p.On p, 1, we now have T vp maxn1 ph E maxn1 ph ph CE ph CE N upN vp, 1 s E N 1vpo N up, 1 s ph CE N 1upo vp.The third equality holds because E s as p pm by assumption, the fourth holds because E N up E N 1up by strict convexity of u and ph N up up.39On p, p, we have T vp maxn1 ph E maxnph CE 1 vp, 1 s vpo 1 up, 1 s vpo vp,with the inequality holding because E 1 up N 1N up up and s vp.On p, p, we still have 1 s vp vp, while1 ph E 1 vp 1 ph J,1p vBJ,1p0,1p s XJ10,1pcid:2s B0,1ph CuB 1 ph ph vp F p, 0,1pcid:2s B0,1ph CuB1 h Cup0,1pcid:3 E N up0,1pcid:3 C1 1with F p, C 1N 1up 0,1ph CuB0,1pcid:2s B0,1pcid:3 .As 1and BN erN er 1, we have F p, 0. Moreover, as 0,1p p 10,1p, we have 0,1p p 1 1 p00,1p B0,1p p 11 1 p000,1p uB0,1p 0cid:18 01cid:19Nup,andhence F p, Ccid:20 1N 1 0cid:18 01cid:19Ncid:21 up p 1 1 p0s p 11 1 p00h,which is continuously differentiable at any p, 0, 1 IR. For 0, the nonlinearpart of F is a negative multiple of u, so F is strictly concave in p. As Fpp, 0 Cup ph vp 0, we see that for sufficiently small 0, Fpp, 0 and hence F p, F p, for p p. As F p, 0 Cup s ph s vp 0, we thus have T v v on p, p for sufficiently small if we can show that Fp, 0 0. Computing Fp, 0 cid:2 r N r 0 N 1 0cid:3 s ph p 2it is straightforward to check that Fp, 0 0 if and only if p p.1 1 p20h ps,40On 0, p, finally, the monotonicity of v on p, 1 implies that E 1 vp is increasing in p.We thus have1 ph E 1 vp 1 ph E 1 vp vp F p, vp sand hence T vp s vp.Lemma D.9 If 0 0, then p p p 1.Proof: As N 1 andr 0 1 0 r N 0 N 1 0 0cid:18 01cid:19N 0cid:18 01cid:191,we have 1. Combined with the fact that N , this implies palready the desired result in the case that jp 1 and p p N .N p N p p 1, which is Suppose therefore that jp N p 1 and p p N . From Lemma B.1, we know that p pif and only ifpN VN,pjp N 1V 1 jp s rcp 0.Arguing as in the proof of that lemma, we can rewrite the lefthand side of this inequality asp 211p20hN 0cid:18 01cid:19NcpN 10cid:18 01cid:191 cpup 1up; 1psrcp.1; 1From the proof of Lemma D.8, moreover, we know that Fp, 0 0, which is equivalent top 21 1 p2cp ps rcp 0.0h 0cid:18 01cid:19NThus, p p if and only ifr 0 1 0 cpup; 1r 0 11 0 cp 1up 1; 1.Now, for 0 andp s 0h 11h s s 0h,a straightforward computation reveals thatcpup; 1s 0hcid:16 s 0h 1hscid:171 1cid:16 1 cid:171.41Applying this to p p and p 1 p1, we see that p p if and only if the functiong r 0 1 0 1cid:16 1 cid:171satisfies g g1.It is straightforward to show that g has the same sign as where 1r 0r 1 11 0 1.It is thus enough to show that . Our assumption that jp N p 1 translates into N 101 11 0.r As N 1N 12 , this implies that is greater than 101 11 021 0.The proof is complete, therefore, if we can show that .Simple algebra shows that this inequality is equivalent to the concave quadraticq 1r 1 1 0r 20 1 022being positive at 1. We know from Keller and Rady 2010 thatqcan indeed conclude that q1 0. 1r 1 20r and q r01010. As 1r 1 0r 0 are both positive, we 1 r01010rr E Analysis of the Fully Revealing Case 0 0Modifying notation slightly, we write for the probability that, conditional on 1, a playerhas at least one success on his own risky arm in any given round, and g for the correspondingexpected payoff per unit of time.18Consider an SSE played at a given prior p, with associated payoff W . If K 1 playersunsuccessfully choose the risky arm, the belief jumps down to a posterior denoted p K. Notethat an SSE allows the continuation play to depend on the identity of these players. Takingthe expectation over all possible combinations of K players who experiment, however, wecan associate with each posterior p K, K 1, an expected continuation payoff WK .If K 0, so that no player experiments, the belief does not evolve, but there is no reasonthat the continuation strategies and so the payoff should remain the same. We denote 18I.e., 1 e 1 1 1 and g 1h.42the corresponding payoff by W0. In addition, we write 0, 1 for the probability withwhich each player experiments at p, and QK for the probability that at least one player has asuccess, given p, when K of them experiment. The players common payoff must then satisfythe following optimality equation:W max1 p 0g N 1XK1cid:18N 11 s N 1XK0cid:18N 1K cid:19K 1 N 1K QK1g 1 QK1WK1 ,K cid:19K 1 N 1K QK g 1 QKWK 1 N 1W0 .The first term corresponds to the payoff from playing risky, the second from playing safe.As it turns out, it is more convenient to work with odds ratiosl p 1 pand l K p K1 p Kwhich we refer to as belief as well. Note thatp K p 1 Kp 1 K 1 pimplies that l K 1 K l. Note also that 1 QK p 1 K 1 p 1 p1 l K , QK p 1 pl K 1 pl l K.We definem sg s, W s1 pg s, K WK s1 p Kg s.Note that 0 in any equilibrium, as s is a lower bound on the value. Simple computationsnow give maxl 1 m N 1XK0cid:18N 1l K cid:19K 1 N 1K K1 l K1 ,N 1XK0cid:18N 1K cid:19K 1 N 1K K l K .It is also useful to introduce w l and w K K l K. We then getw max1 m N 1XK0cid:18N 1N 1XK0cid:18N 1K cid:19K 1 N 1K w K1 ,K cid:19K 1 N 1K w K .1 l E.1343We definel m 1 1 .This is the odds ratio corresponding to the singleagent cutoff p Note that p 1 for 0.1 p 1 , i.e., l p 1 1 p 1 .We are now ready to prove Lemma 1, which establishes that no perfect Bayesian equilibrium involves experimentation below p 1 or, in terms of odds ratios, l.Proof of Lemma 1: Let l be the infimum over all beliefs for which a positive probabilityof experimentation by some player can be implemented in a perfect Bayesian equilibrium.Note that l 0: This is because the social planners solution is a cutoff policy, with cutoffbounded away from 0. Below this cutoff, s is both the minmax payoff of a player whichhe can secure by always playing safe and the highest average payoff that is feasible giventhat this is the social optimum. Hence this must be the unique perfect Bayesian equilibriumpayoff, and the unique policy that achieves it from the social planners problem specifiesthat all players play safe.Consider some prior belief l l, l1 , so that a single failed experiment takes theposterior belief below l, and fix an equilibrium in which at least one player experiments withpositive probability in the first period. Let this be player n. As the normalized equilibriumpayoff w at the belief l is bounded below by l, and since by construction the payoff equalsl K at any belief l K for K 1, player ns payoff from playing safe is at least1 l XIN nYi Ii Yi N In1 i lI,while the payoff from playing risky is1 m XIN nYi Ii Yi N In1 i lI1.Thus, we must have1 m l XIN nYi I l XIN ni Yi N In1 IYi I l.1 i lI lI1i Yi N In1 iThe sum in the second line achieves its maximum of 1 when i 0 for all i 6 n. Thisimpliesl m 1 1 land hence l l, establishing the lemma.44For all beliefs l l, therefore, any equilibrium has w l, or 0, for each player.We now turn to the proof of Proposition 8.Proof of Proposition 8: Following terminology from repeated games, we say that wecan enforce action 0, 1 at belief l if we can construct an SSE for the prior belief l inwhich players prefer to choose in the first round rather than deviate unilaterally.Our first step is to derive sufficient conditions for enforcement of 0, 1. The conditions to enforce these actions are intertwined, and must be derived simultaneously.To enforce 0 at l, it suffices that one round of using the safe arm followed bythe best equilibrium payoff at l exceeds the payoff from one round of using the risky armfollowed by the resulting continuation payoff at belief l 1 as only the deviating player willhave experimented. See below for the precise condition.What does it take to enforce 1 at l? If a player deviates to 0, we jump to w N 1rather than w N in case all experiments fail. Assume that at l N 1 we can enforce 0.As explained above, this implies that at l N 1, a players continuation payoff can be pusheddown to what he would get by unilaterally deviating to experimentation, which is at most1mw N where w N is the highest possible continuation payoff at belief l N . To enforce 1 at l, it then suffices thatw 1 m w N 1 l 1 m w N ,with the same continuation payoff w N on the lefthand side of the inequality. The inequalitysimplifies tow N 1 m l;by the formula for w, this is equivalent to w l, i.e., 0. Given that l 1 m N l N 1 1 N l 1 m N ,to show that 0, it thus suffices thatl m 1 1 1 1 N l,and that N 0, which is necessarily the case if N is an equilibrium payoff. Note that1 N l l, so that l N l implies l l. In summary, to enforce 1 at l, it sufficesthat l N l and 0 be enforceable at l N 1.How about enforcing 0 at l? Suppose we can enforce it at l 1, l 2, . . . , l N 1, and thatl N l. Note that 1 is then enforceable at l from our previous argument, given ourhypothesis that 0 is enforceable at l N 1. It then suffices that1 l 1 m w N 1 N m N w N ,45where again it suffices that this holds for the highest value of w N . To understand thisexpression, consider a player who deviates by experimenting. Then the following period thebelief is down one step, and if 0 is enforceable at l 1, it means that his continuationpayoff there can be chosen to be no larger than what he can secure at that point by deviatingand experimenting again, etc. The righthand side is then obtained as the payoff from Nconsecutive unilateral deviations to experimentation in fact, we have picked an upper bound,as the continuation payoff after this string of deviations need not be the maximum w N . Thelefthand side is the payoff from playing safe one period before setting 1 and getting themaximum payoff w N , a continuation strategy that is sequentially rational given that 1is enforceable at l by our hypothesis that 0 is enforceable at l N 1.Plugging in the definition of N , this inequality simplifies to2 N N 2 N l N m 1 l m,which is always satisfied for beliefs l m, i.e. below the myopic cutoff lm which coincideswith the normalized payoff m.To summarize, if 0 can be enforced at the N 1 consecutive beliefs l 1, . . . , l N 1, withl N l and l lm, then both 0 and 1 can be enforced at l. By induction, thisimplies that if we can find an interval of beliefs l N , l with l N l for which 0 can beenforced, then 0, 1 can be enforced at all beliefs l l, lm.Our second step is to establish that such an interval of beliefs exists. This second stepinvolves itself three steps. First, we derive some simple equilibrium, which is a symmetric Markov equilibrium. Second, we will show that we can enforce 1 on sufficiently finitelymany consecutive values of beliefs building on this equilibrium; third, we show that this canbe used to enforce 0 as well.It will be useful to distinguish beliefs according to whether they belong to the intervall, 1 1l, 1 1l, 1 21l, . . . For IN , let I 1 1 1l, 1 11l. For fixed , every l l can be uniquely mapped into a pair x, 0, 1 INsuch that l 1 1x l, and we alternatively denote beliefs by such a pair. Notealso that, for small enough 0, one unsuccessful experiment takes a belief that belongs tothe interval I 1 to within O2 of the interval I . Recall that 1 O2.Let us start with deriving a symmetric Markov equilibrium. Hence, because it is Markovian, 0 in our notation, that is, the continuation payoff when nobody experiments isequal to the payoff itself.Rewriting the equations, using the risky arm gives the payoff 19 l 1 m 1 1 N 1l N 1XK0cid:18N 1K cid:1K 1 N 1K1 K 1 N 1 .K cid:19K 1 N 1K K1,19To pull out the terms involving the belief l from the sum appearing in the definition of , use thefact that PN 1K0cid:0N 146while using the safe arm yields 1 1 N 1l 1 N 1 N 1XK1cid:18N 1K cid:19K 1 N 1K K.In the Markov equilibrium we derive, players are indifferent between both actions, and sotheir payoffs are the same. Given any belief l or corresponding pair , x, we conjecture anequilibrium in which a, x2 O3, b, x2 O3, for some functions a, bof the pair , x only. Using the fact that 1 O2, 1 r O2, we replacethis in the two payoff expressions, and take Taylor expressions to get, respectively,and 0 cid:18rb, x 1m 1 rN 1a, xcid:19 3 O4.0 b, x rm 1 x 2 O3.We then solve for a, x, b, x, to getwith corresponding value r1 rx N 12 O3, 1mrx 2 O3.This being an induction on K, it must be verified that the expansion indeed holds at thelowest interval, I1, and this verification is immediate.20We now turn to the second step and argue that we can find N 1 consecutive beliefsat which 1 can be enforced. We will verify that incentives can be provided to do so,assuming that are the continuation values used by the players whether a player deviatesor not from 1. Assume that N 1 players choose 1. Consider the remaining one.His incentive constraint to choose 1 is1 m N 1 N l 1 l 1 N 1l N 1,E.14where N , N 1 are given by at l N , l N 1. The interpretation of both sides is as before,the payoff from abiding with the candidate equilibrium action vs. the payoff from deviating.Fixing l and the corresponding pair , x, and assuming that N 1,21 we insert our 20Note that this solution is actually continuous at the interval endpoints. It is not the only solutionto these equations; as mentioned in the text, there are intervals of beliefs for which multiple symmetric Markov equilibria exist in discrete time. It is easy to construct such equilibria in which 1 and theinitial belief is in a subinterval of I1.21Considering N 1 would lead to N 0, so that the explicit formula for would not applyat l N . Computations are then easier, and the result would hold as well.47formula for , as well as 1 O, 1 r O. This gives N 1cid:182 11 rcid:19 x.Hence, given any integer N IN , N 3N 1, there exists 0 such that for every 0, , 1 is an equilibrium action at all beliefs l l1 , for 3N 1, . . . , N we pick the factor 3 because 11 r 1.Fix N 1 consecutive beliefs such that they all belong to intervals I with 3N 1say, 4N , and fix for which the previous result holds, i.e. 1 can be enforced atall these beliefs. We now turn to the third step, showing how 0 can be enforced as wellfor these beliefs.Suppose that players choose 0. As a continuation payoff, we can use the payoff fromplaying 1 in the following round, as we have seen that this action can be enforced atsuch a belief. This givesl 1 m 1 N l l N .Note that the discounted continuation payoff is the lefthand side of E.14. By deviatingfrom 0, a player gets at mostl 1 m 1 l l 1 .Again inserting our formula for , this reduces tomrN 111 r 0.Hence we can also enforce 0 at all these beliefs. We can thus apply our inductionargument: there exists 0 such that, for all 0, , both 0, 1 can be enforced atall beliefs l l1 4N , lm.Note that we have not established that, for such a belief l, 1 is enforced with acontinuation in which 1 is being played in the next round at belief l N l1 4N .However, if 1 can be enforced at belief l, it can be enforced when the continuation payoffat l N is highest possible; in turn, this means that, as 1 can be enforced at l N , thiscontinuation payoff is at least as large as the payoff from playing 1 at l N as well. Byinduction, this implies that the highest equilibrium payoff at l is at least as large as the oneobtained by playing 1 at all intermediate beliefs in l1 4N , l followed by, say, theworst equilibrium payoff once beliefs below this range are reached.Similarly, we have not argued that, at belief l, 0 is enforced by a continuationequilibrium in which, if a player deviates and experiments unilaterally, his continuation payoffat l 1 is what he gets if he keeps on experimenting alone. However, because 0 can beenforced at l 1, the lowest equilibrium payoff that can be used after a unilateral deviation at 48l must be at least as low as what the player can get at l 1 from deviating unilaterally to riskyagain. By induction, this implies that the lowest equilibrium payoff at belief l is at least as lowas the one obtained if a player experiments alone for all beliefs in the range l1 4N , lfollowed by, say, the highest equilibrium payoff once beliefs below this interval are reached.Note that, as 0, these bounds converge uniformly in to the cooperative solutionrestricted to no experimentation at and below l l and the singleagent payoff, respectively, which was to be shown. This is immediate given that these values correspond toprecisely the cooperative payoff with N or 1 player for a cutoff that is within a distanceof order of the cutoff l, with a continuation payoff at that cutoff which is itself within times a constant of the safe payoff.This also immediately implies as for the case 0 0 that for fixed l lm, both 0, 1can be enforced at all beliefs in lm, l for all , for some 0: the gain from a deviationis of order , yet the difference in continuation payoffs selecting as a continuation payoffa value close to the maximum if no player unilaterally defects, and close to the minimumif one does is bounded away from 0, even as 0.22 Hence, all conclusions extend: fixl l, ; for every o 0, there exists 0 such that for all , the best SSE payoffstarting at belief l is at least as much as the payoff from all players choosing 1 at allbeliefs in l o, l using s as a lower bound on the continuation once the belief l o isreached; and the worst SSE payoff starting at belief l is no more than the payoff from aplayer whose opponents choose 1 if and only if l l, l o, and 0 otherwise.The first part of the Proposition follows immediately, picking arbitrarily p p 1, pm and 1 , as noted, and ii for 1 , p, player is payoff in any equilibrium is weakly lower than his bestreply payoff 1, p, as easily follows from E.13, the optimality equation for p pm, 1. The second part follows from the fact that i pany p pagainst p 1 for all p pw.231 p References Abreu, D. 1986: Extremal Equilibria of Oligopolistic Supergames, Journal of Economic Theory, 39, 195225.Abreu, D., D. Pearce and E. Stacchetti 1986: Optimal Cartel Equilibria 22This obtains by contradiction. Suppose that for some 0, , there is l lm, l for whicheither 0 or 1 cannot be enforced. Consider the infimum over such beliefs. Continuation payoffscan then be picked as desired, which is a contradiction as it shows that at this presumed infimumbelief 0, 1 can in fact be enforced.23Consider the possibly random sequence of beliefs visited in an equilibrium. At each belief, a flowloss of either 1 m or 1 l is incurred. Note that the first loss is independent of the numberof other players experimenting, while the second is necessarily lower when at each round all otherplayers experiment.49with Imperfect Monitoring, Journal of Economic Theory, 39, 251269.Abreu, D., D. Pearce and E. Stacchetti 1993: Renegotiation and Symmetryin Repeated Games, Journal of Economic Theory, 60, 217240.Biais, B., T. Mariotti, G. Plantin and J.C. Rochet 2007: Dynamic Security Design: Convergence to Continuous Time and Asset Pricing Implications,Review of Economic Studies, 74, 345390.Bolton, P. and C. Harris 1999: Strategic Experimentation, Econometrica, 67,349374.Cronshaw, M.B. and D.G. Luenberger 1994: Strongly Symmetric Subgame Perfect Equilibria in Infinitely Repeated Games with Perfect Monitoring and Discounting, Games and Economic Behavior, 6, 220237.Dutta, P.K. 1995: A Folk Theorem for Stochastic Games, Journal of Economic Theory, 66, 132.Fudenberg, D. and D.K. Levine 2009: Repeated Games with Frequent Signals, Quarterly Journal of Economics, 124, 233265.Fudenberg, D., D.K. Levine and S. Takahashi 2007: Perfect Public Equilibrium when Players Are Patient, Games and Economic Behavior, 61, 2749.Heidhues, P., S. Rady and P. Strack 2012: Strategic Experimentation with Private Payoffs, SFB TR 15 Discussion Paper No. 387.H orner, J., T. Sugaya, S. Takahashi and N. Vieille 2011: Recursive Methods in Discounted Stochastic Games: An Algorithm for 1 and a Folk Theorem, Econometrica, 79, 12771318.H orner, J. and L. Samuelson 2013: Incentives for Experimenting Agents,RAND Journal of Economics, 44, 632663.Keller, G. and S. Rady 2010: Strategic Experimentation with Poisson Bandits,Theoretical Economics, 5, 275311.Keller, G., S. Rady and M. Cripps 2005: Strategic Experimentation with Exponential Bandits, Econometrica, 73, 3968.M uller, H.M. 2000: Asymptotic Efficiency in Dynamic PrincipalAgent Problems, Journal of Economic Theory, 39, 251269.Murto, P. and J. V alim aki 2013: Delay and Information Aggregation in Stopping Games with Private Information, Journal of Economic Theory, 148, 24042435.Sadzik, T. and E. Stacchetti 2013: Agency Models with Frequent Actions,working paper, UCLA and New York University.50", "filename": "796734178.pdf", "person": ["Johannes H\u00f6rner", "H\u00f6rner, Johannes", "Nicolas Klein", "Klein, Nicolas", "Sven Rady", "Rady, Sven"], "date": ["2014"]}