{"lang": "en", "identifier_url": ["http://www.econstor.eu/bitstream/10419/56369/1/587124989.pdf"], "title": ["Metropolis-Hastings prefetching algorithms"], "plaintext": "E cient parallelisation of MetropolisHastingsalgorithms using a prefetching approach Ingvar Strid cid:3Dept. of Economic Statistics and Decision Support,Stockholm School of Economics SSEEFI Working Paper Series in Economics and Finance No. 70620091202 revised versionAbstract Prefetching is a simple and general method for singlechain parallelisation of the MetropolisHastings algorithm based on the idea of evaluating the posterior in parallel and ahead of time. Improved MetropolisHastings prefetching algorithms arepresented and evaluated.It is shown how to use available information to makebetter predictions of the future states of the chain and increase the e ciency ofprefetching considerably. The optimal acceptance rate for the prefetching randomwalk MetropolisHastings algorithm is obtained for a special case and it is shownto decrease in the number of processors employed. The performance of the algorithms is illustrated using a wellknown macroeconomic model. Bayesian estimationof DSGE models, linearly or nonlinearly approximated, is identicid:133ed as a potentialarea of application for prefetching methods. The generality of the proposed method,however, suggests that it could be applied in other contexts as well.Keywords: Prefetching, MetropolisHastings, Parallel Computing, DSGEmodel, Optimal acceptance rate, Markov Chain Monte Carlo MCMCcid:3Email adress: ingvar.stridhhs.se. Tel: 4687369232. Fax: 468348161. Permanent adress: Stockholm School of Economics, P.O. Box 6501, SE113 83 Stockholm, Sweden. I thank Sune Karlsson, John Geweke, Ingelin Steinsland, Karl Walentin, Darren Wilkinson and Mattias Villani for comments and discussions that helped improve this paper. I also thank seminar participants at Sveriges Riksbank, cid:214rebro University and the 14th International Conference on Computing in Economics and Finance in Paris. The Center for Parallel Computers at the Royal Instititute of Technology in Stockholm provided computingtime for experiments conducted in the paper.1 Introduction E cient singlechain parallelisation of Markov Chain Monte Carlo MCMC algorithmsis di cult due to the inherently sequential nature of these methods. Exploitation of theconditional independence structure of the underlying model in constructing the parallel algorithm and parallelisation of a computationally demanding likelihood evaluationare examples of problemspecicid:133c parallel MCMC approaches Wilkinson 2006; Strid2007b. The prefetching approach and cid:145automaticcid:146parallelisation using parallel matrixroutines are more general approaches Brockwell 2006; Yan et al. 2007.In this paper we propose simple improvements to the prefetching MetropolisHastingsalgorithm suggested by Brockwell 2006, which is a parallel processing version of themethod originally proposed by Metropolis et al. 1953 and later generalised by Hastings1970. As the name suggests the idea of prefetching is to obtain several draws from theposterior distribution in parallel via multiple evaluations of the posterior ahead of time.It is assumed that the proposal density depends on the current state of the chain, suchthat the future states must be predicted. We show how the random walk MetropolisHastings RWMH prefetching algorithm can be improved by utilising information on theacceptance rate, the posterior and the sequence of realised uniform random numbers inmaking these predictions. It is also explained how the optimal acceptance rate of the RWMH algorithm depends on the number of processors in a parallel computing setting.When the proposal density does not depend on the current state the prediction problemvanishes. Parallelisation of the MetropolisHastings algorithm is simplicid:133ed considerablyand reminiscent of parallelisation by running multiple chains. The attractiveness of theindependence chain MetropolisHastings ICMH algorithm is therefore obvious from aparallel computing perspective.The main focus here is on developing e cient prefetching versions of the oneblock RWMH algorithm. The oneblock case is the most attractive setting from a pure parallele ciency perspective. Further it allows for a simple exposition of concepts and algorithms.The prefetching method generalises to the multiple blocks case, as we describe briecid:135y. Ingeneral, however, we expect it to be less eective in that context, at least in situationswhere some of the full conditional posteriors can be sampled directly using Gibbs updates.Prefetching has obvious limitations in terms of parallel e ciency but there are at leastthree reasons why the approach is still interesting. First is the generality of the oneblockprefetching method; it is largely problem independent. Second, prefetching is easy tocombine with other parallel approaches and can therefore be used to multiply the eectof a competing parallel algorithm, via construction of a twolayer parallel algorithm. Asa result, even if prefetching, by itself, is reasonably e cient only for a small numberof processors its contribution to overall performance is potentially large. Finally, themethod is easy to implement and provides a cheap way of speeding up already existingserial programs.The prefetching algorithms are applied exclusively in the context of Bayesian estima1tion of Dynamic Stochastic General Equilibrium DSGE models. Estimation of largescale linearised DSGE models or nonlinearly approximated DSGE models of any size arecomputationally demanding exercises. This class of models is also chosen because theoneblock RWMH algorithm has been the predominant choice of sampling method, forreasons explained below.Despite the focus on macroeconomic models the generality of the prefetching approachsuggests that the methods should be useful in other contexts. Estimation of long memory time series models and cid:11stable distributions using the RWMH algorithm are directexamples Brockwell 2006; Lombardi 2007. For latent variable models we argue thatprefetching is suitable in conjunction with marginalisation techniques whereas other parallel approaches are required with sampling schemes based on data augmentation and Gibbsupdates. To illustrate this further we discuss how to apply prefetching methods to thehierarchical Bayesian models with Gaussian Markov Random Field GMRF componentsencountered, for example, in spatial statistics KnorrHeld and Rue 2005.In section 2 a brief overview of parallel MCMC is given. In section 3 terminology isintroduced and the main ideas are conveyed via some simple examples before the algorithms are presented. In section 4 the prefetching algorithms are illustrated using twoexample problems. First the algorithms are compared using a wellknown mediumscalemacroeconomic model due to Smets and Wouters 2003 and the potential gains fromusing prefetching methods in the context of Bayesian estimation of largescale linearised DSGE models are discussed. Second the algorithms are applied to the nonlinear estimation of a small DSGE model in an easytouse personal high performance computingPHPC environment, using Matlab on a multicore computer. Finally, in section 5 it isillustrated how prefetching can be combined with lower level parallelism to increase theoverall parallel e ciency of an estimation algorithm.2 A brief overview of parallel MCMCParallel algorithms require the existence of independent tasks that can be performedconcurrently. The granularity, or size, of the tasks determines to what extent an algorithmcan be successfully parallelised. Bayesian inferential techniques cid:133t unequally well with theparallel paradigm. Nonsequential methods, e.g.importance sampling, are better suitedfor parallelisation than sequential methods, such as Gibbs sampling. The relative meritsof dierent sampling algorithms thus change in a parallel setting. This becomes apparentbelow when the random walk prefetching and parallel independence chain MetropolisHastings algorithms are compared. Furthermore, as we demonstrate in this paper theoptimal scaling of the proposal density of the RWMH algorithm changes in a parallelcomputing environment, since statistical e ciency is no longer the sole concern.The most obvious approach to parallel MetropolisHastings is simply to run multiplechains in parallel. This parallel chains approach does not require any parallel programming 2although a parallel program could be used to automise the procedure of running the sameprogram with dierent inputs on several machines Rosenthal 2000; Azzini et al. 2007.In some situations it may also be of interest to parallelise a single chain. Poor mixing and long burnin times are factors which increase the attractiveness of singlechainparallelisation Wilkinson 2006. More generally, MCMC methods are computationallyintensive for a wide range of models and can require days or even weeks of execution time.Parallelisation of a single chain can be divided into within and between draw parallelisation. The former includes, necessarily problemspecicid:133c, parallelisation of the likelihoodevaluation, since typically this is the computationally challenging part of the posteriorevaluation Strid 2007b. The blocking strategy based on exploitation of the underlyingmodelcid:146s conditional independence structure also falls into this category Wilkinson 2006;Whiley and Wilson 2004. In both these approaches several processors collaborate toobtain a draw from the posterior. Between draw parallelism has been given the nameprefetching Brockwell 2006.In the oneblock prefetching approach each processorworks independently on a posterior evaluation.Parallel independence chain MetropolisHastings ICMH and parallel approaches basedon regeneration are important special cases Brockwell and Kadane 2005. From a parallel computing perspective both these methods are largely equivalent to the parallel chainsapproach. Any other singlechain parallel algorithm requires frequent communication between processors. The precise nature of the parallel computer then determines whether analgorithm can be successfully used. Withindraw parallel algorithms are expected to betypically substantially more communication intensive than the prefetching algorithm.Hence if a particular parallel computer is deemed unsuitable for the prefetching algorithm,e.g. because the interconnection network is too slow in relation to the capacity of processors, then it should be even more inappropriate for any other, competing, withindrawsinglechain parallel algorithm.3 Prefetching 3.1 Terminology The objective is to generate draws, fcid:18ig Ri1 where R is the length of the chain, froma posterior distribution, using the oneblock MetropolisHastings prefetching algorithm.Throughout the paper we use the convention that parameters with superindices refer todraws whereas parameters with subindices refer to proposals. The assumption that allparameters in the vector cid:18 are updated jointly, i.e.in one block, allows for a simpleexposition of concepts.The prefetching algorithm can be pictured using a tree; here called a Metropolis treecid:133gure 1. The nodes in the tree represent the possible future states of the chain. Thenumber of levels of the tree, K, is related to the number of nodes, M; through M 2Kcid:01.3The branches represent the decisions to accept or reject a proposal. Accepts are picturedas downleft movements and rejects as downright movements in the tree.Figure 1 Fourlevel Metropolis tree.Level 1Level 2Level 3Level 4Acid:1s 8Accepts 1HHHHHHHHcid:8cid:8cid:8cid:8cid:8cid:8cid:8cid:8s 2s 3Rejectcid:0cid:0cid:0Acid:0cid:0Acid:1As 6cid:1Acid:1cid:1s 12RAAs 13Rcid:1Acid:1s 7Acid:1cid:1s 14ARAAs 15cid:0s 4Acid:0cid:0cid:1Acid:1cid:1ARAAs 9s 5Rcid:1cid:1cid:1AARAAs 11Acid:1s 10A node in the tree is associated with a state of the chain and a proposal based onthis state. An evaluation of a proposed parameter will always occur at the start nodei 1 1. This corresponds to the evaluation required for a serial MetropolisHastingsalgorithm. The state of the chain at node i 1 is cid:18i, assuming that i draws have beenobtained previously, and the proposed parameter is cid:181 v f :jcid:18i. If cid:181 is accepted thechain moves from node 1 to node 2, with state cid:18i1 cid:181, and otherwise the chain movesto node 3 and cid:18i1 cid:18i. More generally the states of the chain at nodes ip and 2ip 1are the same whereas the proposed parameter is unique to each node.The number of processesprocessors the terms are used interchangeably in this paperis given by P . A description of the P nodes and the associated proposed parameters, atwhich the posterior is evaluated in parallel, is called a tour of size PT P fi 1; i 2; :::; i P ; cid:18i 1; cid:18i 2; :::; cid:18i Pg .Often we refer to a tour merely by the node indices, assuming that it is understood howto determine the parameter points. The number of draws, D P; T , produced by a tour isa stochastic variable with support 1; :::; P . A chain consists of the draws obtained from asequence of tours, f Tng Nn1, and the last parameter reached by a tour becomes the startingpoint for the next tour.For an even node ip the parent node is ip 2 . If a nodebelongs to a tour, then its parent must also belong to the tour. The expected number ofdraws obtained from a tour of size P is denoted D P; T and it is given by 2 and for an odd node it is ipcid:01D P; T Pr ip ,PXp141where Pr ip is the probability of reaching node ip and trivially Pr 1 1. The expectednumber of draws of the optimal tour, the tour that maximises D P; T conditional on thebranch probabilities in the Metropolis tree, is denoted D P :For some of the algorithms below the expected draws per tour can be obtained exactlywhereas for others it is estimated via the average 1Ncid:22d NXn1using the output of the MetropolisHastings prefetching algorithm, where fdng Nrealised number of draws from the tours f Tng Ncid:0! D,n1 andpcid:22ddn,2n1 are thethe posterior evaluations are useless.by a law of large numbers. To obtain a chain of length R the required number of toursis N ceilcid:2R cid:22dcid:3 and R N P R posterior evaluations occur in total. Thus R cid:0 R of Expected draws per tour, DP , is equivalent to the theoretical speedup of the algorithm, i.e.it is the speedup in the absence of communication costs and other factorswhich aect parallel e ciency in practice. Draws per tour therefore provides an upperbound on the observed relative speedup S P T 1T P ,3where T p is the time of executing the parallel program using p processors in a particularhardware and software environment. The dierence DP cid:0 S P 0 thus depends onthe precise nature of the parallel computing environment.The maximum possible depth Ma PD is the maximum number of draws that can beobtained from a tour, such that M a P D cid:20 P . Also, let L L ip be the function thatmaps Metropolis tree indices to levels, e.g. L 7 3. A branched tour is a tour forwhich M a P D P . In other words it is a tour with the property that at least two nodesat the same level of the Metropolis tree are evaluated, i.e. L ip L ip for some pairip; ip 2 T . A nonbranched tour satiscid:133es M a P D P , or alternatively L ip P forexactly one ip 2 T . The concepts are illustrated in cid:133gure 2.A static prefetching algorithm is an algorithm where all tours consist of the same nodes,i.e. the indices i 1; :::; i P are constant across tours. For a dynamic prefetching algorithm,on the other hand, the nodes of the tour vary as we move through the chain.Parallel random number generation is not an issue in our context since naturally allrandom numbers are generated by one process. The random numbers should be generatedsuch that the chain is independent of the number of processors used. If this is the casewe say that the random numbers are drawspecicid:133c and it is assured that prefetching does 5Figure 2 Tours.s 1cid:8cid:8cid:8cid:8cid:8cid:8cid:8cid:8s 2ARAAs 5RAAs 11RAs 1HHHHHHHHcid:8cid:8cid:8cid:8cid:8cid:8cid:8cid:8s 2s 6s 5RAcid:0cid:0cid:0cid:0s 3a Nonbranched tour PMa PD4b Branched tour P8, Ma PD5RAAs 7RAABs 15s 31BBBBRnot aect the statistical properties of the chain. This is an implementational issue. Theattachment of random numbers to draws implies that if tour n cid:0 1 produces dncid:01 Pdraws then the random numbers attached to levels dncid:01 1; :::; P of the Metropolis tree,i.e. the levels that are not reached by the tour, should instead be used in tour n.3.2 Examples EXAMPLE 1 Assume that two processors, P 1 and P 2, are available for implementation of the random walk MetropolisHastings RWMH algorithm. The cid:133rst processor isnecessarily employed for evaluation of the posterior kernel p atcid:181 cid:18i cid:15i1,where cid:18i is the current state of the chain.The second processor, P 2, can be used either to prepare for an accept in the cid:133rststage, i.e. for evaluation of the posterior atcid:182 cid:181 cid:15i2 cid:18i cid:15i1 cid:15i2,or to prepare for a reject and evaluate the posterior atcid:183 cid:18i cid:15i2.Assume that the acceptance probability can be chosen approximately as cid:11 by properselection of the increment density g cid:15, e.g. by scaling the covariance matrix of a Normal 6proposal density appropriately. Now, if cid:11 0:5 it is obviously optimal to use P 2 toprepare for a reject at the cid:133rst stage R1 and the optimal tour is T 2 f 1; 3g.If the cid:133rst proposal is rejected two draws are obtained in one time unit using the twoprocessors. If, on the other hand, cid:181 is accepted the posterior evaluation at cid:183 is useless.In this case one draw is obtained in one time unit using two processors. The expectednumber of draws per time unit, or the theoretical speedup, is D 2;f 1; 3g cid:11 2 1 cid:0 cid:11 2 cid:0 cid:11.This is an example of static prefetching since the tours that constitute the chain willcontain the same node indices. It should also be noted that if instead cid:11 0:5 the analysisis symmetric. In the remainder of the paper we restrict attention to the case cid:11 0:5.The reason for this is our focus on the random walk variant of the MetropolisHastingsmethod and it is further motivated by the discussion in section 3.3.5 below.EXAMPLE 2 In the case of three processors, P 1,P 2 and P 3, assume cid:11 0:5 as before.The optimal allocation of P 1 and P 2 is clearly the same as in the example above. Forthe third processor there are three options. First it can be employed for evaluation ofcid:187 cid:18i cid:15i3,i.e. anticipating rejects in the cid:133rst two stages R1,R2. A second possibility is to evaluatecid:182 in anticipation of an accept at the cid:133rst stage. This yields the branched tour f 1; 2; 3gand D 3;f 1; 2; 3g 2 draws are obtained with certainty, since P 2 was used to preparefor a reject at the cid:133rst stage.The cid:133nal possibility is to prepare for a reject in the cid:133rst stage followed by an acceptin the second stage R1,A2 and evaluate In the cid:133rst case R1,R2 the expected number of draws iscid:186 cid:18i cid:15i2 cid:15i3.D 3;f 1; 3; 7g cid:11 2 1 cid:0 cid:11 cid:11 3 1 cid:0 cid:112 .4For example, with cid:11 0:25 this yields 2:31. It is optimal to choose the nonbranched tourf 1; 3; 7g ifwhich is if D 3;f 1; 3; 7g D 3;f 1; 2; 3g 2,Note thatcid:11 cid:20cid:25 0:38.3 cid:0 p 52for cid:11 0:5 so it is never optimal to prepare for R1,A2.D 3;f 1; 3; 7g cid:0 D 3;f 1; 3; 6g 1 cid:0 cid:11 1 cid:0 2cid:11 0,p 5cid:012 cid:25 0:62 would be Again, if instead cid:11 0:5 is assumed the optimal tour for cid:11 cid:21given by f 1; 2; 4g due to the symmetry.7EXAMPLE 3 In the random walk MetropolisHastings algorithm a proposal at stageif ui1 X i1 for somei 1 is accepted with probability cid:11i1 minf X i1; 1g, i.e.ui1 cid:24 U 0; 1 wherep cid:18i cid:15i1.X i1 p cid:18iConsider example 1 again and assume that ui1 10cid:05 where ui1 is the realised uniformrandom number associated with draw i 1. Clearly, in this hypothetical situation it iswise to use P 2 to prepare for an accept, assuming no information about the posterior ratio X i1 is available. The sequence of uniform random numbers, fuig Ri1 which are cid:133xed fromthe outset, thus carries some information on how to best structure prefetching.EXAMPLE 4 The acceptance rate, cid:11; and the sequence of uniform random numbers,fuig Mi1, can be used to improve the e ciency of prefetching. Finally, knowledge aboutthe posterior, p, can be used to make better predictions on where the chain is moving.Assume that an approximation to the posterior, pcid:3, is available and that the evaluationof pcid:3 takes a fraction of the time to evaluate p: Then this approximate distribution canbe used to suggest the states which are likely to be visited subsequently.In the limiting case when pcid:3 p we say that prefetching is perfect; it is analogous toimportance sampling using the posterior as the importance function. In other words it isa situation when neither prefetching or importance sampling is necessary.3.3 Algorithms 3.3.1 Oneblock randomwalk MetropolisHastings prefetching In this section the oneblock MetropolisHastings prefetching algorithm is presented. Thisalgorithm applies when all the parameters in cid:18 are updated jointly and the proposaldensity f depends on the current state of the chain, e.g. for the oneblock random walk MetropolisHastings sampler which we focus on here. It would be possible to use otherproposal densities, e.g. a more general autoregressive proposal, but this route is notpursued here. Multiple block prefetching is discussed briecid:135y at the end of this sectionand in appendix B.The algorithm assumes that the posterior evaluation time is not parameter dependentand it is intended for application on a homogeneous cluster. The key assumption forapplication of the algorithm in practice is, loosely speaking, that the time of a posteriorevaluation is signicid:133cant in relation to the other steps of the algorithm.Algorithm 1 MetropolisHastings prefetching algorithm 1. Choose a starting value cid:180: Set the draw counter S0 0.83. Distribute cid:18ip; p 1; ::; P , to the processes scatter.2. Prefetching step Assume that the chain is in state cid:18Sncid:01 when the nth tour begins,where Sncid:01 is the state of the draw counter after n cid:0 1 completed tours. Constructa tour, Tn fi 1; :::; i P ; cid:18i 1; cid:18i 2; :::; cid:18i Pg serial.4. Evaluate pcid:0cid:18ipcid:1 in parallel.5. Return pcid:0cid:18ipcid:1 gather.6. MetropolisHastings step Run the MetropolisHastings algorithm for the tour T togenerate Dn draws, where 1 cid:20 Dn cid:20 P: Update the draw counter, Sn Sncid:01 Dn,and assign the starting state for the next tour, cid:18Sn serial.7. Go to 2. Stop when Sn cid:21 R draws from the posterior have been obtained. cid:0The posterior evaluations, step 4 above, are performed in parallel. Communicationbetween processes takes place in steps 3 and 5 and message passing collective communication tokens are used to describe the required operations. Steps 2 and 6 are performedby a master process or are replicated by all processors.The MetropolisHastings step, step 6 in the above algorithm, is implemented in thefollowing way for the random walk algorithm:Algorithm 2 Random walk MetropolisHastings step 1. Set j 1. The current state of the chain is cid:18Sncid:01, where Sncid:01 is the state of thedraw counter after n cid:0 1 completed tours. Let cid:18i 1 cid:181 be the proposal conditionalon the current state, i.e. cid:181 fcid:0:jcid:18Sncid:01cid:1 cid:18Sncid:01 cid:15Sncid:011.2. The current node is ip, with associated state cid:18Sncid:01jcid:01, and its level is L ip j. Ifu Sncid:01j cid:11Sncid:01j min1;pcid:0cid:18Sncid:01jcid:01 cid:15Sncid:01jcid:1p cid:18Sncid:01jcid:01 ,5where u U 0; 1, accept the draw and set cid:18Sncid:01j cid:18Sncid:01jcid:01 cid:15Sncid:01j. Otherwiseset cid:18Sncid:01j cid:18Sncid:01jcid:01.3. If i the proposal cid:18ip cid:18Sncid:01jcid:01 cid:15Sncid:01j was accepted and ip 2ip or ii if cid:18ipwas rejected and ip 2ip 1 for some node ip 2 T then move to ip. In this case setj j 1 and the current node to ip and return to 2. Otherwise exit. cid:09The output from algorithm 2 are the Dn draws from the tour n, cid:18Sncid:011; :::; cid:18Sn; andthe output from the prefetching algorithm consists of the drawswhere N is the total number of completed tours andn1 cid:8cid:181; cid:182; :::; cid:18SNcid:9 ,cid:8cid:18Sncid:011; :::; cid:18Sncid:9NNXn1Dn SN cid:21 R.The remaining discussion in this section focuses on the prefetching step step 2 ofalgorithm 1. The prefetching, or tour construction, problem consists of two separateparts:1. Determine a rule for calculating the probabilities of reaching the nodes in the Metropolis tree based on some set of information.2. Find the tour that maximises the expected number of draws per tour, D, based onthese probabilities. We call this the optimal tour, with the implicit understandingthat optimality is conditioned on the specicid:133c rule used.The second task is easy and an algorithm which constructs the optimal tour givena rule for assigning the probabilities of the Metropolis tree is presented in appendix A.Below a parallel independence chain MetropolisHastings algorithm is cid:133rst presented andthen cid:133ve variants of prefetching are discussed, named as follows: 1 Basic prefetching,2 Static prefetching, 3 Dynamic prefetching using the sequence of uniform randomnumbers, 4 Dynamic prefetching using a posterior approximation and 5 Most likelypath. Finally we discuss prefetching with multiple blocks and other proposal densities.3.3.2 Independence chain MetropolisHastings In the special case when the proposal density is cid:133xedparallelisation is simplicid:133ed further since there are no dependencies between posteriorevaluations. Letwhere Rp is the number of posterior evaluations performed by process p and R is thelength of the chain.fcid:0cid:18jcid:18icid:1 f cid:18 ,R Rp,PXp110Algorithm 3 Parallel Independence Chain MetropolisHastings ICMH algorithm 1. Each process p generates cid:18p cid:8cid:181p; cid:182p; :::; cid:18Rppcid:9 wherecid:18ip cid:24 f; i 1; :::; Rp,and collects the values of the posterior evaluated at these parameters in the vectorpp cid:8p cid:181p ; p cid:182p ; :::; pcid:0cid:18Rppcid:1cid:9 parallel.2. The master process gathers cid:18p and pp, p 1; :::; P gather.3. Run the MetropolisHastings algorithm with the posterior already evaluated at Rparameter values serial. cid:0In this algorithm the master process only collects the results from the other processesonce. If memory limitation is a concern it can be solved by collecting the local resultsmore often. In the case of inhomogeneous processors load balance is easily restored byallowing Rp to vary across processors.The parallel ICMH algorithm is embarrassingly parallel both in the sense that it issimple to implement and because there is no essential dependency between the paralleltasks. As a consequence this algorithm will yield close to linear speedup, i.e. S P cid:25 P ,on any homogeneous parallel computer. The prefetching RWMH algorithm is also simpleto implement but good parallel performance for this algorithm requires a balance betweenthe problem or problem size and hardwarenetwork since communication is frequent.3.3.3 Basic prefetching The basic prefetching algorithm suggested by Brockwell 2006 has the property that allfuture states at the same level in the Metropolis tree are treated as being equally likelyto be reached. No information is used to structure prefetching. In our framework thisalgorithm is obtained if every branch in the Metropolis tree is assigned the probability 0:5.The basic prefetching tour evaluates the proposed parameters of all nodes up to agiven level of the Metropolis tree, i.e. T P f 1; 2; :::; Pg, and thus produces a certainnumber of draws D P M a P D log 2P 1.6This approach provides a lower bound on the scalability that can be achieved usingprefetching. Note that if the number of processors P does not correspond to the values 2p cid:0 1; p 1; 2; ::: there is no clear guide on how to select the cid:145surpluscid:146 nodes forevaluation.113.3.4 Static prefetching The researcher can typically target an acceptance rate, cid:11, with good precision by selectionof the proposal density, f, in the MetropolisHastings algorithm: Knowing cid:11 it is easy toimprove on the basic algorithm. Let T Pjcid:11; cid:11 denote the tour when the acceptance rateis cid:11 and the cid:145perceivedcid:146acceptance rate used to construct the tour is cid:11, thus allowing forimperfect targeting. The static prefetching tour is obtained by attaching the probability cid:11to all accept downleft branches of the Metropolis tree and 1 cid:0 cid:11 to reject downrightbranches. This was explained in some detail in examples 1 and 2 above for the cases P 2 and P 3. The nonbranched tour shown in cid:133gure 2 is obtained if we choose P 8and target, for example, cid:11 0:25.The node indices of the tours only needs to be obtained once per chain and theapproach is general, i.e. model independent, since it only depends on the acceptance rate.For large enough cid:11 andor P the tour will be branched.In cid:133gure 3 the expected number of draws per tour, D cid:11; P , and the maximum possibledepth Ma PD of the optimal static prefetching tour is plotted against the acceptance ratecid:11 cid:11 2 0; 0:5 for P 3; 7; 15: The expected number of draws decreases smoothly incid:11 and above some threshold cid:11h P it becomes optimal to choose the basic prefetchingtour. If cid:11 is below some threshold cid:11l P the optimal tour is nonbranched and an cid:145alwaysprepare for a rejectcid:146 strategy is chosen. The Ma PD curve jumps at points where theoptimal tour changes.What happens if the tour is constructed based on a perceived acceptance rate cid:11 whenthe true acceptance rate is in fact cid:11? In cid:133gure 4 the expected number of draws is plottedfor cid:11 2 0; 0:5 and P 3; 7; 15; 31 when the true acceptance rate is cid:11 0:25. It is seenthat it is optimal to use cid:11 cid:11 in constructing the tour but the optimal choice of cid:11 isnot unique. For example, as was seen in example 2 above, in the case of three processorsany cid:11 2 0; 0:38 will suggest the nonbranched tour T 3j 0:25; cid:11 f 1; 3; 7g which is theoptimal tour if cid:11 0:25:3.3.5 Optimal static prefetching A parallel e ciency perspective obviously suggests targeting a low acceptance rate, asseen in cid:133gure 3. Intuitively it becomes very easy to predict where the chain is movingwhen cid:11 is small.The optimal acceptance rate, cid:11opt;1 , for the random walk MetropolisHastings algorithm has been derived under various assumptions on the target density p Roberts et al.1997; Roberts and Rosenthal 2001. Here we show how the optimal acceptance ratedepends on the number of processors in a parallel computing framework, thus considering jointly Markov chain, or statistical, e ciency and parallel e ciency of the staticprefetching RWMH algorithm.12Figure 3 Static prefetching performance, cid:11 cid:11.Figure 4 Static prefetching performance. The true acceptance rate is cid:11 0:25 and toursare constructed based on cid:11.1300.050.10.150.20.250.30.350.40.450.50246810121416Alpha Drawscid:160percid:160tour,cid:160Ma PDP3P7P15Ma PD,cid:160dashed Drawscid:160percid:160tour,cid:160solid 00.050.10.150.20.250.30.350.40.450.501234567Alphatilde Drawscid:160percid:160tour P7P15P31P3We revisit the special case where the posterior density has the product formp cid:18 p cid:18k ,k Yi1k l 2k andand the increment density, g, is of the form g cid:15 N 0; Ikcid:272l is the scaling parameter. In the highdimensional limit, i.e. as k cid:0! 1, and undercertain regularity conditions on p it can be shown that the Markov chain converges to adiusion process and the optimal acceptance rate is obtained by maximising the cid:145e ciencyfunctioncid:146k where cid:272E1 cid:11 _ cid:11 cid:2hcid:8cid:01cid:16 cid:112cid:17i 2,7where cid:8 is the standard normal cumulative distribution function. This yields the famousresult of an optimal acceptance rate cid:11opt;1 arg maxcid:11 E1 cid:11 cid:25 0:234 see Theorem 1 in Roberts and Rosenthal 2001 and the subsequent discussion.In cid:133gure 5 the joint output measure E cid:11; P E1 cid:11 cid:2 D cid:11; P cid:2 c,8is plotted as a function of the acceptance rate cid:11 and the number of processors P . Theconstant c is chosen such that E cid:11opt;1; 1 1. The cid:133gure is interpreted as follows: thetime needed to achieve a given accuracy in estimating any function h cid:18 of the posterioris inversely related to the output E.The optimal acceptance rate in a parallel setting is the acceptance rate which minimisesthe time of obtaining a sample of a cid:133xed size and quality, e.g. a cid:133xed number of iid draws,from the posterior, conditional on the number of processors used. It is seen in cid:133gure 5 thatthe optimal acceptance rate implies an cid:145always prepare for a rejectcid:146prefetching strategy.The optimal rate is below the nonbranched tour boundary which is decid:133ned implicitlyby cid:11 1 cid:0 cid:11Pcid:01 : For example, using P 7 processors the optimal acceptance rateis cid:11opt;7 0:120 and a sample of a given size and quality from the posterior can beobtained E cid:11opt;7; 7 4:3 times faster than when applying one processor and the optimalacceptance rate cid:11opt;1 0:234:Our choice of c implies that E cid:11opt;P ; P is interpreted as the optimal speedup of thestatic prefetching RWMH algorithm and it satiscid:133es D cid:11opt;1; P cid:20 E cid:11opt;P ; P cid:20 D cid:11opt;P ; P ,where equality holds for P 1. For the case of P 7 processors we have E cid:11opt;1; 7 D cid:11opt;1; 7 3:65. The benecid:133t of lowering the acceptance rate from cid:11opt;1 0:234 tocid:11opt;7 0:120, the gain in parallel e ciency, is larger than the cost, the loss of statisticale ciency.14Figure 5 Optimal static prefetching acceptance rates.More generally the optimal acceptance rate cid:11opt;P will be determined by the curvatureof the e ciency function E1 cid:11. In a serial computing setting a cid:135at e ciency curve impliesthat it is cid:145of little value to cid:133nely tune algorithms to the exact optimal valuescid:146Robertsand Rosenthal 2001. In a parallel computing setting the implication is that parallele ciency can be improved, by lowering the acceptance rate, without incurring a largecost in terms of statistical e ciency.In our applications below the e ciency function E1 cid:11 is not available analyticallyand hence the optimal acceptance rate cid:11opt;P cannot be solved for. Instead we target anacceptance rate cid:11 0:25 for all P , a cid:145conservativecid:146choice, and compare the parallel e ciency of dierent prefetching approaches while keeping the statistical properties cid:133xed. Inthe main illustration the empirical static prefetching optimal speedup and the associatedempirical optimal acceptance rates are also obtained.Finally, in the processor limit E cid:11; P o lim Pcid:0!1cid:11opt;P 0.In other words we can aord to make the algorithm arbitrarily poor, in the statisticalsense, only if parallel computing technology is arbitrarily cheap.lim Pcid:0!1narg maxcid:1115Processors Acceptancecid:160rate 051015202530354000.050.10.150.20.250.3Output Optimalcid:160acceptancecid:160rate Boundary,cid:160nonbranchedcid:160tour 931753.3.6 Dynamic prefetching based on the uniform random numbers The probability of accepting a proposal cid:18z drawn from f cid:18jcid:18i depends on the realisedvalue of a uniform random number, ui1. Let X i1 p cid:18zp cid:18i,and consider the conditional acceptance probabilitycid:11i1cid:0ui1cid:1 Prcid:0ui1 X i1jui1cid:1 ,where u is treated as cid:133xed and X as stochastic. Since it is possible to characterise therelationship between cid:11 and u the static prefetching algorithm can be improved upon.In the applications below we incorporate information from the sequence of realised uniform random numbers in the following way. First R draws from the MetropolisHastingssampler are obtained. Next, the unit interval is divided into K subintervals Ik and anempirical acceptance ratecid:11k ffu Xg fu 2 Ikggfu 2 Ikg,9is calculated for each subinterval k 1; :::; K based on approximately RK draws. Theconstant acceptance rate used in the static prefetching algorithm above is then replacedby an acceptance probability which depends on a uniform random number10cid:11i1cid:0ui1cid:1 KXk1cid:11k Icid:0ui1 2 Ikcid:1 ,K ; kwhere Ik kcid:01K and I is the indicator function. This algorithm has the property thatthe branch probabilities in the Metropolis tree are levelspecicid:133c, i.e. all accept branchesat the same level in the tree will have the same probability attached to them.The procedure is illustrated in cid:133gure 6 where the acceptance rate is plotted against theuniform random number for the linear estimation example presented later in the paper.Here R 100; 000 draws are used to obtain estimates of cid:11k for K 20 equally sized subintervals. For example it is seen that cid:111 0:60 is the estimate of Pr u Xju 2 0; 0:05 :3.3.7 Dynamic prefetching based on a posterior approximation If a posterior approximation, pcid:3, is available and if evaluation of pcid:3 is fast in comparisonwith the posterior kernel p a possibility is to use pcid:3 to determine at which parametersto evaluate p. The objective is thus the same as in importance sampling or independence chain MetropolisHastings ICMH sampling, i.e. to cid:133nd a good approximation 16Figure 6 Acceptance rate as a function of the uniform random number in the estimationof a linear DSGE model.to the posterior. However, a dierence is that the quality of the approximating densityused for prefetching has no implications for the statistical analysis; it will cid:145merelycid:146aectcomputational e ciency.For the RWMH algorithm one possible strategy is to simply replace the constant cid:11 inthe construction of the static prefetching tour with the probabilitymincid:12;pcid:3 cid:18i ,pcid:3cid:0cid:18ipcid:1where cid:18ip cid:18icid:15 and thus incorporate information about the posterior in the constructionof the tour. We call this dynamic prefetching using a posterior approximation. If cid:12 1then Pr ip 0 for all p, i.e. all nodes of the Metropolis tree have positive probability ofbeing visited. In the application below we let cid:12 1, thus eliminating parts of the tree.As in the static prefetching case the tour will in general be branched.Another possibility is to run the chain P steps ahead using the approximate density,thereby also incorporating the information contained in the sequence of realised uniformrandom numbers. The states visited using the approximate posterior yield the parameter points at which the posterior p is subsequently evaluated in parallel. The tour isnonbranched and we call this type of algorithm a most likely path algorithm.1700.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.7Uniformcid:160randomcid:160number Acceptancecid:160rate Averagecid:160acceptancecid:160rate0.243.4 Marginalisation and blocking The Dynamic Stochastic General Equilibrium DSGE models used for the illustrationsin this paper are represented as statespace models. The coe cients of the DSGEstatespace model are nonlinear functions of the structural parameters, cid:18. The latent variables,collected in x, are integrated out of the joint posterior p cid:18; xjy using cid:133ltering techniquesand attention is restricted to the marginal posterior density, p cid:18jy, where y denotes thedata. Sampling from the conditional p cid:18jx; y is precluded for these models. In the mainexample, in section 4.1, the Kalman cid:133lter is applied for integration in a degenerate linearand Gaussian statespace LGSS model. If there is interest in the posterior distributionfor the state variables, p xjy, this distribution is obtained via smoothing after p cid:18jyhas been sampled, see e.g. Durbin and Koopman 2001. The marginalisation techniquehas also been used in other contexts, e.g. Garside and Wilkinson 2004.For comparison, consider the more typical LGSS model where interest centers onp cid:18; xjy and where, in contrast to the linearised DSGE model, the classic twoblock dataaugmentation scheme is available. The conditional posterior p cid:18jx; y is sampled using Gibbs or Metropolis updates and p xjcid:18; y is sampled using a simulation smoother, e.g.Strickland et al. 2009. A Gibbs step, viewed as a MetropolisHastings update with acceptance probability 1, implies perfect prefetching and hence prefetching does not apply.In situations where a MetropolisHastings block sampler with reasonably good mixingproperties is available we expect prefetching to be of limited use and, if thought necessary, other parallelisation approaches should be considered. Wilkinson 2006 shows howto exploit the conditional independence structure to parallelise the multimove simulationsmoother used for estimation of the baseline stochastic volatility SV model. This approach can also be applied to extended SV models and the stochastic conditional durationSCD model, for which e cient block sampling schemes have been developed Omori and Watanabe 2008; Takahashi et al. 2009; Strickland et al. 2006. A similar cid:145parallelblockscid:146approach is also applied by Whiley and Wilson 2004.There are two features, in addition to the inability to sample from p cid:18jx; y, whichincrease the attractiveness of the oneblock prefetching approach for DSGE models. First,it is typically a nontrivial task, at least a priori, to split the parameter vector cid:18 into blockssuch that there is weak posterior dependency between blocks. Second, the resulting Metropolis sampler with B blocks is cid:145penalisedcid:146by a Bfactor increase in computationaltime relative to the oneblock sampler.More generally, although prefetching generalises to the case of Metropolis multipleblock sampling parallel e ciency is expected to suer in that context, at least when asubset of the full conditionals can be sampled directly.If high dimensionality of theparameter vector, rather than exploitable structure, is the motive for splitting the parameters into blocks then Metropolis block prefetching could be interesting but this is notinvestigated further here. A brief description of block prefetching is given in appendix B.Another potentially interesting application for prefetching methods, not fully explored 18in this paper, is the class of hierarchical Gaussian Markov random cid:133eld GMRF modelswith a wide range of applications, e.g. in spatial statistics KnorrHeld and Rue 2005.KnorrHeld and Rue 2002 develop a cid:145hyperblockcid:146sampler for this type of model, usingthe methods for fast sampling from GMRFs presented by Rue 2001. This is another,more elaborate, example of a marginal updating scheme where the hyperparameters cid:18and the latent cid:133eld x are updated jointly using a MetropolisHastings step.KnorrHeld and Rue 2002 demonstrate that the hyperblock sampler mixes morerapidly than the traditional singlesite updating scheme and various intermediate blockingschemes in three typical disease mapping applications. The results are driven by thestrong posterior dependency between cid:18 and x in these applications. A oneblock samplerfor linear mixed regression models, along similar lines, is suggested by Chib and Carlin1999. The marginalisation approach is also used by Gamerman et al. 2003 for spatialregression models and in Moreira and Gamerman 2002 for time series models.The possibility of prefetching parallelisation may be interpreted as a further argumentin favour of the sophisticated oneblock sampling scheme for GMRF models, in additionto the documented improvement in chain mixing. In fact, the cid:145always prepare for a rejectcid:146prefetching strategy applies to a more general oneblock sampler, suggested by Wilkinsonand Yeung 2004 for the related class of linear Gaussian Directed Acyclic Graph DAGmodels. In appendix B we outline how to apply prefetching in the GMRF context andalso mention the possibility of combining prefetching with the parallel GMRF samplingapproach suggested by Steinsland 2007.4 Illustrations 4.1 Linear estimation of a DSGE model Long burnin times and poor mixing are factors that can motivate interest in singlechainparallel methods.In the area of Bayesian estimation of Dynamic Stochastic General Equilibrium DSGE models the oneblock random walk MetropolisHastings algorithmis the most commonly used estimation method and our impression is that poor mixing ofchains is a typical experience of researchers in this cid:133eld. For an exception, see Belaygorodand Dueker 2006.The performance of the cid:133ve variants of the prefetching algorithm presented above andthe parallel ICMH algorithm is compared using one of the core macroeconomic modelsat the European Central Bank, the Smets and Wouters SW model Smets and Wouters2003. The model is chosen because it is well known and since it is the backbone oflarger DSGE models currently developed at many central banks. A major determinant ofthe computational cost of estimating a DSGE model is the number of state variables of themodel. Our version of the SW model has 15 state variables, including 8 shocks. Recentlydeveloped largescale microfounded models in use at central banks have as many as 6519state variables and there is at least one very recent example of a model which containsmore than 100 state variables Adolfson, Lasen, Lind and Villani 2007; Christianoet al. 2007. In a relative sense, and using the jargon of parallel computing, this impliesthat our estimation problem can be viewed as cid:133negrained.The empirical analysis of largescale DSGE models is restricted by the computationalcost of estimating them. Reestimation is necessary as dierent model specicid:133cationsare evaluated or when new data arrives. Furthermore, a central bank is facing realtime constraints on these activities.In our view the development of larger and morecomplex models and the unavailability of adequately e cient sampling methods increasethe attractiveness of applying the parallel methods presented here.4.1.1 Model, prior, solution and likelihood The economic content of the model is largely unimportant for our evaluation of the parallelalgorithms and therefore it is not presented here. Similar models have been analysedandor estimated in many articles Smets and Wouters 2003; del Negro et al. 2005.The model used here is presented in detail in Strid 2007a.The model consists of a set of nonlinear expectational equations describing the equilibrium and a linear approximation to the policy function of the model is obtained thesolution. The linear approximate policy function is represented as a linear statespacemodel:Xt T cid:18 Xtcid:01 R cid:18 cid:15t11and Yt d cid:18 ZXt vt,12where 11 is the state equation and 12 the measurement equation. Here Xt dimensionnx is a vector containing the state variables and Yt ny is a vector containing the observedvariables. The structural parameters of the model are collected in the vector cid:18 ncid:18 andthe coe cient matrices, T which is dense, R and Z, and the vector d are nonlinearfunctions of cid:18. The innovations, cid:15t ncid:15, and the measurement errors, vt nv, are assumedto be independent and normally distributed, cid:15t v N 0; cid:6cid:15 and vt v N 0; cid:6v.The likelihood evaluation consists of two parts. First the model is solved using oneof the many available methods Klein 2000. Second the likelihood function of themodel is evaluated using the Kalman cid:133lter, see e.g. Durbin and Koopman 2001. Theprior distribution for cid:18 is similar to the distributions typically used in the literature onestimation of New Keynesian models.For the model estimated here we have the dimensions nx 15; ny 5; ncid:15 8 andncid:18 23. The model is estimated on Euro Area data for the period 1982:12003:4 88observations. The cid:133ve data series used for estimation are the shortterm interest rate,incid:135ation, output growth, the consumptiontooutput ratio and the investmenttooutputratio.204.1.2 Parallel approaches for linearised DSGE models In the context of estimation of largescale linearised DSGE models, what other strategies for singlechain parallelisation are available apart from prefetching? An obviouswithindraw approach is to apply parallel matrix algorithms e.g. using Sca LAPACKor PLAPACK to the computations involved in Kalman cid:133ltering, primarily the matrixmultiplications of the Riccati equation which account for a signicid:133cant part of Kalmancid:133ltering time. However, if this approach is attempted it is crucial to apply parallel routines whenever fruitful, including in the solution algorithm. The extent to which a fewstandard matrix operations account for overall Kalman cid:133ltering and model solution timedetermines the parallel e ciency properties of the approach. Furthermore, it is hard toimprove on optimal serial DSGEspecicid:133c Kalman cid:133lter implementations using parallelmethods, see Strid and Walentin 2008.Note again that it would be straightforward, at least in principle, to combine suchlocal computations strategies, which are somewhat more demanding to implement, withprefetching.4.1.3 Parallel e ciency: draws per tour The posterior approximation used for prefetching variants 4 and 5 is a normal approximation pcid:3 N cid:18m; cid:6m where cid:18m is the posterior mode, Hm the Hessian matrix at them . The scaled inverse Hessian is also used as the covariance matrix formode and cid:6m Hcid:01the proposal density in the random walk MetropolisHastings algorithm, i.e. proposalsare generated withcid:18z cid:18i cid:15i1,m and c is a scaling parameter.where cid:15i1 cid:24 N 0; c Hcid:01Prior to optimisation of the posterior p and estimation using the RWMH algorithmsome parameters in cid:18 are reparameterised to make the parameter space unbounded. Thereparameterisation serves two purposes: cid:133rst, it makes optimisation easier and, second,the e ciency of sampling using the RWMH algorithm is improved because the approximation to normality is better. In the context of prefetching the reparameterisation willthus also improve the parallel e ciency of the algorithms which rely on the posteriorapproximation.For each combination of prefetching algorithm and number of processors P the RWMHprefetching algorithm is used to sample R 500; 000 draws from the posterior distribution. In total the posterior is evaluated R N P RP cid:22d times where N is the numberof tours and cid:22d is the average number of draws per tour. In all estimations the chain isstarted at the posterior mode and 50; 000 draws are discarded as burnin. A target acceptance rate cid:11 0:25 is used to construct the tours in static prefetching 2 and dynamicprefetching based on the uniform random numbers 3 and the average acceptance ratecid:11 turns out to be 0:24. Later on we allow cid:11 to vary with P for the static prefetching 21Table 1 Performance of prefetching algorithms for the linearised DSGE model. Averagenumber of draws per tour, cid:22d.Processors 1Algorithm variant 11. Basic prefetching 12. Static prefetching 3. Dynamic prefetching, uniform 14. Dynamic prefetching, post. approx. 15. Most likely path 116. Optimal static prefetching speedup Optimal empirical acceptance rate 0.22 0.16 0.13 0.09 0.0531 135212.31 3.54 4.75 6.00 12.38 3.81 5.04 6.24 12.57 4.58 6.45 8.04 2.66 4.99 7.31 8.42 8.702.44 4.30 6.55 6.55 73154algorithm. Detailed results on prefetching performance are presented for P 2p cid:0 1;p 1; :::; 5. These values are chosen because they allow for a neat comparison with thebenchmark basic prefetching algorithm, which has integer draws per tour for these valuesof P see expression 6.In table 1 the average number of draws per tour cid:22d are presented. It is seen, cid:133rst, thatstatic prefetching allows us to make easily reaped, although quite small, gains in comparison with the basic algorithm. Second, incorporating knowledge about the sequence ofuniform random numbers lead to additional small gains. These e ciency improvementsare model independent and can always be obtained.Third, in our example the inclusion of information about the posterior via the normalapproximation yields the largest gains. For example, using the most likely path algorithm5 with P 7 processors the extra draws, i.e. the draws which are added to the sure draw,is doubled in comparison with the basic algorithm 1. If a reasonable approximation tothe posterior is available it thus appears possible to improve the parallel e ciency ofprefetching quite substantially.Finally, we report empirical measures of the static prefetching optimal speedup andoptimal acceptance rates for this problem 6. Note that these quantities are adjusted fordierences in sampling e ciency when targeting dierent acceptance rates. Thus directcomparison with the other approaches is possible. The experiments conducted to obtainthese numbers are described in appendix A. Also note that we have not been able toincrease the qualityadjusted speedup for the most likely path algorithm 5 by targetinglower acceptance rates in this way.The results reported above must be interpreted with caution. If the parallel performance of the most likely path algorithm is becoming cid:145too goodcid:146 it suggests that otherapproaches, such as independence chain MetropolisHastings ICMH, should be considered. For our estimation problem we have vericid:133ed that the sampling e ciency of ICMHusing standard choices of proposal densities, e.g. the multivariate t, is inferior to the e ciency of RWMH. The acceptance rate of the best ICMH sampler is 13%. If the thinning 22factor for ICMH is roughly eight times the factor used with the RWMH sampler it is foundthat the relative numerical e ciencies, or the ine ciency factors, of the two approachesare roughly similar.4.1.4 Parallel e ciency on an HPC cluster Draws per tour, DP ; is an abstract measure of scalability since the cost of constructingtours and communication cost is not taken into account. In order to assess the magnitudeof the dierence between theoretical and actual speedup, DP cid:0 S P , the algorithmsare taken to a parallel computer. The prefetching algorithm is implemented using Fortranand the Message Passing Interface MPI and tested on the Lenngren cluster at the Centerfor Parallel Computers PDC at the Royal Institute of Technology KTH in Stockholm,a high performance computing HPC environment. The cluster uses Intel Xeon 3.4GHzprocessors connected by an Incid:133niband 1GB network. The MPI implementation on thecluster is Scali MPI. Further information on the performance characteristics of the clusteris available at www.pdc.kth.se.For each combination of P and algorithm an acceptance rate cid:11 0:25 is targetedand R 500; 000 draws from the posterior are obtained. In table 2 the draws per tour D and relative speedup S, as decid:133ned in 3, are reported for P 2p; p 0; ::; 5 for thestatic prefetching algorithm 2 and the best performing algorithm according to table 1,the most likely path algorithm 5. Using one processor R 500; 000 draws are obtainedin 80 minutes and using P 8 processors the static prefetching algorithm 2 executes in 23 minutes and the most likely path algorithm 5 in 15 minutes.It is seen that for the particular model, programming language, implementation andhardware the speedup is acceptably close to the upper bound, at least for P 1 cid:0 8. Weconclude that on a representative HPC cluster the RWMH prefetching algorithm can beimplemented successfully for a cid:133negrained problem. For the largescale models mentionedabove we expect S cid:25 D in this environment.4.1.5 Discussion In an article written by researchers at the Riksbank, the central bank of Sweden, someeconometric issues related to the estimation of a largescale linearised open economy DSGE model the RAMSES model are addressed Adolfson, Lind and Villani 2007.We present some of their observations because we believe they are representative to thiscid:133eld of research:1. Computing time is a major concern when Bayesian methods are employed to analyselargescale DSGE models.2. The oneblock RWMH algorithm has been the common choice of sampler in theestimation of linearised DSGE models. A substantial problem of the oneblock 23Table 2 Performance of prefetching algorithms for the linearised DSGE model on the Lenngren cluster.Static prefetching Processors, P1Draws per tour, D 1Relative speedup, S 1:00Most likely path Processors, P1Draws per tour, D 1Relative speedup, S 1:0021:761:7242:772:7083:773:46164:924:61326:165:1821:881:8243:353:2385:395:15167:366:45328:547:34ICHM algorithm is that it easily gets stuck for long spells when the parameterspace is highdimensional.3. Blocking approaches are di cult to implement for these models because full conditional posteriors are not easy to simulate from. Furthermore, this approach requiresmultiple evaluations of the likelihood per posterior draw.4. It is found for the RWMH algorithm that decreasing the targeted acceptance ratefrom 0:25 to 0:10 leaves the ine ciency factors largely unaltered.5. Reparameterisation increases the e ciency of sampling substantially.These observations are largely concid:133rmed by our exercise above and taken togetherwe believe that they increase the attractiveness of prefetching methods in the context oflargescale linearised DSGE models.Computing time The issue of when computing time becomes a real concern is largelycontextspecicid:133c. Clearly in our environment, characterised by a relatively small DSGEmodel, Fortran code and decent computing power, the rationale for parallel methods isperhaps limited, as indicated by the absolute execution times reported. Even for a modelof this size it is however clear that the scope of experiments that can be performed in agiven time span is increased.In a more typical desktop computing environment, and using stateoftheart DSGEmodelling tools like DYNARE or YADA, single runs may take several days for the most recent generation of policyrelevant DSGE models. Total project computing time, includinga substantial amount of experimentation, is measured in months.24RWMH vs. ICMH The parallel e ciency of ICMH is far superior to that of RWMH.Using ICMH with P 64 processors to generate one million draws from the posteriordistribution of the SW model the total execution time is around 2:5 minutes on the Lenngren cluster and the relative speedup is S64 63. Although the ICMH sampler hasmuch to recommend to it, especially in a parallel setting, in our example problem it doesnot seem straightforward to cid:133nd an e cient proposal density. In a serial programmingcontext the choice of estimation strategy would certainly be to use the RWMH algorithm.In a parallel framework the tradeos are slightly more complicated and several factorsmust be taken into account.The following example claricid:133es the tradeos for our example under the assumptionthat ICMH requires a thinning factor which is 8 times that of RWMH to achieve roughlythe same sampling e ciency. If P 128 processors are used to estimate the model abovewith the two algorithms, parallel ICMH and RWMH prefetching, then a posterior sampleof a given quality, as judged by the ine ciency factor, can be obtained roughly twice asfast for the ICMH algorithm in comparison with prefetching. If P 8 processors are usedthe execution time of prefetching is more than 5 times faster than for parallel ICMH.Acceptance rate The fourth observation above is especially interesting in relation toprefetching. As demonstrated above the empirical optimal static prefetching e ciencyis not far behind the e ciency of the most likely path approach for the relevant rangeof processors in our problem. These two approaches are also the simplest to implement,since they imply nonbranched tours.To conclude a simple heuristic strategy is suggested: choose the scaling of the incremental density to obtain the optimal static prefetching acceptance rate conditional onthe number of processors see cid:133gure 5 and use the cid:145always prepare for a reject cid:146tour. Itcan be expected that often this will be reasonably close to the optimal overall prefetchingstrategy.4.2 Nonlinear estimation of a DSGE model In this section the MetropolisHastings prefetching algorithm is applied to the problem ofestimating a nonlinearly approximated smallscale DSGE model using Bayesian methods.In the previous section we established that in a high performance computing HPC environment, i.e. on the Lenngren cluster, the prefetching algorithm works successfully for acid:133negrained estimation problem. Our main objective in this section is to assess prefetchingperformance in a particular personal high performance computing PHPC environment:using the parallelism of Matlab the Distributed Computing Toolbox, DCT on a quadcore desktop computer. Alternatively, and perhaps more correctly, we may interpret thisexercise as an evaluation of the parallel functionality of Matlab, using the prefetchingalgorithm as the test ground. The multicoreMatlab environment is presumably one ofthe most accessible and easytouse PHPC environments.25The nonlinear estimation example is chosen cid:133rst because it is an estimation problemthat should be su ciently coarsegrained to deliver reasonable parallel e ciency also inthe PHPC environment. Note that for a given model the particle cid:133lter based evaluation ofthe likelihood for the quadratically estimated model is roughly 1000 times slower than the Kalman cid:133lter likelihood evaluation for the corresponding linearly approximated model.Second, the discontinuous likelihood approximation in the nonlinear case implies thatprefetching methods which rely on a posterior approximation variants 4 and 5 abovecannot be implemented without modicid:133cation. In other words, the most successful variantsof prefetching in the linear estimation example of the previous section are not readilyavailable to us here.4.2.1 Model, prior, solution and likelihood The prototypical smallscale New Keynesian model is borrowed from An 2005. Again,the economic content of the model is largely irrelevant for our purposes here and thereforeno discussion of the model is provided.The policy function of the model is approximated to the second order using the approach of SchmittGrohe and Uribe 2004. The approximative solution can be cast inthe following statespace form. The state equation is separated into an equation for theexogenous state variables the shocksX1t AX1tcid:01 t,13and an equation for the endogenous predetermined variables and a subset of the nonpredetermined variableswhere Xtcid:01 cid:2 X T1t X TIn this way a linear observation equation,X2t B Xtcid:01 Cvech Xtcid:01 X Ttcid:01 e,2tcid:01 cid:3 T and Xt cid:2 X T1t X T2t cid:3 T .1415Yt d ZXt vt,is obtained. The innovations and measurement errors are assumed to be independentand normally distributed, t cid:24 N 0; Q and vt cid:24 N 0; H : The second order policyfunction approximation of a large class of DSGE models can be cast in this form. Thestatespace representation of the corresponding linearly approximated model, which wasused in the previous example, is obtained by letting C 0 and e 0. The matrices A cid:182 ; B cid:181 ; and C cid:181 and the vectors e cid:18 and d cid:181 are functions of the parametersof the economic model, which are collected in cid:18 cid:0 cid:18Tstructural parameters of the model and cid:182 contains the auxiliary parameters, i.e. theparameters describing the shock processes. The dimensions of the vectors are nx 1 3;nx 2 4; nx 7; ny 3; ncid:18 13 and ncid:15 3:cid:18T2 cid:1T where cid:181 consists of the 126The model is estimated on simulated data and we use the same datageneratingprocess, cid:18dgp, as An 2005 and a prior distribution for cid:18 similar to the one in Ancid:146spaper. The likelihood function of the model is evaluated using an Adaptive Linear Particle Filter ALPF, designed for the particular statespace model described by equations1315 Strid 2007a. The particle cid:133lter and its application to the nonlinear estimationof DSGE models is not discussed in detail here and the reader is referred to the referenced articles Arulampalam et al. 2002; Doucet et al. 2000; FernndezVillaverde and RubioRamcid:237rez 2007; Amisano and Tristani 2007.4.2.2 Implementation The estimation routine is implemented in Matlab, using a Fortran mex function only forthe systematic resampling SR step of the Adaptive Linear Particle Filter. The SR algorithm cannot be implemented as vectorised code, implying that a Fortran implementationof this part of the particle cid:133lter is considerably faster than its Matlab counterpart. Thelikelihood evaluation accuracy of the ALPF cid:133lter applied here is at least as good as fora standard particle cid:133lter with 40; 000 particles and the time of a posterior evaluation isroughly 2:5s for this Matlab implementation.The parallel section of the prefetching algorithm, step 4 of algorithm 1, is implementedusing the parallel forloop parfor construct contained in the Parallel Computing Toolbox.The number of calls to parfor is thus equal to the total number of tours, N.4.2.3 Experiment The MetropolisHastings prefetching algorithm is executed on an Opteron 275, 2.2 Ghz,using three of the four available cores. Three variants of prefetching are tested: i staticprefetching, ii dynamic prefetching based on the uniform random numbers and iii amodicid:133ed most likely path MLP algorithm. The modicid:133cation of the MLP algorithmin iii consists of updating the mode of the normal approximation to the posterior inorder to obtain a successively better approximation, even though the exact mode cannotbe obtained. This type of adaptation only aects the quality of prefetching and not thestatistical properties of the chain. The starting mode of the approximation is the linearposterior mode.An acceptance rate cid:11 0:25 is targeted and the actual rate is 0:24. The potentialsuboptimality of this choice for P 3 is disregarded. In each case 50; 000 draws fromthe posterior are obtained. Draws per tour and speedup are presented in Table 3. First,the problem is su ciently coarsegrained for implementation in the particular PHPCenvironment, in the sense that absolute speedup is acceptably close to draws per tour.Second, the results suggest that it is hard to improve on static prefetching when a goodposterior approximation is not immediately available. All three variants roughly halvesestimation time when three cores are used, from 35 to 17 hours of computing time.27Table 3 Performance of prefetching algorithms for the nonlinear DSGE model in a multicoreMatlab environment.Processors Draws per tour Speedup Static prefetching Dynamic prefetching Modicid:133ed MLP111:032:322:0932:372:11111:0111:032:322:065 Twolayer parallelism In this section we briecid:135y discuss the possibility of either i combining prefetching withlower level parallelism or ii using prefetching performance to evaluate competing parallelalgorithms. In the context of the MetropolisHastings algorithm lower level, and hencecompeting, parallelisation means any type of withindraw parallelisation of the posteriorevaluation. Since prefetching is a general, i.e. largely problem independent, singlechainparallel algorithm it can be used to suggest admissible regions for the number of processorsused by the lower level parallel algorithm. Competing parallel algorithms are necessarily problemspecicid:133c, presumably more complicated both to develop and implement andthey certainly require more frequent communication between processors. This suggests anatural benchmark role for the prefetching algorithm.The potential performance gain of combining prefetching with a lower level parallelalgorithm is illustrated using the nonlinear estimation example considered above. A Parallel Standard Particle Filter PPF is used for the likelihood evaluation. The speedupnumbers for the PPF refer to FortranMPI implementations run on the Lenngren cluster Strid 2007b. The number of particles employed is N 40; 000. For the staticprefetching SP algorithm an acceptance rate of cid:11 0:25 is targeted.In cid:133gure 7 it is seen that the PPF is much more e cient than the prefetching approach.The kink in speedup for the PPF is explained by the fact that we estimate a speedupfunction based on a set of observations SP ; P 1; 2; 4; :::; 64 ; while ruling out thepossibility of superlinear speedup. However, when the number of processors exceed P cid:3opt 22 it becomes optimal to switch to the twolayer parallel algorithm which combines staticprefetching with the PPF. As the number of processors increase the speedup dierencebetween the twolayer algorithm and the PPF becomes quite pronounced.It is alsopossible that a minor improvement in speedup, adjusted for statistical e ciency, can beachieved by targeting a lower acceptance rate for prefetching.The simple calculations underlying cid:133gure 7 show how consideration of the prefetchingalternative can guide decisions on the tolerable number of processors to use for the lowerlevel parallel algorithm for a given estimation problem and problem size here the numberof particles employed in the particle cid:133lter. Importantly this type of scalability analysiscan be performed without ever implementing the prefetching algorithm.28Figure 7 Twolayer parallel performance. Static prefetching and a parallel particle cid:133lterapplied for the estimation of a nonlinear DSGE model.6 Conclusions Prefetching algorithms have obvious limitations in terms of parallel e ciency. This is dueto the inherently sequential nature of the MetropolisHastings algorithm. The advantagesof the independence chain MetropolisHastings algorithm in a parallel context are obvious.In this paper we have shown how to substantially increase the e ciency of prefetchingusing some simple techniques. Even using these techniques on reasonably wellbehaved,unimodal, problems it is hard to imagine anyone applying more than, say, 10cid:0 15 processors to a prefetching algorithm. Despite this the simplicity of implementing the methodand the possibility of a multiplication eect if combined with lower level parallelisationwere claimed to motivate interest in the method.The paper has highlighted some complicated tradeos. First, for the prefetchingalgorithm a parallel e ciency perspective suggests targeting of a low acceptance rate.This must be weighted against Markov chain e ciency considerations. For the randomwalk MetropolisHastings algorithm it was demonstrated that the optimal acceptancerate decreases with the number of processors applied to the algorithm. Second, cid:145goodcid:146scalability of prefetching when a posterior approximation is used to construct tours mayindicate that other sampling approaches should also be considered.Bayesian estimation of DSGE models was identicid:133ed as one potential arena for prefetching methods. In this context the experiences reported by researchers at the Riksbank,290102030405060700102030405060Processors Speedup SPPPFSP,cid:160PPFInadmissiblecid:160regioncid:160forcid:160PPFwhich largely correspond to the results reported here for a smaller model, suggest thatprefetching methods is a viable alternative in reducing estimation time. The generality ofthe proposed method, however, suggests that it could be applied in other contexts as well.Brockwell 2006 applies prefetching to long memory time series models. The applicationof prefetching with the hyperblock sampling approach for GMRF models and in relatedcontexts should be more thoroughly explored in future research.Finally, it would be straightforward to implement a prefetching version of the adaptive RWMH algorithm proposed by Haario et al. 2001. In many cases we expect the proposaldistribution of the RWMH and the posterior approximation used to make prefetching predictions to coincide, possibly with a dierent scaling as in our linear estimation example.The adaptive RWMH can then potentially increase statistical and parallel e ciency simultaneously.Appendix AThe prefetching algorithm Let L L ip be the function that maps Metropolis tree node indices to levels, e.g.L 10 4: The expected number of draws D P for the tour T P fi 1; :::; i Pg is givenby D P p Pr D p PXp1PXp1Pr D cid:21 p ph Pr D cid:21 p cid:0 Pr D cid:21 p 1i PXp10 Xip:Lipp PXp1Prip1A PXp1Prip,16where Prcid:16 D pcid:17 is the probability of obtaining p draws and Pr ip is the probability ofreaching node ip of the Metropolis tree, where Pr ip 1.A general prefetching algorithm which constructs the optimal tour, i.e. the tour thatmaximises 16, conditional on the probabilities assigned to the branches of the Metropolistree, is presented below. The algorithm satiscid:133es the obvious need to avoid calculation ofall branch probabilities up to level P in order to obtain a tour of size P . Instead thenumber of probabilities that must be calculated grows linearly in P . Observe that if ipbelongs to the tour then its parent must belong to the tour. This cid:145connectedness propertycid:146is used to restrict the number of probabilities calculated.The expected number of draws per tour may be written recursively as D P D P cid:0 1 Pr i P ,30and the nodes of the optimal tour satisfy 1 Pr 1 Pr i 1 cid:21 ::: cid:21 Pr i P 0,such that the marginal value of an additional processor is decreasing.Algorithm 4 Prefetching tour constructionI 1 ?.1. Set p 2; D 1 1; T 1 fi 1; cid:18i 1g where i 1 1 and decid:133ne the sets cid:5 1 2. When the pth step begins the tour is T p cid:0 1 cid:8i 1; :::; ipcid:01; cid:18i 1; :::; cid:18ipcid:01cid:9 and ipcid:01was added to the tour in the previous iteration. Construct a set of candidate nodesindices I p I p cid:0 1 f 2ipcid:01; 2ipcid:01 1g .3. Construct a set containing the probabilities of reaching the candidate nodescid:5 p cid:5 p cid:0 1 f Pr 2ipcid:01 ; Pr 2ipcid:01 1g .The required probabilities are Pr 2ipcid:01 cid:25p L Pr ipcid:0117and18where Pr ipcid:01 is available from the previous iteration and where L L ipcid:01. Theprobabilities attached to the branches of the Metropolis tree, cid:25p, decid:133ne the dierentvariants of prefetching and they are given below.Pr 2ipcid:01 1 1 cid:0 cid:25p L Pr ipcid:01 ,4. Find the index imax, the node with the largest probability of being reached amongthe candidate nodes in I p, i.e. Primax max cid:5 p and let ip imax.5. Add ip to the tour, T p T p cid:0 1 fipg, and calculate the maximum expecteddraws per tour, D p D p cid:0 1 Prip: Then remove ip and Prip from I p andcid:5 p respectively but store Prip temporarily for calculation of 17 and 18 in the nextiteration.6. If p P go to step 2, otherwise stop. cid:0Variants of prefetching At iteration p of the algorithm above a node ip is allocated to the pth process. Thenode ipcid:01 was added in the previous iteration. Note that for branched algorithms ipis not necessarily the child of ipcid:01. For a nonbranched algorithm the pth processor willnecessarily evaluate a parameter at level p in the tree, i.e. ip 2ipcid:01 or ip 2ipcid:01 1and hence Lip p. For branched algorithms at least one node ip will satisfy L ip p.31The static prefetching algorithm 2 is decid:133ned bycid:25p cid:11,and the basic prefetching algorithm 1 is obtained as the special casecid:25p cid:11 0:5.The prefetching algorithm which utilises information from the sequence of uniform randomnumbers 3 is obtained whenwhere L is the level of the parentcid:25p cid:11cid:0u Lcid:1 ,The conditional acceptance rate, 10, is repeated here L L ipcid:01 .cid:11cid:0u Lcid:1 KXk1cid:11k Icid:0u L 2 Ikcid:1 ,where Ik kcid:01K and where cid:11k was decid:133ned in 9. Note that the uniform random K ; knumbers fupg Pp1 are connected to levels 1 to P of the Metropolis tree. Since the randomnumbers are drawspecicid:133c the attachment of random numbers to levels of the Metropolistree requires that we keep track of the number of draws obtained previously.The algorithm which is based on a posterior approximation 4 is obtained whencid:25p mincid:12;pcid:3cid:0cid:18ipcid:01 cid:0 cid:15Lcid:1 ,pcid:3cid:0cid:18ipcid:01cid:1where 0 cid:12 cid:20 1 and cid:15L is the random vector associated with level L ipcid:01. Note thatcid:18ipcid:01 cid:0 cid:15L is the state associated with node ipcid:01. The random vectors fcid:15pg Pp1 are specicid:133cto levels 1 to P of the Metropolis tree, in the same way as the uniform random numbers.The most likely path algorithm 5 is decid:133ned bycid:25p Iupcid:01 min1;pcid:3cid:0cid:18ipcid:01 cid:0 cid:15pcid:01cid:1 ,pcid:3cid:0cid:18ipcid:01cid:1since the tour is nonbranched and thus we know that L L ipcid:01 p cid:0 1.For the latter two algorithms, with our notation it is more convenient to express theposterior ratios in terms of proposed parameters. In implementations it is of course easyto keep track of the associated states as well.32Optimal static prefetching for the linear DSGE model The DSGE model is estimated ten times, targeting acceptance rates cid:111; :::; cid:1110 evenlyspread in the interval 0:015; 0:40 : In each estimation 500; 000 draws from the posteriorare obtained and the mean cid:22cid:28 cid:11 of the ine ciency factorscid:28j cid:11 1 2IXi1Corrcid:0cid:18tj; cid:18tij cid:1 , j 1; :::; ncid:18,is calculated for each cid:11. The estimated output is E cid:11; P D cid:11; P cid:28mcid:22cid:28 cid:11 wherecid:22cid:28 cid:11 is approximated using simple linear interpolation on the grid of acceptance ratesand cid:28m mincid:11 cid:22cid:28 cid:11. The empirical optimal acceptance rates across processors obtainedin this way are quite similar to those in cid:133gure 5 and therefore we are content with reportingonly the values that appear in table 1.Appendix BThe oneblock sampler for Gaussian Markov random cid:133eld models The reader is referred to the articles referenced below and KnorrHeld and Rue 2005 fora treatment of Gaussian Markov random cid:133eld GMRF models and their use in variousareas of statistics. Let x x 1; :::; xnT be a GMRF which depends on a vector ofhyperparameters, cid:18, with prior density p cid:18. Let y denote the data and assume that theposterior density of interest is of the formp x; cid:18jy p xjcid:18 p cid:18Yiwhere p yijxi is the likelihood for one observation.the start node of a tour. KnorrHeld and Rue 2002 suggest the updating scheme The current state of the chain is denoted cid:17i xi; cid:18i and cid:17 x; cid:18 is the state atp yijxi ,19cid:18z cid:24 f 1cid:0:jcid:18icid:1 ;xz cid:24 f 2cid:0:jcid:18z; y; xicid:1 ,where the proposal cid:17z cid:18z; xz is acceptedrejected jointly using a MetropolisHastingsstep. The vector of hyperparameters cid:18 may be sampled using a random walk step. Ifthe likelihood function is Gaussian the full conditional posterior of the cid:133eld is a GMRFand it is used to sample the proposal, i.e. f 2 :jcid:18z; y; xi p :jcid:18z; y. Otherwise f 2 is a GMRF approximation to this conditional. GMRFs can be sampled using the fast samplingmethods presented by Rue 2001. The proposed joint update is accepted with probabilityminf 1; Ag where 33A p cid:18z p xzjcid:18z p yjcid:18z; xz f 2 xijcid:18i; y; xzp cid:18i p xijcid:18i p yjcid:18i; xi f 2 xzjcid:18z; y; xiif we assume a symmetric proposal for cid:18.,20In the oneblock prefetching algorithm presented in the paper generation of proposalsis performed serially in the prefetching step. In typical applications in spatial statistics thedimension of the latent cid:133eld x is large and sampling from the GMRF approximation f 2accounts for a major part of computational time. Therefore it is required that prefetchingis applied only to the, typically lowdimensional, set of hyperparameters, cid:18. The GMRFproposals for the latent cid:133eld, xi 1; :::; xi P , are then sampled in parallel and conditional onthe hyperparameters of the tour T fcid:18i 1; :::; cid:18i Pg.If the observation model is Gaussian the ratio 20 simplicid:133es to A p cid:18z p yjcid:18zp cid:18i p yjcid:18i,21and all the strategies for prefetching presented in the paper apply to the vector of hyperparameters, cid:18. In KnorrHeld and Rue 2002 p xjcid:18; y is approximated using a secondorder Taylor approximation around the current state of the latent variable, xi, such thatf 2 :jcid:18z; y; xi p :jcid:18z; y; xi. In this case there is only one possibility for prefetching: totarget a low acceptance rate in order to obtain the cid:145always prepare for a rejectcid:146 staticprefetching tour. All proposals, xi 1; :::; xi P , are generated conditional on the latent cid:133eldat the start node of the tour, x, and the processspecicid:133c proposal cid:18ip cid:24 f 1 :jcid:18i.It is realised that this strategy also applies in the more general case where there is xdependence in f 1 since all cid:18ip are generated conditional on the state at the start node,cid:17. This proposal is suggested by Wilkinson and Yeung 2004 in the context of linear Gaussian directed acyclic graph LGDAG models.Further, if the proposal in 19 is used with the LGDAG model parallel sampling couldi1 is obtained using prefetching.proceed in two stages. First the marginal chain fcid:18ig Ri1 are sampled The acceptance probability is given by 21. Next the latent variables fxig Rin embarassingly parallel fashion from f 2 p xjcid:18i; y NChi; K i. Here it is assumedthat hi K i Exjcid:18i; y and the Cholesky factor Gi of the sparse precision matrix K i,which are used to evaluate p cid:18jy in the cid:133rst stage, have been stored. The twostageapproach is possible since p yjcid:18, can be evaluated in a simplicid:133ed way for the LGDAGmodel. However, sampling from f 2 is cheap given that the Cholesky factor G is available.Hence we expect the, seemingly wasteful, approach where x is instead sampled inside theprefetching algorithm to be more e cient in practice. In this case there is no need tostore hi and Gi after xi has been sampled.The crudest approximation considered by Rue et al. 2004 is a GMRF approximationin the mode xm of p xjcid:18; y such that f 2 :jcid:18z; y; xi p :jcid:18z; y; xmcid:18z : In this case all thestrategies for cid:18prefetching apply. This follows since the current value of the latent cid:133eld,34xi, is not explicitly needed to construct the GMRF approximation. However, a cid:145suitablevaluecid:146of x is required as a starting value for cid:133nding the mode, xm. A natural candidatein a prefetching context is x or, if it is important to reduce communication requirement,ip, from the previous tour. In the latter case the latent cid:133eldthe locally stored mode, xmneed not be transferred between processors.A potential weakness of the resulting prefetching algorithm is the inhomogeneouscomputing time in the optimisation step but we expect this to be of minor importance.Some e ciency loss could be acceptable and, if thought necessary, the number of iterationsof the optimiser, e.g. NewtonRaphson, could be cid:133xed at a low number to restore loadbalance. Note that a cruder approximation yields a lower acceptance rate which, inturn, increases prefetching e ciency. Finally it would be possible to construct a twolayer parallel algorithm using the parallel sampling approach for GMRFs suggested by Steinsland 2007.Multiple block prefetching Consider the multiple block MetropolisHastings algorithm with B blocks where the parameter vector cid:18 is split into blocks cid:180; :::; cid:18Bcid:01. A chain of length R requires B RBblock updates. For notational convenience, assume that the blocks are updated in a cid:133xedorder from block 0 to B cid:0 1 using the proposal densitiescid:18b cid:24 fbcid:0:jcid:18icid:01;b; cid:18i;b; cid:18icid:01;bcid:1 , b 0; :::; B cid:0 1,where cid:18i;b cid:18i;b1; cid:18i;b2; :::; cid:18i;Bcid:01 and cid:18icid:01;b is the state of block b when the ith sweepbegins. The choice of scanning strategy does not aect prefetching since the scan orderfor the complete chain is realised at the outset of sampling.The probability of a move for block b is given bycid:11b min1;pcid:0cid:18b; cid:18i;b; cid:18icid:01;bcid:1p cid:18icid:01;b; cid:18i;b; cid:18icid:01;bfb cid:18bjcid:18icid:01;b; cid:18i;b; cid:18icid:01;b ,fbcid:0cid:18icid:01;bjcid:18b; cid:18i;b; cid:18icid:01;bcid:122and we must, as before, assume that evaluation of p is cid:145expensivecid:146. For simplicity weconsider only nonbranched tours. In this context a tour is a collection of proposed blockupdates of the form T ncid:18modb1;Bi 1; cid:18modb2;Bi 2; :::; cid:18modbP;Bi Po ,where it is assumed that block b has just been updated when the tour is entered. Toursand sweeps thus overlap and blocks are associated with levels of the Metropolis tree. Letcid:18 cid:0cid:18i;cid:20b; cid:18icid:01;bcid:1 be the parameter value at the start node of the tour. For random walkupdates it will be desirable to tune each fb such that bold moves are proposed, in orderto decrease block acceptance rates and increase parallel e ciency.35For example, assume B 3; P 6 and that block 1 has just been updated such thatthe current paraneter is cid:18 cid:18i;0; cid:18i;1; cid:18icid:01;2. The cid:145always prepare for a rejectcid:146 tour isip 2 T . The T fcid:182tour produces at least one block update and at most six block updates, i.e. two draws.63g and each processor evaluates pcid:18bip; cid:18cid:0b where cid:18b 3; cid:1817; cid:1821; cid:18015; cid:18031; cid:181A Gibbs step yields perfect prefetching sincefbcid:0:jcid:18icid:01;b; cid:18i;b; cid:18icid:01;bcid:1 pcid:0:jcid:18i;b; cid:18icid:01;bcid:1 ,implies cid:11b 1 and hence prefetching does not apply for a pure Gibbs sampler. A Gibbscomponent within a Metropolis block sampler would be handled by grouping the Gibbsupdate with a Metropolis update on a single processor, under the assumption that the Gibbs update is cid:145cheapcid:146. Clearly, whenever a Gibbs step accounts for a dominant part ofcomputational time prefetching is not feasible. An example of this is when the simulationsmoother is used to sample the latent variables in linear Gaussian statespace models.References Adolfson, M., Lasen, S., Lind, J. and Villani, M. 2007, cid:145Bayesian Estimation of an Open Economy DSGE Model with Incomplete PassThroughcid:146, Journal of International Economics 722, 481cid:150511.Adolfson, M., Lind, J. and Villani, M. 2007, cid:145Bayesian Inference in DSGE Models Some Commentscid:146, Econometric Reviews 2624, 173cid:150185.Amisano, G. and Tristani, O. 2007, cid:145Euro Area Incid:135ation Persistence in an Estimatednonlinear DSGE modelcid:146. Working paper 754, European Central Bank.An, S. 2005, cid:145Bayesian Estimation of DSGE Models: Lessons from Secondorder Approximationscid:146. Working Paper, University of Pennsylvania.Arulampalam, S., Maskell, S., Gordon, N. and Clapp, T. 2002, cid:145A Tutorial on Particle Filters for OnLine NonLinearNonGaussian Bayesian Trackingcid:146, IEEE Transactions on Signal Processing 502, 174cid:150188.Azzini, I., Girardi, R. and Ratto, M. 2007, cid:145Paralellization of Matlab codes under Windows platform for Bayesian estimation: a Dynare applicationcid:146. Working Paper 1,Euroarea Economy Modelling Centre.Belaygorod, A. and Dueker, M. 2006, cid:145The Price Puzzle and Indeterminacy in an Estimated DSGE modelcid:146. Working Paper 2006025, Federal Reserve Bank of St. Louis.Brockwell, A. 2006, cid:145Parallel Markov Chain Monte Carlo Simulation by Prefetchingcid:146,Journal of Computational and Graphical Statistics 151, 246cid:150261.36Brockwell, A. E. and Kadane, J. B. 2005, cid:145Identicid:133cation of Regeneration Times in MCMC Simulation, with Application to Adaptive Schemescid:146, Journal of Computational and Graphical Statistics 142, 436cid:150458.Chib, S. and Carlin, B. P. 1999, cid:145On MCMC Sampling in Hierarchical Longitudinal Modelscid:146, Statistics and Computing 91, 17cid:15026.Christiano, L. J., Trabandt, M. and Walentin, K. 2007, cid:145Introducing Financial Frictionsand Unemployment into a Small Open Economy Modelcid:146. Working paper 214, Sveriges Riksbank.del Negro, M., Schorfheide, F., Smets, F. and Wouters, R. 2005, cid:145On the Fit and Forecasting Performance of NewKeynesian Modelscid:146. Working Paper Series, No. 491,European Central Bank.Doucet, A., Godsill, S. and Andrieu, C. 2000, cid:145On Sequential Monte Carlo Sampling Methods for Bayesian Filteringcid:146, Statistics and Computing 10, 197cid:150208.Durbin, J. and Koopman, S. J. 2001, Time Series Analysis by State Space Methods,Oxford University Press.FernndezVillaverde, J. and RubioRamcid:237rez, J. F. 2007, cid:145Estimating Macroeconomic Models: A Likelihood Approachcid:146, Review of Economic Studies 744, 1059cid:1501087.Gamerman, D., Moreira, A. R. and Rue, H. 2003, cid:145Spacevarying regression models: Specicid:133cations and Simulationcid:146, Computational Statistics and Data Analysis 423, 513cid:150533.Garside, L. and Wilkinson, D. 2004, cid:145Dynamic LatticeMarkov SpatioTemporal Modelsfor Environmental Datacid:146, Bayesian Statistics 7, 535cid:150542.Haario, H., Saksman, E. and Tamminen, J. 2001, cid:145An adaptive Metropolis algorithmcid:146,Bernoulli 72, 223cid:150242.Hastings, W. 1970, cid:145Monte Carlo Sampling Methods Using Markov Chains and Their Applicationscid:146, Biometrika 571, 97cid:150109.Klein, P. 2000, cid:145Using the Generalized Schur Form to Solve a Multivariate Linear Rational Expectations Modelcid:146, Journal of Economic Dynamics and Control 2410, 1405cid:1501423.KnorrHeld, L. and Rue, H. 2002, cid:145On Block Updating in Markov Random Field Modelsfor Disease Mappingcid:146, Scandinavian Journal of Statistics 294, 597cid:150614.37KnorrHeld, L. and Rue, H. 2005, Gaussian Markov Random Fields: Theory and Applications, Chapman and Hall.Lombardi, M. J. 2007, cid:145Bayesian Inference for Alphastable Distributions: A Random Walk MCMC Approachcid:146, Computational Statistics and Data Analysis 515, 2688cid:1502700.Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A. and Teller, E. 1953, cid:145Equationof State Calculations by Fast Computing machinescid:146, Journal of Chemical Physics 216, 1087cid:1501092.Moreira, A. R. and Gamerman, D. 2002, cid:145Bayesian Analysis of Econometric Time Series Models using Hybrid Integration Rulescid:146, Communications in Statistics. Theory and Methods 311, 49cid:15072.Omori, Y. and Watanabe, T. 2008, cid:145Block Sampler and Posterior Mode estimationfor Asymmetric Stochastic Volatility Modelscid:146, Computational Statistics and Data Analysis 526, 2892cid:1502910.Roberts, G., Gelman, A. and Gilks, W. 1997, cid:145Weak Convergence and Optimal Scaling of Random Walk Metropolis Algorithmscid:146, The Annals of Applied Probability 71, 110cid:150120.Roberts, G. O. and Rosenthal, J. S. 2001, cid:145Optimal Scaling for Various Random Walk Metropolis Algorithmscid:146, Statistical Science 164, 351cid:150367.Rosenthal, J. S. 2000, cid:145Parallel Computing and Monte Carlo algorithmscid:146, Far Eastern Journal of Theoretical Statistics 4, 207cid:150236.Rue, H. 2001, cid:145Fast Sampling of Gaussian Markov Random Field Modelscid:146, Journal ofthe Royal Statistical Society Ser B. 632, 325cid:150338.Rue, H., Steinsland, I. and Erland, S. 2004, cid:145Approximating Hidden Gaussian Markov Random Modelscid:146, Journal of the Royal Statistical Society 664, 877cid:150892.SchmittGrohe, S. and Uribe, M. 2004, cid:145Solving Dynamic General Equilibrium Models Using a SecondOrder Approximation to the Policy Functioncid:146, Journal of Economic Dynamics and Control 28.Smets, F. and Wouters, R. 2003, cid:145An Estimated Stochastic Dynamic General Equilibrium Model of the Euro Areacid:146, Journal of the European Economic Association 15, 1123cid:1501175.Steinsland, I. 2007, cid:145Parallel Exact Sampling and Evaluation of Gaussian Markov Random Fieldscid:146, Computational Statistics and Data Analysis 516, 2969cid:1502981.38Strickland, C. M., Forbes, C. S. and Martin, G. 2006, cid:145Bayesian Analysis of the Stochastic Conditional Duration Modelcid:146, Computational Statistics and Data Analysis 509, 2247cid:1502267.Strickland, C. M., Forbes, C. S. and Martin, G. M. 2009, cid:145E cient Bayesian Estimationof Multivariate State Space Modelscid:146, Computational Statistics and Data Analysis 5312, 4116cid:1504125.Strid, I. 2007a, cid:145A Simple Adaptive Particle Filter for SecondOrder Approximated DSGE Modelscid:146. Mimeo, Stockholm School of Economics.Strid, I. 2007b, cid:145Parallel particle filters for likelihood evaluation in DSGE models: Anassessmentcid:146. Mimeo, Stockholm School of Economics.Strid, I. and Walentin, K. 2008, cid:145Block Kalman Filters for Largescale DSGE modelscid:146,Computational Economics 333, 277cid:150304.Takahashi, M., Omori, Y. and Watanabe, T. 2009, cid:145Estimating Stochastic Volatility Models using Daily Returns and Realized Volatility Simultaneouslycid:146, Computational Statistics and Data Analysis 536, 2404cid:1502426.Whiley, M. and Wilson, S. P. 2004, cid:145Parallel Algorithms for Markov Chain Monte Carloin Latent Spatial Gaussian Modelscid:146, Statistics and Computing 143, 171cid:150179.Wilkinson, D. 2006, Parallel Bayesian Computation, in E. J. Kontoghiorghes, ed.,cid:145Handbook of Parallel Computing and Statisticscid:146, Chapman and Hall, chapter 16,pp. 477cid:150508.Wilkinson, D. and Yeung, S. K. 2004, cid:145A Sparse Matrix Approach to Bayesian Computation in Large Linear Modelscid:146, Computational Statistics and Data Analysis 443, 493cid:150516.Yan, J., Cowles, M. K., Wang, S. and Armstrong, M. P. 2007, cid:145Parallelizing MCMC for Bayesian Spatiotemporal Geostatistical Modelscid:146, Statistics and Computing 174, 323cid:150335.39", "filename": "587124989.pdf", "person": ["Ingvar Strid", "Strid, Ingvar"], "date": ["2008"]}